{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["JHT8k32TjGlI","gVVs9gePkSml"],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyOGF5m0VFC/BTOo11cOklls"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"207c6878016c4b8db2c588484ee5ae98":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50f356271852486a85859bcfc3fe5006","IPY_MODEL_9732b25679d44ad39b57510d6b3e5666","IPY_MODEL_1afb250c196249ccb861b6a4dd180e2d"],"layout":"IPY_MODEL_a9302f3d87fa4d7dad824922e4587988"}},"50f356271852486a85859bcfc3fe5006":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ff1bf1fcd364cd9b85947320e121212","placeholder":"​","style":"IPY_MODEL_10a861f88b16474ba086b72fa8079ffc","value":"config.json: 100%"}},"9732b25679d44ad39b57510d6b3e5666":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_921c03295ee94d349d860ea018771562","max":4104,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23496bb0677649f1b00f5cb052b20855","value":4104}},"1afb250c196249ccb861b6a4dd180e2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d9f2c48895844c980a7ff57c9890bab","placeholder":"​","style":"IPY_MODEL_e8b2ded7967d45c2868463167629fb8a","value":" 4.10k/4.10k [00:00&lt;00:00, 299kB/s]"}},"a9302f3d87fa4d7dad824922e4587988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ff1bf1fcd364cd9b85947320e121212":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10a861f88b16474ba086b72fa8079ffc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"921c03295ee94d349d860ea018771562":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23496bb0677649f1b00f5cb052b20855":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d9f2c48895844c980a7ff57c9890bab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8b2ded7967d45c2868463167629fb8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0aade5eb1df04d34b0496e0b9525cafd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_79900d7d2a354a19be83d91908da5a85","IPY_MODEL_1ad161116a434c5bbd162c27e0635212","IPY_MODEL_f364ff7620544ed3b0326e39fe6d986e"],"layout":"IPY_MODEL_8f2ab5f4a6e94bfda07d2763b80083da"}},"79900d7d2a354a19be83d91908da5a85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0b7e9033f4b41ce832b0435e33adf0b","placeholder":"​","style":"IPY_MODEL_346e109c8a864f3eb100784ac536a2da","value":"pytorch_model.bin: 100%"}},"1ad161116a434c5bbd162c27e0635212":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d25e790f731c48dbbf367dc0c92d64be","max":598641023,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cfe41c4e8c364804ba766fe4f441f778","value":598641023}},"f364ff7620544ed3b0326e39fe6d986e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f61cafb202da488885d3baccac6ab640","placeholder":"​","style":"IPY_MODEL_bd9dfcacd74e4a3d833874a4b12b3c70","value":" 599M/599M [00:02&lt;00:00, 242MB/s]"}},"8f2ab5f4a6e94bfda07d2763b80083da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0b7e9033f4b41ce832b0435e33adf0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"346e109c8a864f3eb100784ac536a2da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d25e790f731c48dbbf367dc0c92d64be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfe41c4e8c364804ba766fe4f441f778":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f61cafb202da488885d3baccac6ab640":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd9dfcacd74e4a3d833874a4b12b3c70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81ade6f552114fc6917693e084925a3a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2cd2fb355e3949e79ced32f13c590ca8","IPY_MODEL_c4da21a0d9f94baea442387364f9794f","IPY_MODEL_ddd7e504ed59481cb55af1e902e5bd90"],"layout":"IPY_MODEL_a921811d276e44b1a1bbf82e55b4ca5d"}},"2cd2fb355e3949e79ced32f13c590ca8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d7f6f8e5fde49538aac6f2e91a6a099","placeholder":"​","style":"IPY_MODEL_b9754b4cf508416c92339534a5f52766","value":"preprocessor_config.json: 100%"}},"c4da21a0d9f94baea442387364f9794f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2272904dde94d60979050a118204382","max":316,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90587da309104dd3b1ecc3c42a824c46","value":316}},"ddd7e504ed59481cb55af1e902e5bd90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f088793c3bda45a0a3b603bf382be821","placeholder":"​","style":"IPY_MODEL_22a6bb0fd2d842ff85abc9500c66251f","value":" 316/316 [00:00&lt;00:00, 24.3kB/s]"}},"a921811d276e44b1a1bbf82e55b4ca5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d7f6f8e5fde49538aac6f2e91a6a099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9754b4cf508416c92339534a5f52766":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2272904dde94d60979050a118204382":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90587da309104dd3b1ecc3c42a824c46":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f088793c3bda45a0a3b603bf382be821":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22a6bb0fd2d842ff85abc9500c66251f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c3de5cdeca34835bcdd5a8b89a27270":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47ba7aa60d764fa482753f6abcdc38e4","IPY_MODEL_f88481ee7bec4b0d9533580038e19548","IPY_MODEL_0a105c2fc25f4f9cb2fd16f48cc2d36e"],"layout":"IPY_MODEL_9aca16ff19d946ad8253c9363b9e5601"}},"47ba7aa60d764fa482753f6abcdc38e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab2ae39632c54fdbaae41bcbb4836015","placeholder":"​","style":"IPY_MODEL_0f89df7dc0d043449d528a26808e9bcc","value":"tokenizer_config.json: 100%"}},"f88481ee7bec4b0d9533580038e19548":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_36c2202879904a66857a83b4598c82c3","max":905,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d219de78793463b82e1230c0d31410b","value":905}},"0a105c2fc25f4f9cb2fd16f48cc2d36e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f29459a4c24a4d94a8e1b30d3c6b2534","placeholder":"​","style":"IPY_MODEL_d879364f3dc0401db1ed6ca7eec9912f","value":" 905/905 [00:00&lt;00:00, 63.5kB/s]"}},"9aca16ff19d946ad8253c9363b9e5601":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab2ae39632c54fdbaae41bcbb4836015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f89df7dc0d043449d528a26808e9bcc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36c2202879904a66857a83b4598c82c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d219de78793463b82e1230c0d31410b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f29459a4c24a4d94a8e1b30d3c6b2534":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d879364f3dc0401db1ed6ca7eec9912f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36444a9b06fb4d04b9bfbb1dffc8319b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_273eec5664684f318d1fb796e247f9f3","IPY_MODEL_8e301e0e1c2742e09bc09ba1bc0806be","IPY_MODEL_15dc2191119f4453bbf43d280668b20e"],"layout":"IPY_MODEL_0b47057e945a45b3b83090593f7c5294"}},"273eec5664684f318d1fb796e247f9f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb27b8b71140454ea554e9727b996669","placeholder":"​","style":"IPY_MODEL_9a029a1d47f44612a8d4a4856cfe9d28","value":"vocab.json: 100%"}},"8e301e0e1c2742e09bc09ba1bc0806be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aff030731a9f43c5b257cc0ccc36839d","max":961143,"min":0,"orientation":"horizontal","style":"IPY_MODEL_21f218f3fd724c2fadf6bc65720ca6fb","value":961143}},"15dc2191119f4453bbf43d280668b20e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9ac8fffddfc414bb1df3c15907ebbb2","placeholder":"​","style":"IPY_MODEL_53781fd3d61a407dbc63a8b00afc1e0d","value":" 961k/961k [00:00&lt;00:00, 7.21MB/s]"}},"0b47057e945a45b3b83090593f7c5294":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb27b8b71140454ea554e9727b996669":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a029a1d47f44612a8d4a4856cfe9d28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aff030731a9f43c5b257cc0ccc36839d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21f218f3fd724c2fadf6bc65720ca6fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9ac8fffddfc414bb1df3c15907ebbb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53781fd3d61a407dbc63a8b00afc1e0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3e1d0deb1f743e782d4071d997fef92":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96a63ac5457246548d0506b1c43c39e6","IPY_MODEL_a6b558e1a6c243af98425d9a1faea830","IPY_MODEL_ffc87baaf61e4a128a0a8f04ad2fe368"],"layout":"IPY_MODEL_90358bb9dc7f4dfca339ab131f5a5a80"}},"96a63ac5457246548d0506b1c43c39e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f13df8019280480b93f99599b6109303","placeholder":"​","style":"IPY_MODEL_61c131df5d75421e86318023fef3cddb","value":"merges.txt: 100%"}},"a6b558e1a6c243af98425d9a1faea830":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d81e26af0ad34b2d9e07fe29dbaaaf44","max":524619,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b68a9c4d12b34209b5f792235eb06569","value":524619}},"ffc87baaf61e4a128a0a8f04ad2fe368":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40bfecf1cea343d0b38898f660fff4eb","placeholder":"​","style":"IPY_MODEL_bfc4d42eab2d4a50bc0f9b57c1c279b2","value":" 525k/525k [00:00&lt;00:00, 4.06MB/s]"}},"90358bb9dc7f4dfca339ab131f5a5a80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f13df8019280480b93f99599b6109303":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61c131df5d75421e86318023fef3cddb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d81e26af0ad34b2d9e07fe29dbaaaf44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b68a9c4d12b34209b5f792235eb06569":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40bfecf1cea343d0b38898f660fff4eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfc4d42eab2d4a50bc0f9b57c1c279b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dff30123d444ddfb7025582c1a01c24":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e335686f09e5489ba06713f6bf975516","IPY_MODEL_2e67a43cc7df4472a69060666921a438","IPY_MODEL_fe82014672fc435fa07228b02a18335b"],"layout":"IPY_MODEL_5f423d5d42e24c65b4aedacd7dd649bc"}},"e335686f09e5489ba06713f6bf975516":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dcc429b1e2d4dd89348cecf203ba87e","placeholder":"​","style":"IPY_MODEL_c19722f88cac466d9052282bbc847264","value":"tokenizer.json: 100%"}},"2e67a43cc7df4472a69060666921a438":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_234641e834764678a3cba75e27cb2041","max":2224003,"min":0,"orientation":"horizontal","style":"IPY_MODEL_829d5bc38b1b4d31a7f75441eecb9d51","value":2224003}},"fe82014672fc435fa07228b02a18335b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e047e7d37058410188c72f410722567a","placeholder":"​","style":"IPY_MODEL_803dd42f788b469f95e5054acc2f9d62","value":" 2.22M/2.22M [00:00&lt;00:00, 28.9MB/s]"}},"5f423d5d42e24c65b4aedacd7dd649bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dcc429b1e2d4dd89348cecf203ba87e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c19722f88cac466d9052282bbc847264":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"234641e834764678a3cba75e27cb2041":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"829d5bc38b1b4d31a7f75441eecb9d51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e047e7d37058410188c72f410722567a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"803dd42f788b469f95e5054acc2f9d62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95c7a0c02039439ba2868888447fa231":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c71a2ce163cf4b2e9cadb7f9b517600a","IPY_MODEL_c46cf18b106c4b08ba8a25b2c1a40b6f","IPY_MODEL_ba2a7fc62fa0495ebbb96b051d313cf8"],"layout":"IPY_MODEL_6ac8dd0f13ad4e76b1ff61509e289a3f"}},"c71a2ce163cf4b2e9cadb7f9b517600a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03fade87f04b4b249cd734aed8db5f1f","placeholder":"​","style":"IPY_MODEL_ec47e20ed8f24de992d34ef7e95d2b33","value":"special_tokens_map.json: 100%"}},"c46cf18b106c4b08ba8a25b2c1a40b6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccc3007c4c184606815d4eea3c9e6c68","max":389,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da54085c072b458783fda9ea35699e9a","value":389}},"ba2a7fc62fa0495ebbb96b051d313cf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_874e0e63e38b4091b03321fd3da47b27","placeholder":"​","style":"IPY_MODEL_87bcd061c248461a9442602e314e79ee","value":" 389/389 [00:00&lt;00:00, 31.4kB/s]"}},"6ac8dd0f13ad4e76b1ff61509e289a3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03fade87f04b4b249cd734aed8db5f1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec47e20ed8f24de992d34ef7e95d2b33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccc3007c4c184606815d4eea3c9e6c68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da54085c072b458783fda9ea35699e9a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"874e0e63e38b4091b03321fd3da47b27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87bcd061c248461a9442602e314e79ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"448212f279d44dd8a981f35d7c0805e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47b10df4b003463abaf082f492ea8f54","IPY_MODEL_f6bac7515bd74318ba047ec4b6909ba0","IPY_MODEL_2f9c87774a3d4916a04bfd9194eeb8b1"],"layout":"IPY_MODEL_509395a6585a46899b80dae980b48cb4"}},"47b10df4b003463abaf082f492ea8f54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d334c51933934d8ab5aca971c0fed16a","placeholder":"​","style":"IPY_MODEL_8f44d552615f4b44a45e810d28b21d95","value":"config.json: 100%"}},"f6bac7515bd74318ba047ec4b6909ba0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f2b3080a10f47d585b3ebbbdaf54bc3","max":4609,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d67c56f77054ad98da16231f0d10618","value":4609}},"2f9c87774a3d4916a04bfd9194eeb8b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdc51a0afc634d809735201e48247583","placeholder":"​","style":"IPY_MODEL_bbec1284655844fb80390431d4dc6949","value":" 4.61k/4.61k [00:00&lt;00:00, 331kB/s]"}},"509395a6585a46899b80dae980b48cb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d334c51933934d8ab5aca971c0fed16a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f44d552615f4b44a45e810d28b21d95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f2b3080a10f47d585b3ebbbdaf54bc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d67c56f77054ad98da16231f0d10618":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bdc51a0afc634d809735201e48247583":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbec1284655844fb80390431d4dc6949":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"645cd6cefe9d4a21ac949b53c4bb8358":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ecfb70403d34f4fa4efaed9faf9e728","IPY_MODEL_a3f2713fb511458ba4bbd1eaf1ac23f9","IPY_MODEL_63a5e009575445d9a52eb5c46f4aa4aa"],"layout":"IPY_MODEL_48c12f3e05944b50baf3541060b1d51b"}},"3ecfb70403d34f4fa4efaed9faf9e728":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8601f6769cd472481566b28ca6115ee","placeholder":"​","style":"IPY_MODEL_246a14fac9564979a76f6e28e117b7e6","value":"pytorch_model.bin: 100%"}},"a3f2713fb511458ba4bbd1eaf1ac23f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4eb660f0476e4b7ca08441598476075b","max":982141993,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be926a9924044421bb30b254b58f71d3","value":982141993}},"63a5e009575445d9a52eb5c46f4aa4aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d5632687e064ef2b7e01324ce6c3d7a","placeholder":"​","style":"IPY_MODEL_b982f1782c4c4ebf9ad4233309070fb2","value":" 982M/982M [00:04&lt;00:00, 225MB/s]"}},"48c12f3e05944b50baf3541060b1d51b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8601f6769cd472481566b28ca6115ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"246a14fac9564979a76f6e28e117b7e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4eb660f0476e4b7ca08441598476075b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be926a9924044421bb30b254b58f71d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d5632687e064ef2b7e01324ce6c3d7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b982f1782c4c4ebf9ad4233309070fb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d95f2a9ee89d4d9b998c300051c4b655":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dd897f4a65b044beacec5c5e079ad021","IPY_MODEL_464cc01cc4dc4a6d9895c520602d677b","IPY_MODEL_41f305b0345948a6b979e236872d5e38"],"layout":"IPY_MODEL_42aad04d36664a22880a0e315335f862"}},"dd897f4a65b044beacec5c5e079ad021":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90961fc4f62948e9a99c8d7deba74a3c","placeholder":"​","style":"IPY_MODEL_885cb41153704e29b00b183bc51d8530","value":"tokenizer_config.json: 100%"}},"464cc01cc4dc4a6d9895c520602d677b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5d3625c3b8b4d2084b89854a1b0b6c6","max":241,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09a44fb8d782460faeb75b2d85094e08","value":241}},"41f305b0345948a6b979e236872d5e38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90798b420dca48e29346eb003ebb1d5e","placeholder":"​","style":"IPY_MODEL_467d81b5d44e499588a1dc20d5cb028f","value":" 241/241 [00:00&lt;00:00, 20.9kB/s]"}},"42aad04d36664a22880a0e315335f862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90961fc4f62948e9a99c8d7deba74a3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"885cb41153704e29b00b183bc51d8530":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5d3625c3b8b4d2084b89854a1b0b6c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09a44fb8d782460faeb75b2d85094e08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"90798b420dca48e29346eb003ebb1d5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"467d81b5d44e499588a1dc20d5cb028f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e509c7c2c1384c8696b17015d2e7bca0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_093181d2ba974e7db17660bf1f9d3c01","IPY_MODEL_2cd2fd9c98f148808b20d2436501b188","IPY_MODEL_3be218e4154a4a788f2e26965e5de713"],"layout":"IPY_MODEL_13eec7c2561649d1bc9c4fdf447b0ff2"}},"093181d2ba974e7db17660bf1f9d3c01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d9fa5ae4998407a8545115e332b9125","placeholder":"​","style":"IPY_MODEL_c9b2fadde2514ee6969ad593b05b6d62","value":"vocab.json: 100%"}},"2cd2fd9c98f148808b20d2436501b188":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc225823ce3a4f12aa6da03d32b84b9b","max":798156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_98fd6b540b284d3d904a547ebcde0722","value":798156}},"3be218e4154a4a788f2e26965e5de713":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b92bf817767f415bb7cb2303d13e84d2","placeholder":"​","style":"IPY_MODEL_ece738a463b24b9d96e1c0cf91704734","value":" 798k/798k [00:00&lt;00:00, 12.2MB/s]"}},"13eec7c2561649d1bc9c4fdf447b0ff2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d9fa5ae4998407a8545115e332b9125":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9b2fadde2514ee6969ad593b05b6d62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc225823ce3a4f12aa6da03d32b84b9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98fd6b540b284d3d904a547ebcde0722":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b92bf817767f415bb7cb2303d13e84d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ece738a463b24b9d96e1c0cf91704734":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"701aa01b4ca64770ab001dd9d11c96f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9f6c4e496fa41c5bed842aba7b1b4a1","IPY_MODEL_44f3de21be324279a195c3aeaa27641a","IPY_MODEL_df9655e3f5f44d73b382319222d7eb33"],"layout":"IPY_MODEL_3e8e50323376488f9045322822a51561"}},"c9f6c4e496fa41c5bed842aba7b1b4a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3297f1313ec9402c902f1aecda31da1a","placeholder":"​","style":"IPY_MODEL_a61c8e4a91d44670acfbbd3bef7417ab","value":"merges.txt: 100%"}},"44f3de21be324279a195c3aeaa27641a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fe6322cf888431aaaf211e4926eecf2","max":456356,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8597d93a700d48f5985d1d6055d3cfa5","value":456356}},"df9655e3f5f44d73b382319222d7eb33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5935aa254cc4bc6a8721d4cfa48c196","placeholder":"​","style":"IPY_MODEL_a5a7bee33d6c4eafa3f3d30bdb3ca1c1","value":" 456k/456k [00:00&lt;00:00, 31.5MB/s]"}},"3e8e50323376488f9045322822a51561":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3297f1313ec9402c902f1aecda31da1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a61c8e4a91d44670acfbbd3bef7417ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1fe6322cf888431aaaf211e4926eecf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8597d93a700d48f5985d1d6055d3cfa5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5935aa254cc4bc6a8721d4cfa48c196":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5a7bee33d6c4eafa3f3d30bdb3ca1c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f421d0bbd6f045a9a654ec2404ebf40f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_51e31bde5277465d91061286c323744b","IPY_MODEL_5c7c3d7bfa124ca6b6b5711e732a82f6","IPY_MODEL_bd0dc962e1e846c4a19e845ac328d811"],"layout":"IPY_MODEL_93d4abe1fe1c4208837ee4e9fbde680b"}},"51e31bde5277465d91061286c323744b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_948091d462d144e0a7ebcbf951127df6","placeholder":"​","style":"IPY_MODEL_c8a98ec306804c1eabe739e1c55ac417","value":"tokenizer.json: 100%"}},"5c7c3d7bfa124ca6b6b5711e732a82f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd631d773342456a9ff6d68d113c7648","max":1355446,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36eac42e0e4f451c9df653d38f434954","value":1355446}},"bd0dc962e1e846c4a19e845ac328d811":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57eabb5961f44b12b658bc45e1bca163","placeholder":"​","style":"IPY_MODEL_769264a4be3d47098371cfacb7715dfd","value":" 1.36M/1.36M [00:00&lt;00:00, 5.26MB/s]"}},"93d4abe1fe1c4208837ee4e9fbde680b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"948091d462d144e0a7ebcbf951127df6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8a98ec306804c1eabe739e1c55ac417":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd631d773342456a9ff6d68d113c7648":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36eac42e0e4f451c9df653d38f434954":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57eabb5961f44b12b658bc45e1bca163":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"769264a4be3d47098371cfacb7715dfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5335eee75ef246879df2b2cb6478533e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c376b3b5fbaa45a78d64b854f5344075","IPY_MODEL_1cf23ac87c314923baed285af1b0a15b","IPY_MODEL_92f8986c755c4178add16dfd0b7997be"],"layout":"IPY_MODEL_625db63f671f46a78d93bf46fe08e34e"}},"c376b3b5fbaa45a78d64b854f5344075":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a41d432f4e74becb87bb56e0e417b7d","placeholder":"​","style":"IPY_MODEL_260f04ec29cb4f11894ce65ed0ad104e","value":"special_tokens_map.json: 100%"}},"1cf23ac87c314923baed285af1b0a15b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_586c3987456644118c019065cd0fc400","max":120,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94693be2eb4043f39a054cead871cde6","value":120}},"92f8986c755c4178add16dfd0b7997be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f02963d4a0a04c7e9859e4b9461ada31","placeholder":"​","style":"IPY_MODEL_ae50f152d61b489eba704abba61b6c9d","value":" 120/120 [00:00&lt;00:00, 10.5kB/s]"}},"625db63f671f46a78d93bf46fe08e34e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a41d432f4e74becb87bb56e0e417b7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"260f04ec29cb4f11894ce65ed0ad104e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"586c3987456644118c019065cd0fc400":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94693be2eb4043f39a054cead871cde6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f02963d4a0a04c7e9859e4b9461ada31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae50f152d61b489eba704abba61b6c9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RNUO6vCWTUJ","executionInfo":{"status":"ok","timestamp":1733315991232,"user_tz":-540,"elapsed":19880,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"28e0b72a-dd9f-4292-c7d2-2b5b3ac5993c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Colab Notebooks/COSE474/FinalProject/20242R0136COSE47402/FinalProject"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C9PC8pmKWhmJ","executionInfo":{"status":"ok","timestamp":1733315994928,"user_tz":-540,"elapsed":844,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"0ea722c1-5388-4923-98de-735d4eebc326"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/COSE474/FinalProject/20242R0136COSE47402/FinalProject\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90AAC8aJXRdr","executionInfo":{"status":"ok","timestamp":1733315997818,"user_tz":-540,"elapsed":319,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"281d4edc-0ead-46b1-ffd1-b37d1f139c90"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["captions_val2017.json  instances_val2017.json  self_attention_layer1_forward.py  TestCode.ipynb\n"]}]},{"cell_type":"markdown","source":["# Preparation"],"metadata":{"id":"thMbL8dehGHE"}},{"cell_type":"code","source":["import json\n","import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt"],"metadata":{"id":"B70qqUwQiY5L","executionInfo":{"status":"ok","timestamp":1733316004644,"user_tz":-540,"elapsed":3469,"user":{"displayName":"김유진","userId":"04225960051398709816"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##Loading CLIP model\n","Training 없이 모델 구조 기반의 inference 실험을 할 것이기에 가장 작은 모델 사용.\n","CLIP 자체로는 이미지 caption을 생성하지 않기에 image captioning model을 추가로 사용."],"metadata":{"id":"PCKTGl-KfdN8"}},{"cell_type":"code","source":["from PIL import Image\n","import requests\n","from transformers import CLIPProcessor, CLIPModel, VisionEncoderDecoderModel, AutoTokenizer\n","\n","# Load the CLIP model and processor\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","\n","# Load the image captioning model\n","caption_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n","caption_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["207c6878016c4b8db2c588484ee5ae98","50f356271852486a85859bcfc3fe5006","9732b25679d44ad39b57510d6b3e5666","1afb250c196249ccb861b6a4dd180e2d","a9302f3d87fa4d7dad824922e4587988","9ff1bf1fcd364cd9b85947320e121212","10a861f88b16474ba086b72fa8079ffc","921c03295ee94d349d860ea018771562","23496bb0677649f1b00f5cb052b20855","6d9f2c48895844c980a7ff57c9890bab","e8b2ded7967d45c2868463167629fb8a","0aade5eb1df04d34b0496e0b9525cafd","79900d7d2a354a19be83d91908da5a85","1ad161116a434c5bbd162c27e0635212","f364ff7620544ed3b0326e39fe6d986e","8f2ab5f4a6e94bfda07d2763b80083da","c0b7e9033f4b41ce832b0435e33adf0b","346e109c8a864f3eb100784ac536a2da","d25e790f731c48dbbf367dc0c92d64be","cfe41c4e8c364804ba766fe4f441f778","f61cafb202da488885d3baccac6ab640","bd9dfcacd74e4a3d833874a4b12b3c70","81ade6f552114fc6917693e084925a3a","2cd2fb355e3949e79ced32f13c590ca8","c4da21a0d9f94baea442387364f9794f","ddd7e504ed59481cb55af1e902e5bd90","a921811d276e44b1a1bbf82e55b4ca5d","6d7f6f8e5fde49538aac6f2e91a6a099","b9754b4cf508416c92339534a5f52766","f2272904dde94d60979050a118204382","90587da309104dd3b1ecc3c42a824c46","f088793c3bda45a0a3b603bf382be821","22a6bb0fd2d842ff85abc9500c66251f","0c3de5cdeca34835bcdd5a8b89a27270","47ba7aa60d764fa482753f6abcdc38e4","f88481ee7bec4b0d9533580038e19548","0a105c2fc25f4f9cb2fd16f48cc2d36e","9aca16ff19d946ad8253c9363b9e5601","ab2ae39632c54fdbaae41bcbb4836015","0f89df7dc0d043449d528a26808e9bcc","36c2202879904a66857a83b4598c82c3","9d219de78793463b82e1230c0d31410b","f29459a4c24a4d94a8e1b30d3c6b2534","d879364f3dc0401db1ed6ca7eec9912f","36444a9b06fb4d04b9bfbb1dffc8319b","273eec5664684f318d1fb796e247f9f3","8e301e0e1c2742e09bc09ba1bc0806be","15dc2191119f4453bbf43d280668b20e","0b47057e945a45b3b83090593f7c5294","bb27b8b71140454ea554e9727b996669","9a029a1d47f44612a8d4a4856cfe9d28","aff030731a9f43c5b257cc0ccc36839d","21f218f3fd724c2fadf6bc65720ca6fb","e9ac8fffddfc414bb1df3c15907ebbb2","53781fd3d61a407dbc63a8b00afc1e0d","c3e1d0deb1f743e782d4071d997fef92","96a63ac5457246548d0506b1c43c39e6","a6b558e1a6c243af98425d9a1faea830","ffc87baaf61e4a128a0a8f04ad2fe368","90358bb9dc7f4dfca339ab131f5a5a80","f13df8019280480b93f99599b6109303","61c131df5d75421e86318023fef3cddb","d81e26af0ad34b2d9e07fe29dbaaaf44","b68a9c4d12b34209b5f792235eb06569","40bfecf1cea343d0b38898f660fff4eb","bfc4d42eab2d4a50bc0f9b57c1c279b2","0dff30123d444ddfb7025582c1a01c24","e335686f09e5489ba06713f6bf975516","2e67a43cc7df4472a69060666921a438","fe82014672fc435fa07228b02a18335b","5f423d5d42e24c65b4aedacd7dd649bc","8dcc429b1e2d4dd89348cecf203ba87e","c19722f88cac466d9052282bbc847264","234641e834764678a3cba75e27cb2041","829d5bc38b1b4d31a7f75441eecb9d51","e047e7d37058410188c72f410722567a","803dd42f788b469f95e5054acc2f9d62","95c7a0c02039439ba2868888447fa231","c71a2ce163cf4b2e9cadb7f9b517600a","c46cf18b106c4b08ba8a25b2c1a40b6f","ba2a7fc62fa0495ebbb96b051d313cf8","6ac8dd0f13ad4e76b1ff61509e289a3f","03fade87f04b4b249cd734aed8db5f1f","ec47e20ed8f24de992d34ef7e95d2b33","ccc3007c4c184606815d4eea3c9e6c68","da54085c072b458783fda9ea35699e9a","874e0e63e38b4091b03321fd3da47b27","87bcd061c248461a9442602e314e79ee","448212f279d44dd8a981f35d7c0805e3","47b10df4b003463abaf082f492ea8f54","f6bac7515bd74318ba047ec4b6909ba0","2f9c87774a3d4916a04bfd9194eeb8b1","509395a6585a46899b80dae980b48cb4","d334c51933934d8ab5aca971c0fed16a","8f44d552615f4b44a45e810d28b21d95","4f2b3080a10f47d585b3ebbbdaf54bc3","4d67c56f77054ad98da16231f0d10618","bdc51a0afc634d809735201e48247583","bbec1284655844fb80390431d4dc6949","645cd6cefe9d4a21ac949b53c4bb8358","3ecfb70403d34f4fa4efaed9faf9e728","a3f2713fb511458ba4bbd1eaf1ac23f9","63a5e009575445d9a52eb5c46f4aa4aa","48c12f3e05944b50baf3541060b1d51b","e8601f6769cd472481566b28ca6115ee","246a14fac9564979a76f6e28e117b7e6","4eb660f0476e4b7ca08441598476075b","be926a9924044421bb30b254b58f71d3","4d5632687e064ef2b7e01324ce6c3d7a","b982f1782c4c4ebf9ad4233309070fb2","d95f2a9ee89d4d9b998c300051c4b655","dd897f4a65b044beacec5c5e079ad021","464cc01cc4dc4a6d9895c520602d677b","41f305b0345948a6b979e236872d5e38","42aad04d36664a22880a0e315335f862","90961fc4f62948e9a99c8d7deba74a3c","885cb41153704e29b00b183bc51d8530","d5d3625c3b8b4d2084b89854a1b0b6c6","09a44fb8d782460faeb75b2d85094e08","90798b420dca48e29346eb003ebb1d5e","467d81b5d44e499588a1dc20d5cb028f","e509c7c2c1384c8696b17015d2e7bca0","093181d2ba974e7db17660bf1f9d3c01","2cd2fd9c98f148808b20d2436501b188","3be218e4154a4a788f2e26965e5de713","13eec7c2561649d1bc9c4fdf447b0ff2","8d9fa5ae4998407a8545115e332b9125","c9b2fadde2514ee6969ad593b05b6d62","dc225823ce3a4f12aa6da03d32b84b9b","98fd6b540b284d3d904a547ebcde0722","b92bf817767f415bb7cb2303d13e84d2","ece738a463b24b9d96e1c0cf91704734","701aa01b4ca64770ab001dd9d11c96f5","c9f6c4e496fa41c5bed842aba7b1b4a1","44f3de21be324279a195c3aeaa27641a","df9655e3f5f44d73b382319222d7eb33","3e8e50323376488f9045322822a51561","3297f1313ec9402c902f1aecda31da1a","a61c8e4a91d44670acfbbd3bef7417ab","1fe6322cf888431aaaf211e4926eecf2","8597d93a700d48f5985d1d6055d3cfa5","e5935aa254cc4bc6a8721d4cfa48c196","a5a7bee33d6c4eafa3f3d30bdb3ca1c1","f421d0bbd6f045a9a654ec2404ebf40f","51e31bde5277465d91061286c323744b","5c7c3d7bfa124ca6b6b5711e732a82f6","bd0dc962e1e846c4a19e845ac328d811","93d4abe1fe1c4208837ee4e9fbde680b","948091d462d144e0a7ebcbf951127df6","c8a98ec306804c1eabe739e1c55ac417","dd631d773342456a9ff6d68d113c7648","36eac42e0e4f451c9df653d38f434954","57eabb5961f44b12b658bc45e1bca163","769264a4be3d47098371cfacb7715dfd","5335eee75ef246879df2b2cb6478533e","c376b3b5fbaa45a78d64b854f5344075","1cf23ac87c314923baed285af1b0a15b","92f8986c755c4178add16dfd0b7997be","625db63f671f46a78d93bf46fe08e34e","4a41d432f4e74becb87bb56e0e417b7d","260f04ec29cb4f11894ce65ed0ad104e","586c3987456644118c019065cd0fc400","94693be2eb4043f39a054cead871cde6","f02963d4a0a04c7e9859e4b9461ada31","ae50f152d61b489eba704abba61b6c9d"]},"collapsed":true,"id":"GrLXBCHJvXNQ","executionInfo":{"status":"ok","timestamp":1733316036813,"user_tz":-540,"elapsed":29666,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"1b838129-9f57-4d06-96de-a84966c84335"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"207c6878016c4b8db2c588484ee5ae98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aade5eb1df04d34b0496e0b9525cafd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ade6f552114fc6917693e084925a3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c3de5cdeca34835bcdd5a8b89a27270"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36444a9b06fb4d04b9bfbb1dffc8319b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3e1d0deb1f743e782d4071d997fef92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dff30123d444ddfb7025582c1a01c24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c7a0c02039439ba2868888447fa231"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/4.61k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448212f279d44dd8a981f35d7c0805e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/982M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"645cd6cefe9d4a21ac949b53c4bb8358"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n","  \"architectures\": [\n","    \"ViTModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"qkv_bias\": true,\n","  \"transformers_version\": \"4.46.2\"\n","}\n","\n","Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n","  \"activation_function\": \"gelu_new\",\n","  \"add_cross_attention\": true,\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"decoder_start_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"is_decoder\": true,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 50256,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.46.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/241 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95f2a9ee89d4d9b998c300051c4b655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e509c7c2c1384c8696b17015d2e7bca0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701aa01b4ca64770ab001dd9d11c96f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f421d0bbd6f045a9a654ec2404ebf40f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5335eee75ef246879df2b2cb6478533e"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Fetching Cat and Dog images from MSCOCO dataset\n","instances_val2027.json으로부터 얻은 강아지와 고양이의 id로 강아지와 고양이 이미지만 뽑기"],"metadata":{"id":"v0TEOS-Qf1ng"}},{"cell_type":"code","source":["## fetch cat&dog images from MSCOCO\n","mscoco_instances_path = \"instances_val2017.json\"\n","mscoco_captions_path = \"captions_val2017.json\"\n","\n","# 강아지와 고양이의 category_id\n","target_categories = {17: \"cat\", 18: \"dog\"}\n","\n","# 필터링된 이미지 ID 저장\n","target_image_ids = set()\n","\n","# Load instances JSON to filter images\n","with open(mscoco_instances_path, \"r\") as f:\n","    instances_data = json.load(f)\n","\n","# Annotation에서 강아지와 고양이가 포함된 이미지 필터링\n","for annotation in instances_data[\"annotations\"]:\n","    if annotation[\"category_id\"] in target_categories:\n","        target_image_ids.add(annotation[\"image_id\"])\n","\n","total_images = len(target_image_ids)\n","print(f\"Number of images with cats or dogs: {total_images}\")\n","\n","# Load captions JSON\n","with open(mscoco_captions_path, \"r\") as f:\n","    captions_data = json.load(f)\n","\n","# Extract image and caption information\n","images_info = {img[\"id\"]: img for img in captions_data[\"images\"]}\n","annotations = captions_data[\"annotations\"]\n","\n","# Group captions by image ID\n","captions_by_image = {}\n","for ann in annotations:\n","    if ann[\"image_id\"] in target_image_ids:  # Filter only target image IDs\n","        captions_by_image.setdefault(ann[\"image_id\"], []).append(ann[\"caption\"])\n","# print(captions_by_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6TTyXEI_uOH","executionInfo":{"status":"ok","timestamp":1733316136509,"user_tz":-540,"elapsed":2151,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"42deb863-9cd4-4ff2-c045-18d223d35210"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images with cats or dogs: 349\n"]}]},{"cell_type":"markdown","source":["#**Original CLIP**\n"],"metadata":{"id":"_cFtbWr2SsOn"}},{"cell_type":"markdown","source":["### MSCOCO Dataset 결과"],"metadata":{"id":"4yh3Vcjaimo1"}},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","# Initialize variables for average score calculation\n","total_similarity_score = 0.0\n","processed_images = 0\n","# num_images = 50  # Number of images to process (adjust as needed)\n","num_images = total_images\n","\n","# Candidate captions\n","candidate_captions = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a bird\", \"a photo of a car\"]\n","# candidate_captions = [\"a photo of a white cat\", \"a photo of a black cat\", \"a photo of a white dog\", \"a photo of a black dog\",\"a photo of an animal\"]\n","\n","# Loop through filtered images\n","for image_id, captions in tqdm(list(captions_by_image.items())[:num_images]):\n","    # Load image from URL\n","    image_url = images_info[image_id][\"coco_url\"]\n","    print(image_url)\n","    image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n","\n","    # plt.imshow(image)\n","    # plt.title(f\"Image ID: {image_id}\")\n","    # plt.axis(\"off\")\n","    # plt.show()\n","\n","    # CLIP score for candidate captions\n","    inputs = clip_processor(text=candidate_captions, images=image, return_tensors=\"pt\", padding=True)\n","    outputs = clip_model(**inputs)\n","\n","    # Compute similarity scores\n","    logits_per_image = outputs.logits_per_image\n","    print(logits_per_image)\n","    probs = logits_per_image.softmax(dim=1)\n","    best_caption_index = torch.argmax(probs).item()\n","    best_caption = candidate_captions[best_caption_index]\n","    print(f\"Best caption by CLIP for image {image_id}: {best_caption}\")\n","\n","    # # Calculate CLIP similarity score for best caption\n","    # pixel_values = clip_processor(images=image, return_tensors=\"pt\").pixel_values\n","    # image_features = clip_model.get_image_features(pixel_values)\n","\n","    # text_inputs = clip_processor(text=[best_caption], return_tensors=\"pt\", padding=True)\n","    # text_features = clip_model.get_text_features(**text_inputs)\n","\n","    # # Normalize embeddings\n","    # image_features = F.normalize(image_features, p=2, dim=1)\n","    # text_features = F.normalize(text_features, p=2, dim=1)\n","\n","    # # Cosine similarity\n","    # similarity_score = (image_features @ text_features.T).item()\n","    # print(f\"CLIP Similarity Score for image {image_id}: {similarity_score}\")\n","\n","    # Generate a detailed caption using the captioning model\n","    pixel_values = clip_processor(images=image, return_tensors=\"pt\").pixel_values\n","    attention_mask = torch.ones(pixel_values.shape[:2], dtype=torch.long)\n","\n","    generated_ids = caption_model.generate(\n","        pixel_values,\n","        attention_mask=attention_mask,  # Explicitly pass the attention mask\n","        max_length=50,                 # Set a custom maximum length for the output\n","        pad_token_id=caption_tokenizer.pad_token_id  # Use the tokenizer's pad token ID\n","    )\n","    generated_caption = caption_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","    print(f\"Generated Caption: {generated_caption}\")\n","\n","    # CLIP의 Best Caption을 Caption Generation 모델의 입력에 포함\n","    best_caption = candidate_captions[best_caption_index]\n","    input_ids = caption_tokenizer(best_caption, return_tensors=\"pt\").input_ids\n","\n","    generated_ids = caption_model.generate(\n","        pixel_values,\n","        input_ids=input_ids,  # CLIP의 Best Caption 반영\n","        attention_mask=attention_mask,\n","        max_length=50,\n","        pad_token_id=caption_tokenizer.pad_token_id\n","    )\n","    generated_caption = caption_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","    print(f\"Generated Caption with CLIP influence: {generated_caption}\\n\")\n","\n","    # Calculate CLIP similarity score for generated caption with clip influence\n","    pixel_values = clip_processor(images=image, return_tensors=\"pt\").pixel_values\n","    image_features = clip_model.get_image_features(pixel_values)\n","\n","    text_inputs = clip_processor(text=[generated_caption], return_tensors=\"pt\", padding=True)\n","    text_features = clip_model.get_text_features(**text_inputs)\n","\n","    # Normalize embeddings\n","    image_features = F.normalize(image_features, p=2, dim=1)\n","    text_features = F.normalize(text_features, p=2, dim=1)\n","\n","    # Cosine similarity\n","    similarity_score = (image_features @ text_features.T).item()\n","    print(f\"CLIP Similarity Score for image {image_id}: {similarity_score}\")\n","\n","\n","    # Update total score and processed image count\n","    total_similarity_score += similarity_score\n","    processed_images += 1\n","\n","# Calculate and print the average CLIP similarity score\n","average_similarity_score = total_similarity_score / processed_images\n","print(f\"\\nAverage CLIP Similarity Score for {processed_images} images: {average_similarity_score}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"i74njiWT3rn4","executionInfo":{"status":"ok","timestamp":1733316708571,"user_tz":-540,"elapsed":565727,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"0109bc85-3d97-4ce7-993b-c0d59248e71e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/349 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["http://images.cocodataset.org/val2017/000000046378.jpg\n"]},{"output_type":"stream","name":"stderr","text":["We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n","You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[27.1647, 21.2905, 25.7971, 20.3364]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 46378: a photo of a cat\n","Generated Caption: a cat with a bite out of it's mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 1/349 [00:02<11:50,  2.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat with a bite out of it \n","\n","CLIP Similarity Score for image 46378: 0.28870147466659546\n","http://images.cocodataset.org/val2017/000000172330.jpg\n","tensor([[27.8398, 22.3475, 21.1272, 25.0792]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 172330: a photo of a cat\n","Generated Caption: a cat standing on the side of a road \n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 2/349 [00:03<10:18,  1.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat standing on the sidewalk \n","\n","CLIP Similarity Score for image 172330: 0.28662464022636414\n","http://images.cocodataset.org/val2017/000000223747.jpg\n","tensor([[25.6788, 20.8276, 20.1105, 18.6999]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 223747: a photo of a cat\n","Generated Caption: a person laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 3/349 [00:04<09:05,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sleeping on a blanket \n","\n","CLIP Similarity Score for image 223747: 0.26075056195259094\n","http://images.cocodataset.org/val2017/000000482917.jpg\n","tensor([[19.2966, 24.3990, 18.5056, 17.7371]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 482917: a photo of a dog\n","Generated Caption: a person laying on the floor with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 4/349 [00:06<08:50,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a couch \n","\n","CLIP Similarity Score for image 482917: 0.24818943440914154\n","http://images.cocodataset.org/val2017/000000065485.jpg\n","tensor([[22.3763, 23.8554, 21.2655, 25.9772]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 65485: a photo of a car\n","Generated Caption: a red truck parked in a parking lot \n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|▏         | 5/349 [00:07<08:46,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a red and white truck \n","\n","CLIP Similarity Score for image 65485: 0.24997827410697937\n","http://images.cocodataset.org/val2017/000000255965.jpg\n","tensor([[28.2140, 21.2175, 21.1200, 19.6218]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 255965: a photo of a cat\n","Generated Caption: a cat sitting on a ledge looking at the camera \n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 6/349 [00:09<09:35,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a ledge \n","\n","CLIP Similarity Score for image 255965: 0.28217875957489014\n","http://images.cocodataset.org/val2017/000000010363.jpg\n","tensor([[25.7040, 19.7851, 19.3076, 26.1214]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 10363: a photo of a car\n","Generated Caption: a cat sitting on top of a car hood \n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 7/349 [00:11<09:32,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the hood \n","\n","CLIP Similarity Score for image 10363: 0.3113110661506653\n","http://images.cocodataset.org/val2017/000000558073.jpg\n","tensor([[27.4564, 20.7561, 21.1311, 19.5326]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 558073: a photo of a cat\n","Generated Caption: a cat sitting on a wall next to a window \n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 8/349 [00:13<09:05,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a wall \n","\n","CLIP Similarity Score for image 558073: 0.2828425168991089\n","http://images.cocodataset.org/val2017/000000565391.jpg\n","tensor([[25.6473, 23.2969, 20.6537, 27.4147]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 565391: a photo of a car\n","Generated Caption: a black and white bird is sitting on a car \n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 9/349 [00:14<09:02,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a bird on the hood \n","\n","CLIP Similarity Score for image 565391: 0.27331575751304626\n","http://images.cocodataset.org/val2017/000000304560.jpg\n","tensor([[28.9066, 21.9899, 21.9246, 20.5639]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 304560: a photo of a cat\n","Generated Caption: a cat sitting in the grass looking at the camera \n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 10/349 [00:16<09:04,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting in the grass \n","\n","CLIP Similarity Score for image 304560: 0.2950246334075928\n","http://images.cocodataset.org/val2017/000000153217.jpg\n","tensor([[26.2469, 21.2999, 20.7917, 22.5804]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 153217: a photo of a cat\n","Generated Caption: a black dog is running on a leash \n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 11/349 [00:17<08:38,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a motorcycle \n","\n","CLIP Similarity Score for image 153217: 0.35492998361587524\n","http://images.cocodataset.org/val2017/000000209747.jpg\n","tensor([[28.7438, 22.3337, 21.8336, 20.2948]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 209747: a photo of a cat\n","Generated Caption: a cat sitting in a sink with a toothbrush in it's mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 12/349 [00:19<08:39,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat in a sink \n","\n","CLIP Similarity Score for image 209747: 0.3003822863101959\n","http://images.cocodataset.org/val2017/000000134096.jpg\n","tensor([[26.9347, 19.9099, 21.1949, 17.3799]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 134096: a photo of a cat\n","Generated Caption: a cat sitting on a sink in a bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▎         | 13/349 [00:20<08:32,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a sink \n","\n","CLIP Similarity Score for image 134096: 0.31705912947654724\n","http://images.cocodataset.org/val2017/000000163155.jpg\n","tensor([[28.0319, 21.8981, 22.1978, 20.1267]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 163155: a photo of a cat\n","Generated Caption: a cat sitting on a wooden bench \n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 14/349 [00:22<09:09,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a wooden bench \n","\n","CLIP Similarity Score for image 163155: 0.2967631220817566\n","http://images.cocodataset.org/val2017/000000360943.jpg\n","tensor([[28.2885, 22.5454, 22.2238, 21.5301]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 360943: a photo of a cat\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 15/349 [00:24<08:59,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a window sill \n","\n","CLIP Similarity Score for image 360943: 0.3198605477809906\n","http://images.cocodataset.org/val2017/000000284623.jpg\n","tensor([[26.8981, 20.7666, 20.4656, 18.3180]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 284623: a photo of a cat\n","Generated Caption: a cat sitting on a sink next to a mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 16/349 [00:25<09:00,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a sink \n","\n","CLIP Similarity Score for image 284623: 0.2909753918647766\n","http://images.cocodataset.org/val2017/000000554002.jpg\n","tensor([[20.7786, 26.5601, 20.1653, 19.3836]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 554002: a photo of a dog\n","Generated Caption: a woman walking down a sidewalk with a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 17/349 [00:27<09:03,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a woman walking on a sidewalk \n","\n","CLIP Similarity Score for image 554002: 0.2853633463382721\n","http://images.cocodataset.org/val2017/000000023272.jpg\n","tensor([[25.3348, 19.6787, 20.7832, 27.6003]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 23272: a photo of a car\n","Generated Caption: a dog is sitting on the hood of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 18/349 [00:29<08:57,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the hood \n","\n","CLIP Similarity Score for image 23272: 0.30087584257125854\n","http://images.cocodataset.org/val2017/000000067213.jpg\n","tensor([[18.1208, 24.4022, 18.2142, 20.1146]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 67213: a photo of a dog\n","Generated Caption: a dog jumping in the air over a swimming pool \n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 19/349 [00:30<08:53,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog jumping in the air \n","\n","CLIP Similarity Score for image 67213: 0.32770514488220215\n","http://images.cocodataset.org/val2017/000000170893.jpg\n","tensor([[22.2117, 28.1660, 21.9364, 18.9100]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 170893: a photo of a dog\n","Generated Caption: a dog is sitting on the toilet in the bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 20/349 [00:32<08:36,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog in a bathroom \n","\n","CLIP Similarity Score for image 170893: 0.32273128628730774\n","http://images.cocodataset.org/val2017/000000107087.jpg\n","tensor([[27.7822, 21.2467, 21.9360, 26.5736]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 107087: a photo of a cat\n","Generated Caption: a cat sitting in the hood of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 21/349 [00:33<08:28,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting in a car window \n","\n","CLIP Similarity Score for image 107087: 0.35280799865722656\n","http://images.cocodataset.org/val2017/000000366884.jpg\n","tensor([[22.3151, 23.6026, 20.7662, 20.2871]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 366884: a photo of a dog\n","Generated Caption: a cat is sitting on a rug in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▋         | 22/349 [00:35<08:57,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a bed \n","\n","CLIP Similarity Score for image 366884: 0.237894669175148\n","http://images.cocodataset.org/val2017/000000139099.jpg\n","tensor([[20.8217, 22.2808, 18.5894, 22.7461]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 139099: a photo of a car\n","Generated Caption: a man in a hat is on a bike \n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 23/349 [00:37<08:52,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with people on it \n","\n","CLIP Similarity Score for image 139099: 0.23479725420475006\n","http://images.cocodataset.org/val2017/000000404484.jpg\n","tensor([[20.9305, 24.9304, 19.6680, 18.0982]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 404484: a photo of a dog\n","Generated Caption: a dog standing on a rug in front of a window \n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 24/349 [00:38<08:40,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog standing on a rug \n","\n","CLIP Similarity Score for image 404484: 0.25341299176216125\n","http://images.cocodataset.org/val2017/000000227044.jpg\n","tensor([[27.9679, 20.9625, 20.3563, 18.1515]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 227044: a photo of a cat\n","Generated Caption: a cat sitting on a sink in a bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 25/349 [00:40<08:36,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a sink \n","\n","CLIP Similarity Score for image 227044: 0.3118378221988678\n","http://images.cocodataset.org/val2017/000000472375.jpg\n","tensor([[17.6772, 23.0884, 16.4206, 19.8669]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 472375: a photo of a dog\n","Generated Caption: a dog is sitting on a motorcycle \n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 26/349 [00:41<08:14,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a motorcycle \n","\n","CLIP Similarity Score for image 472375: 0.3158929646015167\n","http://images.cocodataset.org/val2017/000000101762.jpg\n","tensor([[26.1958, 21.5026, 20.7444, 20.8357]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 101762: a photo of a cat\n","Generated Caption: a cat is standing on a bicycle tire \n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 27/349 [00:43<08:12,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a bicycle \n","\n","CLIP Similarity Score for image 101762: 0.33773496747016907\n","http://images.cocodataset.org/val2017/000000419974.jpg\n","tensor([[20.9023, 23.7031, 20.9684, 16.8877]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 419974: a photo of a dog\n","Generated Caption: a man in a kitchen cutting a cake \n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 28/349 [00:44<08:03,  1.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog in a kitchen \n","\n","CLIP Similarity Score for image 419974: 0.30645182728767395\n","http://images.cocodataset.org/val2017/000000167122.jpg\n","tensor([[24.1445, 21.9116, 22.4332, 26.3843]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 167122: a photo of a car\n","Generated Caption: a car is driving down a street at night \n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 29/349 [00:46<08:01,  1.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on a street at night \n","\n","CLIP Similarity Score for image 167122: 0.2832256257534027\n","http://images.cocodataset.org/val2017/000000365207.jpg\n","tensor([[21.3035, 23.0956, 27.5447, 26.5845]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 365207: a photo of a bird\n","Generated Caption: a mirror reflecting a car's reflection in the side view mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▊         | 30/349 [00:48<09:11,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a bird in the side mirror of a car \n","\n","CLIP Similarity Score for image 365207: 0.3765977621078491\n","http://images.cocodataset.org/val2017/000000209613.jpg\n","tensor([[18.7630, 22.3709, 18.9988, 19.2622]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 209613: a photo of a dog\n","Generated Caption: sheep standing on top of a grass covered field \n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 31/349 [00:50<09:13,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and sheep standing on a hillside \n","\n","CLIP Similarity Score for image 209613: 0.30711913108825684\n","http://images.cocodataset.org/val2017/000000386457.jpg\n","tensor([[26.9079, 21.0494, 20.1394, 18.2191]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 386457: a photo of a cat\n","Generated Caption: a cat looking out of a window \n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 32/349 [00:51<08:44,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat looking through a door \n","\n","CLIP Similarity Score for image 386457: 0.30645328760147095\n","http://images.cocodataset.org/val2017/000000077396.jpg\n","tensor([[25.4239, 19.3640, 18.4102, 18.5086]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 77396: a photo of a cat\n","Generated Caption: a cat is watching a television on the floor \n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 33/349 [00:53<08:30,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat playing with a television \n","\n","CLIP Similarity Score for image 77396: 0.3452642261981964\n","http://images.cocodataset.org/val2017/000000312340.jpg\n","tensor([[28.1087, 21.2831, 22.5913, 20.4599]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 312340: a photo of a cat\n","Generated Caption: a cat sitting in a tree with a window \n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|▉         | 34/349 [00:54<08:18,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting in a tree \n","\n","CLIP Similarity Score for image 312340: 0.25167208909988403\n","http://images.cocodataset.org/val2017/000000530099.jpg\n","tensor([[26.2780, 19.3834, 20.0461, 25.3296]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 530099: a photo of a cat\n","Generated Caption: a cat sitting on a car window sill \n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 35/349 [00:55<08:00,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a car window \n","\n","CLIP Similarity Score for image 530099: 0.33628931641578674\n","http://images.cocodataset.org/val2017/000000149222.jpg\n","tensor([[20.8944, 19.9914, 20.4689, 20.0148]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 149222: a photo of a cat\n","Generated Caption: a computer monitor and keyboard on a desk \n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 36/349 [00:57<07:36,  1.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a desk \n","\n","CLIP Similarity Score for image 149222: 0.24378712475299835\n","http://images.cocodataset.org/val2017/000000329219.jpg\n","tensor([[23.3190, 26.7926, 20.1866, 17.3004]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 329219: a photo of a dog\n","Generated Caption: a man standing next to a dog on a kitchen floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 37/349 [00:58<07:55,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog standing on a kitchen floor \n","\n","CLIP Similarity Score for image 329219: 0.3382919728755951\n","http://images.cocodataset.org/val2017/000000245764.jpg\n","tensor([[27.5269, 21.4803, 21.5291, 18.5096]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 245764: a photo of a cat\n","Generated Caption: a cat is sitting on a toilet seat \n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 38/349 [01:00<08:28,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a toilet seat \n","\n","CLIP Similarity Score for image 245764: 0.331254780292511\n","http://images.cocodataset.org/val2017/000000131273.jpg\n","tensor([[19.1434, 25.6507, 18.5525, 22.2046]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 131273: a photo of a dog\n","Generated Caption: a dog sitting in the driver's seat of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 39/349 [01:02<08:26,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting in a car \n","\n","CLIP Similarity Score for image 131273: 0.2939833402633667\n","http://images.cocodataset.org/val2017/000000169076.jpg\n","tensor([[20.8876, 22.4029, 17.7046, 18.4235]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 169076: a photo of a dog\n","Generated Caption: a black and white dog looking at a tv \n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█▏        | 40/349 [01:03<08:13,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog looking at a tv \n","\n","CLIP Similarity Score for image 169076: 0.2954634428024292\n","http://images.cocodataset.org/val2017/000000466156.jpg\n","tensor([[27.0204, 20.1705, 20.2530, 26.5173]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 466156: a photo of a cat\n","Generated Caption: a cat standing on top of a car hood \n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 41/349 [01:05<08:02,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a car hood \n","\n","CLIP Similarity Score for image 466156: 0.3059234321117401\n","http://images.cocodataset.org/val2017/000000279278.jpg\n","tensor([[20.2024, 22.7803, 21.9742, 22.7798]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 279278: a photo of a dog\n","Generated Caption: a woman is standing on a skateboard with a skateboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 42/349 [01:07<08:06,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a skateboard \n","\n","CLIP Similarity Score for image 279278: 0.30676957964897156\n","http://images.cocodataset.org/val2017/000000579321.jpg\n","tensor([[20.4667, 25.9592, 20.7453, 18.0927]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 579321: a photo of a dog\n","Generated Caption: a dog laying on the ground with its head on a person's shoulder \n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 43/349 [01:08<08:20,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on the ground \n","\n","CLIP Similarity Score for image 579321: 0.29131466150283813\n","http://images.cocodataset.org/val2017/000000289343.jpg\n","tensor([[23.8884, 24.7396, 22.3759, 21.5734]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 289343: a photo of a dog\n","Generated Caption: a man riding a skateboard down a sidewalk \n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 44/349 [01:10<08:10,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog riding a skateboard \n","\n","CLIP Similarity Score for image 289343: 0.27582019567489624\n","http://images.cocodataset.org/val2017/000000498286.jpg\n","tensor([[19.8029, 26.8042, 20.4144, 19.7197]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 498286: a photo of a dog\n","Generated Caption: a dog with a tag on its face \n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 45/349 [01:12<08:38,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a tag on it \n","\n","CLIP Similarity Score for image 498286: 0.2566929757595062\n","http://images.cocodataset.org/val2017/000000494869.jpg\n","tensor([[21.4189, 23.5108, 19.7257, 17.8360]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 494869: a photo of a dog\n","Generated Caption: a woman and a child are in a kitchen \n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 46/349 [01:14<09:03,  1.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a woman in a kitchen \n","\n","CLIP Similarity Score for image 494869: 0.3451197147369385\n","http://images.cocodataset.org/val2017/000000129756.jpg\n","tensor([[18.6481, 24.0343, 18.5494, 17.6865]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 129756: a photo of a dog\n","Generated Caption: a man standing in a field with sheep \n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 47/349 [01:15<08:49,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a man standing in a field \n","\n","CLIP Similarity Score for image 129756: 0.316526859998703\n","http://images.cocodataset.org/val2017/000000568690.jpg\n","tensor([[28.1272, 22.1774, 22.6885, 18.8410]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 568690: a photo of a cat\n","Generated Caption: a cat sitting on top of a toilet bowl \n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 48/349 [01:17<08:23,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a toilet \n","\n","CLIP Similarity Score for image 568690: 0.34478387236595154\n","http://images.cocodataset.org/val2017/000000061471.jpg\n","tensor([[22.0624, 26.6198, 19.5719, 17.0698]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 61471: a photo of a dog\n","Generated Caption: a dog is standing in the bathroom with its mouth open \n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 49/349 [01:19<08:14,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog in a bathroom \n","\n","CLIP Similarity Score for image 61471: 0.32176893949508667\n","http://images.cocodataset.org/val2017/000000047121.jpg\n","tensor([[25.3859, 20.3359, 19.5930, 15.8769]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 47121: a photo of a cat\n","Generated Caption: a cat drinking water from a sink \n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 50/349 [01:20<07:59,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat drinking water from a faucet \n","\n","CLIP Similarity Score for image 47121: 0.3544974625110626\n","http://images.cocodataset.org/val2017/000000007386.jpg\n","tensor([[19.8024, 22.6190, 19.0395, 23.5750]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 7386: a photo of a car\n","Generated Caption: a motorcycle with a dog on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 51/349 [01:22<07:51,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a motorcycle on it \n","\n","CLIP Similarity Score for image 7386: 0.2785927355289459\n","http://images.cocodataset.org/val2017/000000267300.jpg\n","tensor([[18.0333, 24.4604, 18.9438, 17.2763]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 267300: a photo of a dog\n","Generated Caption: a dog laying on a bed with a plate of food \n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 52/349 [01:23<07:56,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 267300: 0.261127233505249\n","http://images.cocodataset.org/val2017/000000327769.jpg\n","tensor([[28.3038, 22.0456, 21.3527, 19.1429]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 327769: a photo of a cat\n","Generated Caption: a cat laying in a bathroom sink \n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 53/349 [01:25<08:11,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat in a bathroom sink \n","\n","CLIP Similarity Score for image 327769: 0.345694363117218\n","http://images.cocodataset.org/val2017/000000176778.jpg\n","tensor([[17.5528, 16.4859, 16.6052, 15.0838]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 176778: a photo of a cat\n","Generated Caption: a white toilet sitting next to a bath tub \n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 54/349 [01:27<08:24,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting in a bath tub \n","\n","CLIP Similarity Score for image 176778: 0.23207981884479523\n","http://images.cocodataset.org/val2017/000000260925.jpg\n","tensor([[26.2324, 20.0108, 19.2764, 26.4745]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 260925: a photo of a car\n","Generated Caption: a cat is laying on the hood of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 55/349 [01:28<08:16,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the hood \n","\n","CLIP Similarity Score for image 260925: 0.3044099807739258\n","http://images.cocodataset.org/val2017/000000520301.jpg\n","tensor([[20.4710, 26.9510, 20.5821, 23.3196]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 520301: a photo of a dog\n","Generated Caption: a dog is sitting in the window of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 56/349 [01:30<07:53,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a bus \n","\n","CLIP Similarity Score for image 520301: 0.307598739862442\n","http://images.cocodataset.org/val2017/000000205834.jpg\n","tensor([[20.5231, 27.7421, 20.1056, 19.3666]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 205834: a photo of a dog\n","Generated Caption: a dog is sitting in the dirt with a bowl \n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▋        | 57/349 [01:31<07:46,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting in the dirt \n","\n","CLIP Similarity Score for image 205834: 0.2764361798763275\n","http://images.cocodataset.org/val2017/000000491216.jpg\n","tensor([[27.2894, 25.2683, 21.6113, 19.2554]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 491216: a photo of a cat\n","Generated Caption: a cat is standing on the floor in a kitchen \n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 58/349 [01:33<07:30,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat in a kitchen \n","\n","CLIP Similarity Score for image 491216: 0.3428416848182678\n","http://images.cocodataset.org/val2017/000000109055.jpg\n","tensor([[26.5862, 21.4728, 20.8868, 21.6899]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 109055: a photo of a cat\n","Generated Caption: a cat is standing on a bicycle \n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 59/349 [01:34<07:15,  1.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a bicycle \n","\n","CLIP Similarity Score for image 109055: 0.345462441444397\n","http://images.cocodataset.org/val2017/000000501523.jpg\n","tensor([[28.3372, 21.9777, 21.4012, 19.3327]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 501523: a photo of a cat\n","Generated Caption: a cat is sitting in a bathroom sink \n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 60/349 [01:36<07:07,  1.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a sink \n","\n","CLIP Similarity Score for image 501523: 0.3339810073375702\n","http://images.cocodataset.org/val2017/000000125850.jpg\n","tensor([[27.2066, 21.6479, 19.5877, 20.7649]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 125850: a photo of a cat\n","Generated Caption: a cat laying in a bowl on a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 61/349 [01:37<07:23,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bowl \n","\n","CLIP Similarity Score for image 125850: 0.33204010128974915\n","http://images.cocodataset.org/val2017/000000213445.jpg\n","tensor([[27.6686, 21.5063, 21.0871, 19.0672]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 213445: a photo of a cat\n","Generated Caption: a cat sitting on a table in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 62/349 [01:39<08:00,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a table \n","\n","CLIP Similarity Score for image 213445: 0.3018765449523926\n","http://images.cocodataset.org/val2017/000000466339.jpg\n","tensor([[21.6656, 21.5137, 20.8571, 19.5698]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 466339: a photo of a cat\n","Generated Caption: a door is open to a room with a wooden floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 63/349 [01:41<08:12,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat standing in a doorway \n","\n","CLIP Similarity Score for image 466339: 0.26316142082214355\n","http://images.cocodataset.org/val2017/000000456292.jpg\n","tensor([[25.2134, 24.0728, 22.9028, 20.4290]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 456292: a photo of a cat\n","Generated Caption: a cat standing on a wall next to a graffiti covered wall \n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 64/349 [01:43<08:10,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat standing on a wall \n","\n","CLIP Similarity Score for image 456292: 0.2675565779209137\n","http://images.cocodataset.org/val2017/000000225184.jpg\n","tensor([[19.9464, 25.6293, 19.3498, 18.4023]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 225184: a photo of a dog\n","Generated Caption: a herd of sheep standing on top of a lush green field \n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▊        | 65/349 [01:45<08:07,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and sheep in a field \n","\n","CLIP Similarity Score for image 225184: 0.3313955068588257\n","http://images.cocodataset.org/val2017/000000176857.jpg\n","tensor([[26.1150, 21.0618, 19.8976, 17.7752]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 176857: a photo of a cat\n","Generated Caption: a woman is sitting on a bench in a museum \n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 66/349 [01:46<07:48,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a table \n","\n","CLIP Similarity Score for image 176857: 0.2871533930301666\n","http://images.cocodataset.org/val2017/000000402473.jpg\n","tensor([[25.7593, 18.6877, 18.8386, 18.4346]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 402473: a photo of a cat\n","Generated Caption: a cat laying on a wall next to a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 67/349 [01:48<07:40,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a cat laying on a wall \n","\n","CLIP Similarity Score for image 402473: 0.30476149916648865\n","http://images.cocodataset.org/val2017/000000211042.jpg\n","tensor([[26.5113, 20.8748, 21.6442, 17.1526]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 211042: a photo of a cat\n","Generated Caption: a cat standing on a toilet in a bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 68/349 [01:49<07:28,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a toilet \n","\n","CLIP Similarity Score for image 211042: 0.3099185526371002\n","http://images.cocodataset.org/val2017/000000361621.jpg\n","tensor([[28.1833, 22.5058, 22.1165, 19.8625]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 361621: a photo of a cat\n","Generated Caption: a cat is looking at the camera \n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|█▉        | 69/349 [01:51<07:17,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat with a cat face \n","\n","CLIP Similarity Score for image 361621: 0.2788461744785309\n","http://images.cocodataset.org/val2017/000000078823.jpg\n","tensor([[21.2651, 26.8026, 20.6323, 25.7334]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 78823: a photo of a dog\n","Generated Caption: a dog sitting on top of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 70/349 [01:53<07:35,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a car \n","\n","CLIP Similarity Score for image 78823: 0.3349987864494324\n","http://images.cocodataset.org/val2017/000000061108.jpg\n","tensor([[17.7649, 22.1415, 17.9437, 20.0315]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 61108: a photo of a dog\n","Generated Caption: a bike parked next to a bench \n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 71/349 [01:54<07:17,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a bike \n","\n","CLIP Similarity Score for image 61108: 0.30384355783462524\n","http://images.cocodataset.org/val2017/000000554291.jpg\n","tensor([[27.1981, 22.5356, 21.3535, 20.9228]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 554291: a photo of a cat\n","Generated Caption: a cat is playing with a toy in a bowl \n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 72/349 [01:55<07:08,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat in a bowl \n","\n","CLIP Similarity Score for image 554291: 0.2911188304424286\n","http://images.cocodataset.org/val2017/000000068078.jpg\n","tensor([[21.1262, 20.1331, 20.5615, 18.0097]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 68078: a photo of a cat\n","Generated Caption: a bathroom with a sink, toilet and mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 73/349 [01:57<07:04,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting in a bathroom sink \n","\n","CLIP Similarity Score for image 68078: 0.23462000489234924\n","http://images.cocodataset.org/val2017/000000424162.jpg\n","tensor([[18.2865, 24.9599, 18.1972, 21.5413]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 424162: a photo of a dog\n","Generated Caption: a woman riding a bike with a dog on a leash \n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 74/349 [01:59<07:14,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a woman riding a bike \n","\n","CLIP Similarity Score for image 424162: 0.341637521982193\n","http://images.cocodataset.org/val2017/000000157807.jpg\n","tensor([[27.5537, 20.4546, 20.2361, 17.4095]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 157807: a photo of a cat\n","Generated Caption: a cat standing on top of a toilet bowl \n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██▏       | 75/349 [02:00<07:02,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat standing on a toilet \n","\n","CLIP Similarity Score for image 157807: 0.3587303161621094\n","http://images.cocodataset.org/val2017/000000060835.jpg\n","tensor([[20.4175, 24.5423, 21.5427, 17.8367]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 60835: a photo of a dog\n","Generated Caption: a dog is standing in a cage with a fence \n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 76/349 [02:02<07:02,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog in a cage \n","\n","CLIP Similarity Score for image 60835: 0.30958133935928345\n","http://images.cocodataset.org/val2017/000000240940.jpg\n","tensor([[26.3424, 21.0491, 20.6053, 23.0383]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 240940: a photo of a cat\n","Generated Caption: a cat sitting on the floor watching television \n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 77/349 [02:03<07:12,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a rug watching television \n","\n","CLIP Similarity Score for image 240940: 0.3194321095943451\n","http://images.cocodataset.org/val2017/000000118515.jpg\n","tensor([[28.6044, 22.3098, 21.4719, 20.5310]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 118515: a photo of a cat\n","Generated Caption: a small kitten sitting on a wooden bench \n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 78/349 [02:05<07:39,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a wooden bench \n","\n","CLIP Similarity Score for image 118515: 0.3024660050868988\n","http://images.cocodataset.org/val2017/000000119233.jpg\n","tensor([[24.9192, 19.2914, 18.7075, 17.1159]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 119233: a photo of a cat\n","Generated Caption: a cat laying on top of a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 79/349 [02:07<07:20,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a laptop \n","\n","CLIP Similarity Score for image 119233: 0.29737651348114014\n","http://images.cocodataset.org/val2017/000000377575.jpg\n","tensor([[21.4850, 28.3612, 21.8824, 19.9910]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 377575: a photo of a dog\n","Generated Caption: a dog wearing a santa clause hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 80/349 [02:08<07:07,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog wearing a bear costume \n","\n","CLIP Similarity Score for image 377575: 0.24845848977565765\n","http://images.cocodataset.org/val2017/000000291664.jpg\n","tensor([[21.2185, 27.7100, 20.1470, 18.7186]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 291664: a photo of a dog\n","Generated Caption: a dog is standing next to a fire hydrant \n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 81/349 [02:10<07:16,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a fire hydrant \n","\n","CLIP Similarity Score for image 291664: 0.35832223296165466\n","http://images.cocodataset.org/val2017/000000222235.jpg\n","tensor([[25.8974, 19.2419, 20.9562, 16.8762]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 222235: a photo of a cat\n","Generated Caption: a cat sitting in a tree next to a plant \n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 82/349 [02:12<07:10,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat in a garden \n","\n","CLIP Similarity Score for image 222235: 0.2969156801700592\n","http://images.cocodataset.org/val2017/000000117374.jpg\n","tensor([[26.5312, 21.4150, 26.5785, 20.0646]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 117374: a photo of a bird\n","Generated Caption: a cat is sitting on a tree branch \n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 83/349 [02:13<07:04,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a bird sitting on a tree branch \n","\n","CLIP Similarity Score for image 117374: 0.23678691685199738\n","http://images.cocodataset.org/val2017/000000080666.jpg\n","tensor([[28.8879, 21.8815, 21.6326, 20.8872]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 80666: a photo of a cat\n","Generated Caption: a cat sitting on a sidewalk next to a building \n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 84/349 [02:15<07:01,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a sidewalk \n","\n","CLIP Similarity Score for image 80666: 0.2949102520942688\n","http://images.cocodataset.org/val2017/000000088951.jpg\n","tensor([[22.9380, 25.2577, 21.9129, 20.8227]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 88951: a photo of a dog\n","Generated Caption: a man sitting on a bench next to a tree \n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 85/349 [02:16<07:12,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a bench \n","\n","CLIP Similarity Score for image 88951: 0.300060898065567\n","http://images.cocodataset.org/val2017/000000117525.jpg\n","tensor([[19.9764, 24.7558, 20.1685, 18.6517]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 117525: a photo of a dog\n","Generated Caption: a man with a dog wearing a christmas hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▍       | 86/349 [02:18<07:21,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a christmas hat on \n","\n","CLIP Similarity Score for image 117525: 0.2916255593299866\n","http://images.cocodataset.org/val2017/000000029393.jpg\n","tensor([[20.3685, 26.5571, 21.4054, 19.4718]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 29393: a photo of a dog\n","Generated Caption: a dog standing on top of a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▍       | 87/349 [02:20<07:11,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog standing on a ledge \n","\n","CLIP Similarity Score for image 29393: 0.30110621452331543\n","http://images.cocodataset.org/val2017/000000305343.jpg\n","tensor([[18.6389, 16.5163, 17.4946, 14.7659]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 305343: a photo of a cat\n","Generated Caption: two paintings of a man and a woman \n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 88/349 [02:21<06:55,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a vase \n","\n","CLIP Similarity Score for image 305343: 0.2186172604560852\n","http://images.cocodataset.org/val2017/000000532530.jpg\n","tensor([[16.8936, 17.9346, 18.3721, 18.6695]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 532530: a photo of a car\n","Generated Caption: a sign that says \"no parking\" on a street \n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 89/349 [02:23<06:50,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on a street \n","\n","CLIP Similarity Score for image 532530: 0.20036351680755615\n","http://images.cocodataset.org/val2017/000000224200.jpg\n","tensor([[23.1285, 28.2030, 21.8660, 21.0975]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 224200: a photo of a dog\n","Generated Caption: a cat sitting on the sidewalk next to a fire hydrant \n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 90/349 [02:24<06:51,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on the sidewalk \n","\n","CLIP Similarity Score for image 224200: 0.33355534076690674\n","http://images.cocodataset.org/val2017/000000361571.jpg\n","tensor([[21.2321, 28.2417, 20.1182, 19.4444]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 361571: a photo of a dog\n","Generated Caption: a dog wearing a hat and a hat hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 91/349 [02:26<06:44,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a hat on \n","\n","CLIP Similarity Score for image 361571: 0.31982508301734924\n","http://images.cocodataset.org/val2017/000000452891.jpg\n","tensor([[19.8755, 26.9428, 19.7322, 19.3870]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 452891: a photo of a dog\n","Generated Caption: a dog sitting on a bench with a leash \n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▋       | 92/349 [02:27<06:38,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a bench \n","\n","CLIP Similarity Score for image 452891: 0.3034544289112091\n","http://images.cocodataset.org/val2017/000000375493.jpg\n","tensor([[21.0063, 22.1575, 19.2720, 20.5414]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 375493: a photo of a dog\n","Generated Caption: a woman in a dress standing next to a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 93/349 [02:29<06:37,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog wearing a costume \n","\n","CLIP Similarity Score for image 375493: 0.2508750855922699\n","http://images.cocodataset.org/val2017/000000432553.jpg\n","tensor([[17.2565, 24.1820, 16.5266, 18.5423]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 432553: a photo of a dog\n","Generated Caption: a dog is standing on a leash \n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 94/349 [02:31<06:57,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a leash \n","\n","CLIP Similarity Score for image 432553: 0.25810107588768005\n","http://images.cocodataset.org/val2017/000000546829.jpg\n","tensor([[19.4171, 25.9201, 20.5510, 18.7192]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 546829: a photo of a dog\n","Generated Caption: a dog sitting on a bench in the woods \n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 95/349 [02:32<06:48,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a bench \n","\n","CLIP Similarity Score for image 546829: 0.3166244626045227\n","http://images.cocodataset.org/val2017/000000261161.jpg\n","tensor([[19.3535, 26.1800, 19.9641, 18.1717]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 261161: a photo of a dog\n","Generated Caption: a dog sitting on a bench next to a tree \n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 96/349 [02:34<06:47,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a bench \n","\n","CLIP Similarity Score for image 261161: 0.33028367161750793\n","http://images.cocodataset.org/val2017/000000475732.jpg\n","tensor([[24.1572, 19.7692, 18.4693, 18.3594]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 475732: a photo of a cat\n","Generated Caption: a cat wearing a hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 97/349 [02:35<06:26,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat wearing a hat \n","\n","CLIP Similarity Score for image 475732: 0.29638540744781494\n","http://images.cocodataset.org/val2017/000000372819.jpg\n","tensor([[15.9194, 24.5399, 16.8646, 18.1107]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 372819: a photo of a dog\n","Generated Caption: a dog running with a group of people \n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 98/349 [02:37<06:21,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a dog running together \n","\n","CLIP Similarity Score for image 372819: 0.2851381301879883\n","http://images.cocodataset.org/val2017/000000053529.jpg\n","tensor([[18.2199, 23.1006, 18.5787, 23.3075]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 53529: a photo of a car\n","Generated Caption: a dog is looking at a camera while standing in the street \n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 99/349 [02:39<06:34,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the hood \n","\n","CLIP Similarity Score for image 53529: 0.26636624336242676\n","http://images.cocodataset.org/val2017/000000108244.jpg\n","tensor([[26.2733, 20.8419, 20.0141, 19.6998]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 108244: a photo of a cat\n","Generated Caption: a cat laying in a box on top of a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▊       | 100/349 [02:40<06:27,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying in a box \n","\n","CLIP Similarity Score for image 108244: 0.2790294289588928\n","http://images.cocodataset.org/val2017/000000076417.jpg\n","tensor([[20.4853, 28.4904, 21.1380, 25.6674]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 76417: a photo of a dog\n","Generated Caption: a dog sitting in the drivers seat of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 101/349 [02:42<06:29,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting in a car \n","\n","CLIP Similarity Score for image 76417: 0.3029676377773285\n","http://images.cocodataset.org/val2017/000000320554.jpg\n","tensor([[26.4417, 21.6167, 21.2028, 18.8693]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 320554: a photo of a cat\n","Generated Caption: a cat sitting on a bench in the sun \n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 102/349 [02:43<06:43,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a bench \n","\n","CLIP Similarity Score for image 320554: 0.31316789984703064\n","http://images.cocodataset.org/val2017/000000277584.jpg\n","tensor([[27.2066, 21.1055, 21.4685, 20.3324]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 277584: a photo of a cat\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|██▉       | 103/349 [02:45<06:45,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a window sill \n","\n","CLIP Similarity Score for image 277584: 0.3215073049068451\n","http://images.cocodataset.org/val2017/000000089271.jpg\n","tensor([[26.1601, 21.1861, 19.6460, 19.8030]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 89271: a photo of a cat\n","Generated Caption: a cat wearing a hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|██▉       | 104/349 [02:47<06:26,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat wearing a hat \n","\n","CLIP Similarity Score for image 89271: 0.3092997968196869\n","http://images.cocodataset.org/val2017/000000286708.jpg\n","tensor([[26.4712, 20.9470, 20.5757, 19.3042]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 286708: a photo of a cat\n","Generated Caption: a cat wearing a hat and a bow tie \n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 105/349 [02:48<06:19,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat wearing a hat \n","\n","CLIP Similarity Score for image 286708: 0.31709620356559753\n","http://images.cocodataset.org/val2017/000000279145.jpg\n","tensor([[20.3567, 19.1997, 19.7071, 18.6119]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 279145: a photo of a cat\n","Generated Caption: a bench in a garden with plants growing out of it \n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 106/349 [02:50<06:25,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a bench \n","\n","CLIP Similarity Score for image 279145: 0.2556147873401642\n","http://images.cocodataset.org/val2017/000000236166.jpg\n","tensor([[20.5079, 23.4864, 17.9913, 17.5677]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 236166: a photo of a dog\n","Generated Caption: a man wearing a hat and a hat hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 107/349 [02:51<06:13,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog wearing a hat \n","\n","CLIP Similarity Score for image 236166: 0.2630099058151245\n","http://images.cocodataset.org/val2017/000000288685.jpg\n","tensor([[13.4622, 17.4040, 13.3010, 13.7964]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 288685: a photo of a dog\n","Generated Caption: a dog is running in a field with a group of people \n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 108/349 [02:53<06:22,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a group of people \n","\n","CLIP Similarity Score for image 288685: 0.21891126036643982\n","http://images.cocodataset.org/val2017/000000311190.jpg\n","tensor([[20.6288, 27.4112, 19.8926, 23.8745]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 311190: a photo of a dog\n","Generated Caption: a dog wearing a hat and a collar \n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 109/349 [02:54<06:09,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog wearing a hat \n","\n","CLIP Similarity Score for image 311190: 0.3237505853176117\n","http://images.cocodataset.org/val2017/000000413395.jpg\n","tensor([[25.4680, 20.5808, 19.1639, 18.3763]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 413395: a photo of a cat\n","Generated Caption: a man holding a cat on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 110/349 [02:56<06:22,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a man \n","\n","CLIP Similarity Score for image 413395: 0.30799880623817444\n","http://images.cocodataset.org/val2017/000000446522.jpg\n","tensor([[21.4465, 26.8628, 20.2026, 19.2626]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 446522: a photo of a dog\n","Generated Caption: a dog laying on a chair in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 111/349 [02:58<06:30,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a chair \n","\n","CLIP Similarity Score for image 446522: 0.32557711005210876\n","http://images.cocodataset.org/val2017/000000089880.jpg\n","tensor([[15.7130, 22.7038, 15.0783, 16.2534]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 89880: a photo of a dog\n","Generated Caption: a dog with a red collar and a black and white dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 112/349 [02:59<06:18,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a cat \n","\n","CLIP Similarity Score for image 89880: 0.21796762943267822\n","http://images.cocodataset.org/val2017/000000171611.jpg\n","tensor([[18.5920, 19.2172, 18.3609, 19.0985]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 171611: a photo of a dog\n","Generated Caption: a boat is docked at a pier with people on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 113/349 [03:01<06:27,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a boat in the water \n","\n","CLIP Similarity Score for image 171611: 0.25048232078552246\n","http://images.cocodataset.org/val2017/000000312213.jpg\n","tensor([[25.9406, 20.7068, 20.7146, 17.8793]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 312213: a photo of a cat\n","Generated Caption: a cat sitting on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 114/349 [03:02<06:07,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a laptop \n","\n","CLIP Similarity Score for image 312213: 0.3044966757297516\n","http://images.cocodataset.org/val2017/000000512836.jpg\n","tensor([[20.7411, 25.5801, 19.6955, 18.9556]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 512836: a photo of a dog\n","Generated Caption: a woman holding an umbrella while walking in the snow \n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 115/349 [03:04<06:07,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a person holding an umbrella \n","\n","CLIP Similarity Score for image 512836: 0.3085605204105377\n","http://images.cocodataset.org/val2017/000000222317.jpg\n","tensor([[20.7414, 26.0681, 19.9394, 18.8727]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 222317: a photo of a dog\n","Generated Caption: a dog laying on a couch with a stuffed animal \n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 116/349 [03:05<06:01,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a couch \n","\n","CLIP Similarity Score for image 222317: 0.2947212755680084\n","http://images.cocodataset.org/val2017/000000181969.jpg\n","tensor([[20.3377, 25.9356, 18.4342, 17.7453]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 181969: a photo of a dog\n","Generated Caption: a dog laying on a couch with a pillow \n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▎      | 117/349 [03:07<06:00,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a couch \n","\n","CLIP Similarity Score for image 181969: 0.23861472308635712\n","http://images.cocodataset.org/val2017/000000574315.jpg\n","tensor([[26.8921, 20.7254, 20.9436, 17.7153]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 574315: a photo of a cat\n","Generated Caption: a cat sitting on top of a computer keyboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 118/349 [03:09<06:00,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a keyboard \n","\n","CLIP Similarity Score for image 574315: 0.3261670470237732\n","http://images.cocodataset.org/val2017/000000177015.jpg\n","tensor([[25.0828, 19.7404, 19.1613, 16.8435]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 177015: a photo of a cat\n","Generated Caption: a man is sitting on a couch with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 119/349 [03:11<06:31,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a man's lap \n","\n","CLIP Similarity Score for image 177015: 0.3281262218952179\n","http://images.cocodataset.org/val2017/000000210099.jpg\n","tensor([[28.4571, 22.4096, 22.5120, 20.9411]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 210099: a photo of a cat\n","Generated Caption: a cat sitting on a chair in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 120/349 [03:12<06:15,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a chair \n","\n","CLIP Similarity Score for image 210099: 0.3294951021671295\n","http://images.cocodataset.org/val2017/000000574810.jpg\n","tensor([[28.1173, 22.8755, 22.8983, 19.4180]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 574810: a photo of a cat\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▍      | 121/349 [03:14<06:01,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat looking out a window \n","\n","CLIP Similarity Score for image 574810: 0.31189364194869995\n","http://images.cocodataset.org/val2017/000000197528.jpg\n","tensor([[26.7164, 21.4900, 22.3003, 20.1339]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 197528: a photo of a cat\n","Generated Caption: a cat looking out a window \n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▍      | 122/349 [03:15<05:45,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat looking out a window \n","\n","CLIP Similarity Score for image 197528: 0.30303898453712463\n","http://images.cocodataset.org/val2017/000000116825.jpg\n","tensor([[25.8691, 19.7537, 20.2250, 18.2914]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 116825: a photo of a cat\n","Generated Caption: a cat laying on a table next to a remote \n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▌      | 123/349 [03:16<05:32,  1.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a table \n","\n","CLIP Similarity Score for image 116825: 0.26170092821121216\n","http://images.cocodataset.org/val2017/000000246454.jpg\n","tensor([[19.1244, 26.0332, 18.4103, 18.4381]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 246454: a photo of a dog\n","Generated Caption: a woman petting a dog on the street \n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 124/349 [03:18<05:30,  1.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a woman \n","\n","CLIP Similarity Score for image 246454: 0.292329341173172\n","http://images.cocodataset.org/val2017/000000517832.jpg\n","tensor([[18.7278, 25.0144, 17.2820, 16.1767]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 517832: a photo of a dog\n","Generated Caption: a dog sitting on a chair with a pillow \n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 125/349 [03:19<05:39,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a chair \n","\n","CLIP Similarity Score for image 517832: 0.32010823488235474\n","http://images.cocodataset.org/val2017/000000401991.jpg\n","tensor([[18.9286, 23.5439, 16.5897, 15.5150]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 401991: a photo of a dog\n","Generated Caption: a dog laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 126/349 [03:21<05:43,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 401991: 0.27521875500679016\n","http://images.cocodataset.org/val2017/000000126110.jpg\n","tensor([[21.5074, 23.4193, 19.8070, 22.5581]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 126110: a photo of a dog\n","Generated Caption: a dog is looking out of a truck window \n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▋      | 127/349 [03:23<05:50,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog in a van \n","\n","CLIP Similarity Score for image 126110: 0.2794337868690491\n","http://images.cocodataset.org/val2017/000000079229.jpg\n","tensor([[20.0209, 24.4379, 19.5500, 20.7952]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 79229: a photo of a dog\n","Generated Caption: a man riding a horse in a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 128/349 [03:24<05:42,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a horse \n","\n","CLIP Similarity Score for image 79229: 0.30825427174568176\n","http://images.cocodataset.org/val2017/000000520531.jpg\n","tensor([[26.0438, 20.1085, 20.4992, 17.7122]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 520531: a photo of a cat\n","Generated Caption: a cat sitting on a desk next to a computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 129/349 [03:26<05:39,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a desk \n","\n","CLIP Similarity Score for image 520531: 0.3040279448032379\n","http://images.cocodataset.org/val2017/000000179392.jpg\n","tensor([[25.7028, 19.7483, 19.8310, 16.8838]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 179392: a photo of a cat\n","Generated Caption: a black cat sitting on a desk with a lamp \n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 130/349 [03:27<05:33,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a desk \n","\n","CLIP Similarity Score for image 179392: 0.27749985456466675\n","http://images.cocodataset.org/val2017/000000145781.jpg\n","tensor([[20.7939, 26.0857, 19.4650, 19.1286]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 145781: a photo of a dog\n","Generated Caption: a dog laying on the ground with a bottle of water \n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 131/349 [03:29<05:47,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on the ground with a bottle \n","\n","CLIP Similarity Score for image 145781: 0.3081369400024414\n","http://images.cocodataset.org/val2017/000000551815.jpg\n","tensor([[24.9344, 18.6506, 18.7467, 18.7646]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 551815: a photo of a cat\n","Generated Caption: a cat laying on top of a pile of stuffed animals \n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 132/349 [03:31<05:53,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a cat laying on a blanket \n","\n","CLIP Similarity Score for image 551815: 0.2993311285972595\n","http://images.cocodataset.org/val2017/000000316015.jpg\n","tensor([[24.2200, 18.2960, 19.6640, 16.9563]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 316015: a photo of a cat\n","Generated Caption: a cat sitting on a desk next to a computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 133/349 [03:32<05:45,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a desk \n","\n","CLIP Similarity Score for image 316015: 0.29595065116882324\n","http://images.cocodataset.org/val2017/000000525247.jpg\n","tensor([[25.6784, 20.6162, 21.4243, 17.0710]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 525247: a photo of a cat\n","Generated Caption: a cat sitting on top of a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 134/349 [03:34<05:46,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a laptop \n","\n","CLIP Similarity Score for image 525247: 0.31230318546295166\n","http://images.cocodataset.org/val2017/000000030494.jpg\n","tensor([[21.5974, 26.4950, 22.1181, 18.1909]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 30494: a photo of a dog\n","Generated Caption: a cat is running through the woods \n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▊      | 135/349 [03:36<06:03,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog running through a forest \n","\n","CLIP Similarity Score for image 30494: 0.3087993562221527\n","http://images.cocodataset.org/val2017/000000025560.jpg\n","tensor([[25.3741, 19.4912, 19.8547, 16.9773]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 25560: a photo of a cat\n","Generated Caption: a cat sitting on top of a television \n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 136/349 [03:37<05:48,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on top of a television \n","\n","CLIP Similarity Score for image 25560: 0.3170267343521118\n","http://images.cocodataset.org/val2017/000000329319.jpg\n","tensor([[27.7101, 21.3928, 23.0761, 18.7969]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 329319: a photo of a cat\n","Generated Caption: a cat sitting on a wooden bench \n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 137/349 [03:39<05:40,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a wooden bench \n","\n","CLIP Similarity Score for image 329319: 0.31248733401298523\n","http://images.cocodataset.org/val2017/000000416256.jpg\n","tensor([[28.5704, 22.2089, 21.1622, 19.7481]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 416256: a photo of a cat\n","Generated Caption: a cat laying on top of a computer keyboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|███▉      | 138/349 [03:40<05:32,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a computer keyboard \n","\n","CLIP Similarity Score for image 416256: 0.3281266689300537\n","http://images.cocodataset.org/val2017/000000292330.jpg\n","tensor([[17.6723, 24.5528, 18.6061, 16.2146]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 292330: a photo of a dog\n","Generated Caption: a dog running with a baseball bat on a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|███▉      | 139/349 [03:42<05:30,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog catching a ball \n","\n","CLIP Similarity Score for image 292330: 0.28546106815338135\n","http://images.cocodataset.org/val2017/000000235399.jpg\n","tensor([[17.6040, 22.6607, 17.3004, 20.6298]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 235399: a photo of a dog\n","Generated Caption: a dog laying on the bed of a truck \n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 140/349 [03:43<05:24,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 235399: 0.24294117093086243\n","http://images.cocodataset.org/val2017/000000565962.jpg\n","tensor([[25.6055, 22.1524, 19.1473, 21.8221]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 565962: a photo of a cat\n","Generated Caption: a black bear is sitting in the middle of a forest \n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 141/349 [03:45<05:15,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat in the woods \n","\n","CLIP Similarity Score for image 565962: 0.2508801519870758\n","http://images.cocodataset.org/val2017/000000364297.jpg\n","tensor([[25.7964, 20.2395, 19.7981, 17.4665]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 364297: a photo of a cat\n","Generated Caption: a cat is laying on a computer keyboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 142/349 [03:46<05:14,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a computer keyboard \n","\n","CLIP Similarity Score for image 364297: 0.3410225808620453\n","http://images.cocodataset.org/val2017/000000077595.jpg\n","tensor([[27.5349, 20.8259, 20.5566, 19.8831]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 77595: a photo of a cat\n","Generated Caption: a cat laying on a bed with a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 143/349 [03:48<05:35,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bed \n","\n","CLIP Similarity Score for image 77595: 0.283578485250473\n","http://images.cocodataset.org/val2017/000000219578.jpg\n","tensor([[24.0810, 24.1658, 16.2815, 18.7907]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 219578: a photo of a dog\n","Generated Caption: a cat laying on a couch next to a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████▏     | 144/349 [03:50<05:37,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and cat sleeping on a couch \n","\n","CLIP Similarity Score for image 219578: 0.34503671526908875\n","http://images.cocodataset.org/val2017/000000407083.jpg\n","tensor([[17.9571, 23.9319, 18.1160, 21.7668]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 407083: a photo of a dog\n","Generated Caption: a dog is sitting in the driver's seat of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 145/349 [03:52<05:51,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog in a car with a man in the back seat \n","\n","CLIP Similarity Score for image 407083: 0.2949092984199524\n","http://images.cocodataset.org/val2017/000000411665.jpg\n","tensor([[27.8351, 21.6276, 20.8953, 20.3430]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 411665: a photo of a cat\n","Generated Caption: a cat is looking at itself in a mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 146/349 [03:53<05:35,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat looking at itself in a mirror \n","\n","CLIP Similarity Score for image 411665: 0.34828412532806396\n","http://images.cocodataset.org/val2017/000000078565.jpg\n","tensor([[19.0129, 21.2429, 18.0968, 19.0450]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 78565: a photo of a dog\n","Generated Caption: people are on the beach with sailboats \n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 147/349 [03:55<05:29,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a sailboat in the ocean \n","\n","CLIP Similarity Score for image 78565: 0.2623397409915924\n","http://images.cocodataset.org/val2017/000000261706.jpg\n","tensor([[24.3190, 18.0273, 18.9829, 17.3992]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 261706: a photo of a cat\n","Generated Caption: a cat is sitting on a couch with a remote \n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 148/349 [03:56<05:24,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a couch \n","\n","CLIP Similarity Score for image 261706: 0.27419888973236084\n","http://images.cocodataset.org/val2017/000000297830.jpg\n","tensor([[17.7781, 23.5694, 17.9742, 15.8922]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 297830: a photo of a dog\n","Generated Caption: a dog with a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 149/349 [03:58<05:34,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a frisbee in its mouth \n","\n","CLIP Similarity Score for image 297830: 0.18879692256450653\n","http://images.cocodataset.org/val2017/000000425390.jpg\n","tensor([[25.8996, 19.2452, 19.9855, 17.5365]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 425390: a photo of a cat\n","Generated Caption: a cat laying on top of a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 150/349 [04:00<05:29,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a laptop \n","\n","CLIP Similarity Score for image 425390: 0.315390944480896\n","http://images.cocodataset.org/val2017/000000309484.jpg\n","tensor([[19.6736, 25.7002, 18.0662, 18.2449]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 309484: a photo of a dog\n","Generated Caption: a dog chewing on a person's hand \n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 151/349 [04:01<05:25,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog chewing on a toothbrush \n","\n","CLIP Similarity Score for image 309484: 0.3218463361263275\n","http://images.cocodataset.org/val2017/000000433134.jpg\n","tensor([[28.0812, 22.4191, 22.2952, 18.8617]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 433134: a photo of a cat\n","Generated Caption: a cat is standing on the grass with its eyes closed \n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▎     | 152/349 [04:03<05:28,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a grassy field \n","\n","CLIP Similarity Score for image 433134: 0.31241995096206665\n","http://images.cocodataset.org/val2017/000000241326.jpg\n","tensor([[23.4256, 22.0270, 17.6164, 19.2061]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 241326: a photo of a cat\n","Generated Caption: a black cat laying on a couch next to a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 153/349 [04:05<05:23,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a couch \n","\n","CLIP Similarity Score for image 241326: 0.2779662013053894\n","http://images.cocodataset.org/val2017/000000107226.jpg\n","tensor([[13.4061, 20.8886, 13.9862, 17.1412]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 107226: a photo of a dog\n","Generated Caption: a dog is standing in the grass with a group of people \n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 154/349 [04:06<05:23,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a man on a bike \n","\n","CLIP Similarity Score for image 107226: 0.2670271098613739\n","http://images.cocodataset.org/val2017/000000094336.jpg\n","tensor([[27.2490, 19.9609, 20.2693, 18.1942]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 94336: a photo of a cat\n","Generated Caption: a cat sitting in a sink \n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 155/349 [04:08<05:03,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting in a sink \n","\n","CLIP Similarity Score for image 94336: 0.3226466774940491\n","http://images.cocodataset.org/val2017/000000555705.jpg\n","tensor([[25.8102, 18.7622, 18.0975, 19.3575]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 555705: a photo of a cat\n","Generated Caption: a cat laying on a wooden floor next to a shoe \n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▍     | 156/349 [04:09<05:02,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a cat toy \n","\n","CLIP Similarity Score for image 555705: 0.2588161528110504\n","http://images.cocodataset.org/val2017/000000161609.jpg\n","tensor([[19.0509, 25.9413, 18.9829, 17.2718]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 161609: a photo of a dog\n","Generated Caption: a dog wearing a hat is walking down the stairs \n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▍     | 157/349 [04:11<04:58,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog wearing a hat \n","\n","CLIP Similarity Score for image 161609: 0.2585430443286896\n","http://images.cocodataset.org/val2017/000000462728.jpg\n","tensor([[16.8372, 20.6520, 18.7704, 18.5466]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 462728: a photo of a dog\n","Generated Caption: a person riding a surfboard on top of a wave \n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▌     | 158/349 [04:13<05:14,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog riding a wave on a surfboard \n","\n","CLIP Similarity Score for image 462728: 0.298147976398468\n","http://images.cocodataset.org/val2017/000000524280.jpg\n","tensor([[28.8132, 22.9849, 23.4293, 20.3612]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 524280: a photo of a cat\n","Generated Caption: a cat with a green eyes staring at the camera \n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 159/349 [04:15<05:32,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat with a green eye \n","\n","CLIP Similarity Score for image 524280: 0.30273517966270447\n","http://images.cocodataset.org/val2017/000000251572.jpg\n","tensor([[19.1946, 26.2125, 18.5345, 18.4887]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 251572: a photo of a dog\n","Generated Caption: a woman laying on a couch with a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 160/349 [04:16<05:20,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a woman's lap \n","\n","CLIP Similarity Score for image 251572: 0.2935684025287628\n","http://images.cocodataset.org/val2017/000000181859.jpg\n","tensor([[27.4894, 20.7294, 19.9489, 17.4309]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 181859: a photo of a cat\n","Generated Caption: a cat laying in a sink in a bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 161/349 [04:18<05:03,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying in a sink \n","\n","CLIP Similarity Score for image 181859: 0.3365851640701294\n","http://images.cocodataset.org/val2017/000000001675.jpg\n","tensor([[25.9821, 19.9230, 19.5991, 18.2907]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 1675: a photo of a cat\n","Generated Caption: a cat sitting on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▋     | 162/349 [04:19<04:49,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a computer \n","\n","CLIP Similarity Score for image 1675: 0.32517534494400024\n","http://images.cocodataset.org/val2017/000000190140.jpg\n","tensor([[18.6781, 23.1140, 17.5046, 18.7022]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 190140: a photo of a dog\n","Generated Caption: a dog on a boat with a man on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 47%|████▋     | 163/349 [04:21<04:53,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a boat with a man \n","\n","CLIP Similarity Score for image 190140: 0.329497754573822\n","http://images.cocodataset.org/val2017/000000182805.jpg\n","tensor([[16.9471, 22.9408, 17.0367, 16.1035]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 182805: a photo of a dog\n","Generated Caption: a woman holding a dog under her arm \n"]},{"output_type":"stream","name":"stderr","text":["\r 47%|████▋     | 164/349 [04:22<04:52,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a woman with a umbrella \n","\n","CLIP Similarity Score for image 182805: 0.32587388157844543\n","http://images.cocodataset.org/val2017/000000138492.jpg\n","tensor([[22.6233, 26.9940, 22.0845, 18.7637]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 138492: a photo of a dog\n","Generated Caption: a black and white dog catching a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 47%|████▋     | 165/349 [04:24<05:03,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog catching a frisbee in its mouth \n","\n","CLIP Similarity Score for image 138492: 0.3570716083049774\n","http://images.cocodataset.org/val2017/000000291490.jpg\n","tensor([[26.1309, 20.1466, 19.0796, 19.1088]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 291490: a photo of a cat\n","Generated Caption: a cat is sitting on a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 166/349 [04:26<04:59,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat with a cat head \n","\n","CLIP Similarity Score for image 291490: 0.24487905204296112\n","http://images.cocodataset.org/val2017/000000071226.jpg\n","tensor([[21.7560, 22.7017, 14.7135, 17.0658]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 71226: a photo of a dog\n","Generated Caption: a dog laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 167/349 [04:27<05:00,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 71226: 0.2720123529434204\n","http://images.cocodataset.org/val2017/000000131938.jpg\n","tensor([[25.0711, 20.9222, 19.7590, 18.5404]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 131938: a photo of a cat\n","Generated Caption: a cat wearing a tie sitting on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 168/349 [04:29<04:52,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat wearing a tie \n","\n","CLIP Similarity Score for image 131938: 0.35291972756385803\n","http://images.cocodataset.org/val2017/000000434996.jpg\n","tensor([[25.9003, 20.7183, 18.4876, 20.0390]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 434996: a photo of a cat\n","Generated Caption: a cat laying on a blanket next to stuffed animals \n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 169/349 [04:30<04:43,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sleeping on a blanket \n","\n","CLIP Similarity Score for image 434996: 0.2853890359401703\n","http://images.cocodataset.org/val2017/000000398810.jpg\n","tensor([[27.6634, 21.3320, 21.8940, 20.6323]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 398810: a photo of a cat\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 49%|████▊     | 170/349 [04:32<04:39,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a window sill \n","\n","CLIP Similarity Score for image 398810: 0.29383599758148193\n","http://images.cocodataset.org/val2017/000000237864.jpg\n","tensor([[19.7447, 22.6515, 18.9398, 19.3298]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 237864: a photo of a dog\n","Generated Caption: a small elephant walking across a dirt field \n"]},{"output_type":"stream","name":"stderr","text":["\r 49%|████▉     | 171/349 [04:34<04:37,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and an elephant in a field \n","\n","CLIP Similarity Score for image 237864: 0.32096225023269653\n","http://images.cocodataset.org/val2017/000000139872.jpg\n","tensor([[19.6256, 25.5370, 19.1730, 18.4544]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 139872: a photo of a dog\n","Generated Caption: a dog is playing with a frisbee in the grass \n"]},{"output_type":"stream","name":"stderr","text":["\r 49%|████▉     | 172/349 [04:35<04:51,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a frisbee in its mouth \n","\n","CLIP Similarity Score for image 139872: 0.308616042137146\n","http://images.cocodataset.org/val2017/000000106389.jpg\n","tensor([[26.0253, 20.2554, 19.9094, 18.2680]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 106389: a photo of a cat\n","Generated Caption: a cat sitting on a chair in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|████▉     | 173/349 [04:37<04:41,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a chair \n","\n","CLIP Similarity Score for image 106389: 0.3128989338874817\n","http://images.cocodataset.org/val2017/000000140203.jpg\n","tensor([[21.0341, 22.2971, 20.5269, 26.5121]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 140203: a photo of a car\n","Generated Caption: a white and blue truck parked in a parking lot \n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|████▉     | 174/349 [04:39<04:40,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car parked in a parking lot \n","\n","CLIP Similarity Score for image 140203: 0.25806668400764465\n","http://images.cocodataset.org/val2017/000000081766.jpg\n","tensor([[16.3112, 23.9930, 16.7488, 17.8799]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 81766: a photo of a dog\n","Generated Caption: a black dog sitting on a wooden bench \n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 175/349 [04:40<04:53,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a bench \n","\n","CLIP Similarity Score for image 81766: 0.2759988307952881\n","http://images.cocodataset.org/val2017/000000318908.jpg\n","tensor([[20.7500, 25.8610, 20.4943, 17.9954]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 318908: a photo of a dog\n","Generated Caption: a small white dog wearing a pink bow tie \n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 176/349 [04:42<04:47,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog wearing a pink bow tie \n","\n","CLIP Similarity Score for image 318908: 0.2500787377357483\n","http://images.cocodataset.org/val2017/000000131131.jpg\n","tensor([[25.2928, 18.7415, 18.3708, 16.9481]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 131131: a photo of a cat\n","Generated Caption: a cat is sitting on a computer screen \n"]},{"output_type":"stream","name":"stderr","text":["\r 51%|█████     | 177/349 [04:43<04:35,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a computer screen \n","\n","CLIP Similarity Score for image 131131: 0.2679438591003418\n","http://images.cocodataset.org/val2017/000000004795.jpg\n","tensor([[23.7719, 18.6425, 18.5351, 15.8085]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 4795: a photo of a cat\n","Generated Caption: a cat sitting on a computer desk \n"]},{"output_type":"stream","name":"stderr","text":["\r 51%|█████     | 178/349 [04:45<04:25,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a computer keyboard \n","\n","CLIP Similarity Score for image 4795: 0.2806375026702881\n","http://images.cocodataset.org/val2017/000000115885.jpg\n","tensor([[27.6258, 22.1603, 22.7781, 18.7388]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 115885: a photo of a cat\n","Generated Caption: a cat is laying on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 51%|█████▏    | 179/349 [04:46<04:25,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a laptop \n","\n","CLIP Similarity Score for image 115885: 0.33043476939201355\n","http://images.cocodataset.org/val2017/000000490171.jpg\n","tensor([[17.3121, 24.7870, 17.2135, 17.5424]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 490171: a photo of a dog\n","Generated Caption: a dog on a surfboard in the water \n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 180/349 [04:48<04:25,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a surfboard in the ocean \n","\n","CLIP Similarity Score for image 490171: 0.3285575807094574\n","http://images.cocodataset.org/val2017/000000219485.jpg\n","tensor([[19.8864, 20.0129, 19.6191, 20.2414]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 219485: a photo of a car\n","Generated Caption: a window with a sign on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 181/349 [04:49<04:15,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a building \n","\n","CLIP Similarity Score for image 219485: 0.2827305197715759\n","http://images.cocodataset.org/val2017/000000078426.jpg\n","tensor([[26.8343, 20.9348, 20.7019, 18.6110]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 78426: a photo of a cat\n","Generated Caption: a cat sitting on a desk next to a book \n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 182/349 [04:51<04:13,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a desk \n","\n","CLIP Similarity Score for image 78426: 0.33079904317855835\n","http://images.cocodataset.org/val2017/000000049269.jpg\n","tensor([[18.4715, 25.5444, 17.7855, 19.2099]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 49269: a photo of a dog\n","Generated Caption: a brown and white dog and a black and white dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 183/349 [04:53<04:33,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a horse \n","\n","CLIP Similarity Score for image 49269: 0.3400617837905884\n","http://images.cocodataset.org/val2017/000000515025.jpg\n","tensor([[23.7100, 18.2615, 18.2460, 17.6755]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 515025: a photo of a cat\n","Generated Caption: a man is standing on a table with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 53%|█████▎    | 184/349 [04:54<04:26,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a table \n","\n","CLIP Similarity Score for image 515025: 0.22861528396606445\n","http://images.cocodataset.org/val2017/000000403817.jpg\n","tensor([[25.7809, 20.1846, 20.3586, 17.3810]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 403817: a photo of a cat\n","Generated Caption: a cat sitting on a computer desk \n"]},{"output_type":"stream","name":"stderr","text":["\r 53%|█████▎    | 185/349 [04:56<04:16,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a computer desk \n","\n","CLIP Similarity Score for image 403817: 0.3157097101211548\n","http://images.cocodataset.org/val2017/000000424545.jpg\n","tensor([[27.3829, 20.3664, 20.7499, 19.7063]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 424545: a photo of a cat\n","Generated Caption: a cat is sitting on a counter looking at something \n"]},{"output_type":"stream","name":"stderr","text":["\r 53%|█████▎    | 186/349 [04:57<04:15,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat with a cat's head \n","\n","CLIP Similarity Score for image 424545: 0.26086893677711487\n","http://images.cocodataset.org/val2017/000000545826.jpg\n","tensor([[24.4871, 19.9693, 19.0381, 17.6224]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 545826: a photo of a cat\n","Generated Caption: a cat is playing with a toy on a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▎    | 187/349 [04:59<04:15,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat with a cat toy on its head \n","\n","CLIP Similarity Score for image 545826: 0.3076097369194031\n","http://images.cocodataset.org/val2017/000000187236.jpg\n","tensor([[23.9312, 17.5603, 16.7714, 17.1947]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 187236: a photo of a cat\n","Generated Caption: a cat is laying on the floor looking at something \n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 188/349 [05:01<04:12,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on the floor \n","\n","CLIP Similarity Score for image 187236: 0.2307867854833603\n","http://images.cocodataset.org/val2017/000000343076.jpg\n","tensor([[25.9893, 19.3919, 20.1973, 18.1660]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 343076: a photo of a cat\n","Generated Caption: a cat sitting on top of a wooden table \n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 189/349 [05:02<04:05,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a computer \n","\n","CLIP Similarity Score for image 343076: 0.30163654685020447\n","http://images.cocodataset.org/val2017/000000049810.jpg\n","tensor([[27.8882, 22.3454, 23.0359, 21.0118]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 49810: a photo of a cat\n","Generated Caption: a cat sitting on a wooden bench \n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 190/349 [05:04<04:03,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a wooden bench \n","\n","CLIP Similarity Score for image 49810: 0.2720097005367279\n","http://images.cocodataset.org/val2017/000000014831.jpg\n","tensor([[29.0866, 24.5290, 24.3954, 21.1916]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 14831: a photo of a cat\n","Generated Caption: a cat is sleeping on a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▍    | 191/349 [05:06<04:21,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sleeping on a blanket \n","\n","CLIP Similarity Score for image 14831: 0.2929430603981018\n","http://images.cocodataset.org/val2017/000000186296.jpg\n","tensor([[26.8798, 20.3483, 19.8202, 19.3399]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 186296: a photo of a cat\n","Generated Caption: a cat laying on the ground next to a pair of shoes \n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▌    | 192/349 [05:07<04:19,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on the ground \n","\n","CLIP Similarity Score for image 186296: 0.2769957184791565\n","http://images.cocodataset.org/val2017/000000416330.jpg\n","tensor([[27.4266, 22.2699, 20.9564, 19.3040]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 416330: a photo of a cat\n","Generated Caption: a cat is laying on a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▌    | 193/349 [05:09<04:11,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat with a white and black stripe \n","\n","CLIP Similarity Score for image 416330: 0.2847975194454193\n","http://images.cocodataset.org/val2017/000000357941.jpg\n","tensor([[27.6227, 22.0493, 23.0425, 20.4491]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 357941: a photo of a cat\n","Generated Caption: a tv with a cat on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 194/349 [05:10<03:58,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a tv \n","\n","CLIP Similarity Score for image 357941: 0.3491106331348419\n","http://images.cocodataset.org/val2017/000000063552.jpg\n","tensor([[27.3408, 21.4572, 21.1035, 19.9688]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 63552: a photo of a cat\n","Generated Caption: a cat is looking at itself in a mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 195/349 [05:12<03:55,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat in a mirror \n","\n","CLIP Similarity Score for image 63552: 0.32919400930404663\n","http://images.cocodataset.org/val2017/000000015497.jpg\n","tensor([[26.5953, 20.6092, 20.4042, 19.7442]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 15497: a photo of a cat\n","Generated Caption: a cat laying on a desk next to a mouse \n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 196/349 [05:13<03:49,  1.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a computer \n","\n","CLIP Similarity Score for image 15497: 0.29500699043273926\n","http://images.cocodataset.org/val2017/000000399560.jpg\n","tensor([[24.7690, 16.7474, 17.3641, 17.2016]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 399560: a photo of a cat\n","Generated Caption: a cat is sitting in a bowl on the floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▋    | 197/349 [05:15<03:49,  1.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting in a bowl \n","\n","CLIP Similarity Score for image 399560: 0.2719201147556305\n","http://images.cocodataset.org/val2017/000000098839.jpg\n","tensor([[24.9943, 19.1413, 19.6500, 16.7257]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 98839: a photo of a cat\n","Generated Caption: a cat sitting on a television screen \n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 198/349 [05:16<03:44,  1.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a television \n","\n","CLIP Similarity Score for image 98839: 0.3098776340484619\n","http://images.cocodataset.org/val2017/000000278463.jpg\n","tensor([[24.9819, 21.1878, 20.4115, 19.5721]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 278463: a photo of a cat\n","Generated Caption: a laptop computer sitting on top of a wooden table \n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 199/349 [05:18<03:53,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a wooden table \n","\n","CLIP Similarity Score for image 278463: 0.24777919054031372\n","http://images.cocodataset.org/val2017/000000575357.jpg\n","tensor([[18.9066, 25.7677, 21.0291, 19.8239]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 575357: a photo of a dog\n","Generated Caption: a dog running through a field with a red tag \n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 200/349 [05:19<04:01,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog running through a field \n","\n","CLIP Similarity Score for image 575357: 0.31332650780677795\n","http://images.cocodataset.org/val2017/000000366199.jpg\n","tensor([[26.2890, 20.8581, 21.0542, 18.2609]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 366199: a photo of a cat\n","Generated Caption: a cat sleeping on a blanket on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 201/349 [05:21<03:54,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sleeping on a blanket \n","\n","CLIP Similarity Score for image 366199: 0.24693061411380768\n","http://images.cocodataset.org/val2017/000000286422.jpg\n","tensor([[18.4244, 20.1179, 18.0667, 19.0219]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 286422: a photo of a dog\n","Generated Caption: a white boat with a black and white sail on a lake \n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 202/349 [05:23<03:54,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a sailboat \n","\n","CLIP Similarity Score for image 286422: 0.3061825931072235\n","http://images.cocodataset.org/val2017/000000112798.jpg\n","tensor([[25.8284, 19.9106, 19.9862, 18.3413]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 112798: a photo of a cat\n","Generated Caption: a cat laying on a table with a lot of clutter \n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 203/349 [05:24<03:53,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a table \n","\n","CLIP Similarity Score for image 112798: 0.2820810079574585\n","http://images.cocodataset.org/val2017/000000407960.jpg\n","tensor([[24.1069, 18.9328, 18.0147, 17.8341]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 407960: a photo of a cat\n","Generated Caption: a cat sitting on the floor looking out the window \n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 204/349 [05:26<03:49,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on the floor \n","\n","CLIP Similarity Score for image 407960: 0.22567205131053925\n","http://images.cocodataset.org/val2017/000000572388.jpg\n","tensor([[22.9050, 20.3695, 22.2163, 16.9875]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 572388: a photo of a cat\n","Generated Caption: a stuffed bear with a stuffed bear in it's mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 59%|█████▊    | 205/349 [05:27<03:49,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a stuffed animal \n","\n","CLIP Similarity Score for image 572388: 0.25164300203323364\n","http://images.cocodataset.org/val2017/000000051008.jpg\n","tensor([[24.2602, 19.3261, 20.2147, 17.1306]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 51008: a photo of a cat\n","Generated Caption: a cat is sitting on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 59%|█████▉    | 206/349 [05:29<03:40,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a laptop \n","\n","CLIP Similarity Score for image 51008: 0.30299535393714905\n","http://images.cocodataset.org/val2017/000000387383.jpg\n","tensor([[23.4740, 16.7922, 17.2693, 18.4948]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 387383: a photo of a cat\n","Generated Caption: a cat laying on a bed with a cat on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 59%|█████▉    | 207/349 [05:31<03:54,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bed \n","\n","CLIP Similarity Score for image 387383: 0.2662561535835266\n","http://images.cocodataset.org/val2017/000000311789.jpg\n","tensor([[25.5462, 19.7592, 19.5629, 17.6378]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 311789: a photo of a cat\n","Generated Caption: a cat laying on a wooden floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|█████▉    | 208/349 [05:32<03:52,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a wooden floor \n","\n","CLIP Similarity Score for image 311789: 0.23237308859825134\n","http://images.cocodataset.org/val2017/000000080153.jpg\n","tensor([[17.7655, 22.6555, 17.2918, 18.1581]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 80153: a photo of a dog\n","Generated Caption: a man riding skis on top of a snow covered slope \n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|█████▉    | 209/349 [05:34<03:55,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a leash in the snow \n","\n","CLIP Similarity Score for image 80153: 0.2859708368778229\n","http://images.cocodataset.org/val2017/000000189806.jpg\n","tensor([[22.9100, 22.5293, 16.1792, 17.8416]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 189806: a photo of a cat\n","Generated Caption: a cat is sitting on the floor next to a cat laying on the floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 210/349 [05:36<03:58,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a cat laying on the floor \n","\n","CLIP Similarity Score for image 189806: 0.2722196877002716\n","http://images.cocodataset.org/val2017/000000392818.jpg\n","tensor([[17.7795, 24.8771, 16.8077, 19.0633]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 392818: a photo of a dog\n","Generated Caption: a dog wearing a collar and tie \n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 211/349 [05:37<03:42,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog wearing a collar \n","\n","CLIP Similarity Score for image 392818: 0.26679089665412903\n","http://images.cocodataset.org/val2017/000000084650.jpg\n","tensor([[24.8849, 19.0704, 18.6936, 19.6170]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 84650: a photo of a cat\n","Generated Caption: a cat laying on a suitcase on the floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 61%|██████    | 212/349 [05:39<03:35,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a suitcase \n","\n","CLIP Similarity Score for image 84650: 0.29964032769203186\n","http://images.cocodataset.org/val2017/000000509403.jpg\n","tensor([[20.1529, 24.5841, 20.4295, 18.7143]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 509403: a photo of a dog\n","Generated Caption: a man and a child playing with a frisbee in a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 61%|██████    | 213/349 [05:41<03:41,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog running through a field \n","\n","CLIP Similarity Score for image 509403: 0.24690061807632446\n","http://images.cocodataset.org/val2017/000000193162.jpg\n","tensor([[16.4240, 19.1118, 15.0914, 18.8957]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 193162: a photo of a dog\n","Generated Caption: a man is standing in a field with a cow \n"]},{"output_type":"stream","name":"stderr","text":["\r 61%|██████▏   | 214/349 [05:42<03:39,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a cow in a field \n","\n","CLIP Similarity Score for image 193162: 0.2845371663570404\n","http://images.cocodataset.org/val2017/000000432468.jpg\n","tensor([[26.3284, 20.9576, 20.5426, 20.4690]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 432468: a photo of a cat\n","Generated Caption: a cat is sitting on a suitcase \n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 215/349 [05:44<03:42,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat with a backpack \n","\n","CLIP Similarity Score for image 432468: 0.29608580470085144\n","http://images.cocodataset.org/val2017/000000185250.jpg\n","tensor([[17.9166, 22.9455, 18.3621, 16.3377]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 185250: a photo of a dog\n","Generated Caption: a man in a baseball uniform is holding a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 216/349 [05:46<03:46,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a frisbee \n","\n","CLIP Similarity Score for image 185250: 0.26041415333747864\n","http://images.cocodataset.org/val2017/000000119828.jpg\n","tensor([[25.9276, 19.4166, 20.5654, 18.2150]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 119828: a photo of a cat\n","Generated Caption: a black cat sitting on a desk next to a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 217/349 [05:47<03:39,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a desk \n","\n","CLIP Similarity Score for image 119828: 0.3047422170639038\n","http://images.cocodataset.org/val2017/000000416170.jpg\n","tensor([[28.0692, 23.6532, 23.7304, 21.3694]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 416170: a photo of a cat\n","Generated Caption: a window with a metal frame and a fence \n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 218/349 [05:49<03:33,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat in a window \n","\n","CLIP Similarity Score for image 416170: 0.32659465074539185\n","http://images.cocodataset.org/val2017/000000080949.jpg\n","tensor([[25.6039, 19.8667, 19.0551, 18.0407]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 80949: a photo of a cat\n","Generated Caption: a cat laying on top of a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 63%|██████▎   | 219/349 [05:50<03:24,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a laptop \n","\n","CLIP Similarity Score for image 80949: 0.31713226437568665\n","http://images.cocodataset.org/val2017/000000155291.jpg\n","tensor([[26.7880, 20.2161, 20.2146, 18.3309]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 155291: a photo of a cat\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 63%|██████▎   | 220/349 [05:52<03:24,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a window sill \n","\n","CLIP Similarity Score for image 155291: 0.31220144033432007\n","http://images.cocodataset.org/val2017/000000174231.jpg\n","tensor([[25.8695, 19.5297, 20.2668, 19.5077]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 174231: a photo of a cat\n","Generated Caption: a cat laying on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 63%|██████▎   | 221/349 [05:53<03:16,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a laptop \n","\n","CLIP Similarity Score for image 174231: 0.3367147445678711\n","http://images.cocodataset.org/val2017/000000039769.jpg\n","tensor([[22.7387, 15.6618, 15.3255, 17.3027]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 39769: a photo of a cat\n","Generated Caption: a cat laying on a blanket with a cat on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▎   | 222/349 [05:55<03:19,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a cat laying on a blanket \n","\n","CLIP Similarity Score for image 39769: 0.2675136625766754\n","http://images.cocodataset.org/val2017/000000494634.jpg\n","tensor([[25.7285, 21.4569, 21.6192, 17.8059]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 494634: a photo of a cat\n","Generated Caption: a cat is laying on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 223/349 [05:57<03:26,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a laptop \n","\n","CLIP Similarity Score for image 494634: 0.316396564245224\n","http://images.cocodataset.org/val2017/000000469067.jpg\n","tensor([[24.2788, 18.4675, 18.4589, 17.4961]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 469067: a photo of a cat\n","Generated Caption: a woman laying on a bed with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 224/349 [05:58<03:22,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bed \n","\n","CLIP Similarity Score for image 469067: 0.2550966441631317\n","http://images.cocodataset.org/val2017/000000385205.jpg\n","tensor([[25.9222, 19.9126, 21.2783, 18.0727]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 385205: a photo of a cat\n","Generated Caption: a cat laying on a rug next to a stuffed animal \n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 225/349 [06:00<03:19,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a rug \n","\n","CLIP Similarity Score for image 385205: 0.25973597168922424\n","http://images.cocodataset.org/val2017/000000198641.jpg\n","tensor([[26.0021, 20.3454, 21.2652, 17.6458]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 198641: a photo of a cat\n","Generated Caption: a computer desk with a laptop and a monitor \n"]},{"output_type":"stream","name":"stderr","text":["\r 65%|██████▍   | 226/349 [06:01<03:14,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a desk \n","\n","CLIP Similarity Score for image 198641: 0.2930260896682739\n","http://images.cocodataset.org/val2017/000000549220.jpg\n","tensor([[21.9384, 26.7408, 20.3844, 21.7875]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 549220: a photo of a dog\n","Generated Caption: a dog is sitting on a skateboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 65%|██████▌   | 227/349 [06:03<03:08,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a skateboard \n","\n","CLIP Similarity Score for image 549220: 0.36739999055862427\n","http://images.cocodataset.org/val2017/000000338901.jpg\n","tensor([[20.9507, 26.5512, 20.2214, 19.3069]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 338901: a photo of a dog\n","Generated Caption: a dog laying on a couch with a pillow \n"]},{"output_type":"stream","name":"stderr","text":["\r 65%|██████▌   | 228/349 [06:04<03:04,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a couch \n","\n","CLIP Similarity Score for image 338901: 0.28781023621559143\n","http://images.cocodataset.org/val2017/000000297085.jpg\n","tensor([[25.2181, 22.0634, 21.9510, 18.1916]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 297085: a photo of a cat\n","Generated Caption: a tv is on in a room with a large screen \n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 229/349 [06:06<03:02,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a tv \n","\n","CLIP Similarity Score for image 297085: 0.31213679909706116\n","http://images.cocodataset.org/val2017/000000532575.jpg\n","tensor([[19.1212, 25.0290, 18.0664, 17.9516]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 532575: a photo of a dog\n","Generated Caption: a dog is looking out the window of a boat \n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 230/349 [06:07<03:01,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a hat on \n","\n","CLIP Similarity Score for image 532575: 0.2385752946138382\n","http://images.cocodataset.org/val2017/000000409867.jpg\n","tensor([[26.7831, 22.1834, 22.2664, 20.9910]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 409867: a photo of a cat\n","Generated Caption: a window with a bunch of colorful kites in it \n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 231/349 [06:09<03:08,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting in a window \n","\n","CLIP Similarity Score for image 409867: 0.305006742477417\n","http://images.cocodataset.org/val2017/000000474164.jpg\n","tensor([[20.4680, 26.2660, 21.1254, 24.3933]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 474164: a photo of a dog\n","Generated Caption: a dog sitting in the back of a truck \n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▋   | 232/349 [06:11<03:18,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting in the back of a truck \n","\n","CLIP Similarity Score for image 474164: 0.31409382820129395\n","http://images.cocodataset.org/val2017/000000225670.jpg\n","tensor([[21.5083, 24.8042, 17.5490, 18.5657]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 225670: a photo of a dog\n","Generated Caption: a skateboarder jumping over a basketball \n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 233/349 [06:13<03:15,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog jumping in the air with a skateboard \n","\n","CLIP Similarity Score for image 225670: 0.3042554259300232\n","http://images.cocodataset.org/val2017/000000287649.jpg\n","tensor([[26.7041, 19.5257, 20.1608, 17.5149]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 287649: a photo of a cat\n","Generated Caption: a cat sitting on a desk next to a computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 234/349 [06:14<03:11,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a desk \n","\n","CLIP Similarity Score for image 287649: 0.3034367263317108\n","http://images.cocodataset.org/val2017/000000511398.jpg\n","tensor([[17.1736, 24.0615, 18.0572, 17.1114]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 511398: a photo of a dog\n","Generated Caption: a dog laying on the ground next to a towel \n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 235/349 [06:16<03:09,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on the ground \n","\n","CLIP Similarity Score for image 511398: 0.2683791518211365\n","http://images.cocodataset.org/val2017/000000269314.jpg\n","tensor([[25.4625, 22.4855, 22.7924, 20.0180]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 269314: a photo of a cat\n","Generated Caption: a banana tree with a bunch of bananas hanging from it \n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 236/349 [06:18<03:07,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on top of a banana tree \n","\n","CLIP Similarity Score for image 269314: 0.40173619985580444\n","http://images.cocodataset.org/val2017/000000533536.jpg\n","tensor([[23.7397, 18.6231, 20.4329, 16.1720]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 533536: a photo of a cat\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 237/349 [06:19<03:03,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a window sill looking out \n","\n","CLIP Similarity Score for image 533536: 0.21493172645568848\n","http://images.cocodataset.org/val2017/000000412240.jpg\n","tensor([[25.8197, 27.4994, 20.9383, 20.8935]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 412240: a photo of a dog\n","Generated Caption: a cat laying on a table next to a book \n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 238/349 [06:21<02:57,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a table \n","\n","CLIP Similarity Score for image 412240: 0.27546727657318115\n","http://images.cocodataset.org/val2017/000000492284.jpg\n","tensor([[24.9886, 21.5414, 19.5649, 17.3448]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 492284: a photo of a cat\n","Generated Caption: a woman standing next to a rock wall with a sheep \n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 239/349 [06:22<02:58,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat standing on a rock \n","\n","CLIP Similarity Score for image 492284: 0.2629036605358124\n","http://images.cocodataset.org/val2017/000000458255.jpg\n","tensor([[25.4309, 20.4013, 19.9594, 18.1035]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 458255: a photo of a cat\n","Generated Caption: a cat laying on a bed with a person \n"]},{"output_type":"stream","name":"stderr","text":["\r 69%|██████▉   | 240/349 [06:24<03:00,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bed \n","\n","CLIP Similarity Score for image 458255: 0.27417680621147156\n","http://images.cocodataset.org/val2017/000000264441.jpg\n","tensor([[24.9771, 19.6843, 19.1297, 17.9532]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 264441: a photo of a cat\n","Generated Caption: a white fluffy cat laying on a chair \n"]},{"output_type":"stream","name":"stderr","text":["\r 69%|██████▉   | 241/349 [06:26<02:53,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a chair \n","\n","CLIP Similarity Score for image 264441: 0.29002779722213745\n","http://images.cocodataset.org/val2017/000000478393.jpg\n","tensor([[24.0274, 19.5079, 16.0446, 17.2817]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 478393: a photo of a cat\n","Generated Caption: a cat and a dog laying on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 69%|██████▉   | 242/349 [06:27<02:55,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a cat laying on a bed \n","\n","CLIP Similarity Score for image 478393: 0.2905362546443939\n","http://images.cocodataset.org/val2017/000000540928.jpg\n","tensor([[25.5385, 19.8108, 20.1838, 18.6762]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 540928: a photo of a cat\n","Generated Caption: a cat sitting on a chair next to a toy elephant \n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|██████▉   | 243/349 [06:29<02:51,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a wooden chair \n","\n","CLIP Similarity Score for image 540928: 0.29682669043540955\n","http://images.cocodataset.org/val2017/000000231831.jpg\n","tensor([[25.9079, 19.8000, 20.2503, 17.5215]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 231831: a photo of a cat\n","Generated Caption: a cat sitting on a table next to a table cloth \n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|██████▉   | 244/349 [06:30<02:46,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a table \n","\n","CLIP Similarity Score for image 231831: 0.29029005765914917\n","http://images.cocodataset.org/val2017/000000399655.jpg\n","tensor([[18.1093, 24.2645, 17.0222, 16.6103]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 399655: a photo of a dog\n","Generated Caption: a dog wearing a red shirt and a black and white striped shirt \n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 245/349 [06:32<02:49,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog wearing a red and white shirt \n","\n","CLIP Similarity Score for image 399655: 0.22557082772254944\n","http://images.cocodataset.org/val2017/000000489014.jpg\n","tensor([[20.9659, 21.9183, 19.0731, 18.7013]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 489014: a photo of a dog\n","Generated Caption: a white and black dog standing on top of a sailboat \n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 246/349 [06:34<02:46,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a sailboat \n","\n","CLIP Similarity Score for image 489014: 0.3367081880569458\n","http://images.cocodataset.org/val2017/000000149568.jpg\n","tensor([[16.9542, 24.9709, 17.7727, 17.2098]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 149568: a photo of a dog\n","Generated Caption: a dog running with a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 71%|███████   | 247/349 [06:36<03:01,  1.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a man walking on a leash \n","\n","CLIP Similarity Score for image 149568: 0.2733732759952545\n","http://images.cocodataset.org/val2017/000000058111.jpg\n","tensor([[25.9239, 20.0717, 19.9411, 18.0967]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 58111: a photo of a cat\n","Generated Caption: a cat is sitting on a counter top \n"]},{"output_type":"stream","name":"stderr","text":["\r 71%|███████   | 248/349 [06:37<02:52,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a counter \n","\n","CLIP Similarity Score for image 58111: 0.2814854681491852\n","http://images.cocodataset.org/val2017/000000245576.jpg\n","tensor([[25.6905, 20.0403, 19.4180, 17.2966]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 245576: a photo of a cat\n","Generated Caption: a cat is laying on a keyboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 71%|███████▏  | 249/349 [06:39<02:43,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a computer keyboard \n","\n","CLIP Similarity Score for image 245576: 0.3287656009197235\n","http://images.cocodataset.org/val2017/000000534270.jpg\n","tensor([[19.8000, 25.2524, 20.2575, 19.9570]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 534270: a photo of a dog\n","Generated Caption: a man and woman standing next to each other \n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 250/349 [06:41<02:40,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a man standing next to each other \n","\n","CLIP Similarity Score for image 534270: 0.2600768506526947\n","http://images.cocodataset.org/val2017/000000476810.jpg\n","tensor([[25.5480, 19.5999, 18.9679, 17.8163]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 476810: a photo of a cat\n","Generated Caption: a black cat laying on a table next to a remote control \n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 251/349 [06:42<02:38,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a table \n","\n","CLIP Similarity Score for image 476810: 0.22947457432746887\n","http://images.cocodataset.org/val2017/000000347930.jpg\n","tensor([[20.5656, 26.1908, 20.2961, 18.3065]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 347930: a photo of a dog\n","Generated Caption: a dog wearing a hat on a couch \n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 252/349 [06:44<02:31,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a couch \n","\n","CLIP Similarity Score for image 347930: 0.3110140562057495\n","http://images.cocodataset.org/val2017/000000273232.jpg\n","tensor([[18.4382, 23.6376, 19.0159, 18.2564]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 273232: a photo of a dog\n","Generated Caption: a man carrying a surfboard and a dog on a beach \n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 253/349 [06:45<02:35,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog carrying a surfboard on a beach \n","\n","CLIP Similarity Score for image 273232: 0.3280346393585205\n","http://images.cocodataset.org/val2017/000000082807.jpg\n","tensor([[21.7969, 27.0282, 21.5714, 18.6794]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 82807: a photo of a dog\n","Generated Caption: a dog sitting on a table with a cup of coffee \n"]},{"output_type":"stream","name":"stderr","text":["\r 73%|███████▎  | 254/349 [06:47<02:31,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a table \n","\n","CLIP Similarity Score for image 82807: 0.3015853464603424\n","http://images.cocodataset.org/val2017/000000443303.jpg\n","tensor([[23.8384, 18.2053, 18.4977, 18.3019]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 443303: a photo of a cat\n","Generated Caption: a cat is laying on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 73%|███████▎  | 255/349 [06:48<02:26,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bed \n","\n","CLIP Similarity Score for image 443303: 0.26074936985969543\n","http://images.cocodataset.org/val2017/000000300913.jpg\n","tensor([[27.9335, 22.1276, 21.5068, 20.3205]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 300913: a photo of a cat\n","Generated Caption: a cat is laying on a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 73%|███████▎  | 256/349 [06:50<02:23,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a blanket \n","\n","CLIP Similarity Score for image 300913: 0.27559682726860046\n","http://images.cocodataset.org/val2017/000000491757.jpg\n","tensor([[27.2836, 21.5570, 22.0878, 20.4108]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 491757: a photo of a cat\n","Generated Caption: a cat is sitting on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▎  | 257/349 [06:52<02:26,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bed \n","\n","CLIP Similarity Score for image 491757: 0.322451651096344\n","http://images.cocodataset.org/val2017/000000350054.jpg\n","tensor([[26.1344, 21.4107, 21.2097, 18.1624]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 350054: a photo of a cat\n","Generated Caption: a tv is on in a living room \n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 258/349 [06:53<02:22,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a christmas tree \n","\n","CLIP Similarity Score for image 350054: 0.28008583188056946\n","http://images.cocodataset.org/val2017/000000236784.jpg\n","tensor([[16.3354, 21.9668, 14.7640, 16.4640]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 236784: a photo of a dog\n","Generated Caption: a dog laying on a couch with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 259/349 [06:55<02:22,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a couch with a cat \n","\n","CLIP Similarity Score for image 236784: 0.2719476521015167\n","http://images.cocodataset.org/val2017/000000353970.jpg\n","tensor([[26.6276, 20.3028, 20.5316, 18.6931]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 353970: a photo of a cat\n","Generated Caption: a cat is sitting on a chair \n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 260/349 [06:56<02:17,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a chair \n","\n","CLIP Similarity Score for image 353970: 0.3027237355709076\n","http://images.cocodataset.org/val2017/000000143961.jpg\n","tensor([[16.9114, 19.4195, 19.1510, 17.4712]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 143961: a photo of a dog\n","Generated Caption: a woman sitting on the grass with a rainbow umbrella \n"]},{"output_type":"stream","name":"stderr","text":["\r 75%|███████▍  | 261/349 [06:58<02:28,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a woman \n","\n","CLIP Similarity Score for image 143961: 0.20975297689437866\n","http://images.cocodataset.org/val2017/000000206831.jpg\n","tensor([[19.3034, 26.1158, 19.2820, 18.4795]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 206831: a photo of a dog\n","Generated Caption: a dog with a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 75%|███████▌  | 262/349 [07:00<02:27,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a frisbee in its mouth \n","\n","CLIP Similarity Score for image 206831: 0.3119344115257263\n","http://images.cocodataset.org/val2017/000000570664.jpg\n","tensor([[25.7886, 18.8175, 19.7561, 18.7984]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 570664: a photo of a cat\n","Generated Caption: a cat is sitting on a counter top \n"]},{"output_type":"stream","name":"stderr","text":["\r 75%|███████▌  | 263/349 [07:01<02:22,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a counter \n","\n","CLIP Similarity Score for image 570664: 0.25000232458114624\n","http://images.cocodataset.org/val2017/000000166277.jpg\n","tensor([[26.5892, 21.2075, 20.3333, 16.7828]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 166277: a photo of a cat\n","Generated Caption: a cat drinking water from a glass \n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 264/349 [07:03<02:25,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat drinking from a glass \n","\n","CLIP Similarity Score for image 166277: 0.33636704087257385\n","http://images.cocodataset.org/val2017/000000505573.jpg\n","tensor([[18.8797, 24.0046, 18.3121, 16.4900]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 505573: a photo of a dog\n","Generated Caption: a dog wearing a red bow tie \n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 265/349 [07:05<02:16,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog wearing a red bow tie \n","\n","CLIP Similarity Score for image 505573: 0.34039250016212463\n","http://images.cocodataset.org/val2017/000000338624.jpg\n","tensor([[19.1060, 25.2433, 18.6390, 18.7170]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 338624: a photo of a dog\n","Generated Caption: a man walking down a sidewalk with a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 266/349 [07:06<02:12,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog walking down a sidewalk \n","\n","CLIP Similarity Score for image 338624: 0.32030153274536133\n","http://images.cocodataset.org/val2017/000000271728.jpg\n","tensor([[24.4851, 19.6171, 19.4823, 17.9325]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 271728: a photo of a cat\n","Generated Caption: a cat sitting on a couch next to a remote control \n"]},{"output_type":"stream","name":"stderr","text":["\r 77%|███████▋  | 267/349 [07:08<02:10,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a couch \n","\n","CLIP Similarity Score for image 271728: 0.271049439907074\n","http://images.cocodataset.org/val2017/000000405306.jpg\n","tensor([[28.2835, 22.2181, 21.5061, 19.7982]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 405306: a photo of a cat\n","Generated Caption: a cat laying on a bed with a red blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 77%|███████▋  | 268/349 [07:09<02:07,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bed \n","\n","CLIP Similarity Score for image 405306: 0.29605334997177124\n","http://images.cocodataset.org/val2017/000000377000.jpg\n","tensor([[26.6836, 22.2492, 23.1471, 19.4565]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 377000: a photo of a cat\n","Generated Caption: a cat is looking out the window of a window \n"]},{"output_type":"stream","name":"stderr","text":["\r 77%|███████▋  | 269/349 [07:11<02:05,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a window sill \n","\n","CLIP Similarity Score for image 377000: 0.27659353613853455\n","http://images.cocodataset.org/val2017/000000519764.jpg\n","tensor([[25.4831, 19.4744, 19.1439, 17.7081]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 519764: a photo of a cat\n","Generated Caption: a cat sitting on a chair in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 77%|███████▋  | 270/349 [07:12<02:02,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a chair \n","\n","CLIP Similarity Score for image 519764: 0.3002450168132782\n","http://images.cocodataset.org/val2017/000000375278.jpg\n","tensor([[24.3783, 20.3299, 18.2457, 17.2646]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 375278: a photo of a cat\n","Generated Caption: a black cat is laying on a person's lap \n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 271/349 [07:14<02:05,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat with its paw on a person's hand \n","\n","CLIP Similarity Score for image 375278: 0.25708329677581787\n","http://images.cocodataset.org/val2017/000000295478.jpg\n","tensor([[19.1036, 23.4271, 19.9479, 17.7768]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 295478: a photo of a dog\n","Generated Caption: a woman walking down a sidewalk with a bag \n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 272/349 [07:16<02:12,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog wearing a shirt and jeans \n","\n","CLIP Similarity Score for image 295478: 0.2797742486000061\n","http://images.cocodataset.org/val2017/000000554579.jpg\n","tensor([[17.6481, 21.6913, 15.2619, 16.0441]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 554579: a photo of a dog\n","Generated Caption: a woman standing next to a man holding a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 273/349 [07:18<02:11,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a woman in a room \n","\n","CLIP Similarity Score for image 554579: 0.2834914028644562\n","http://images.cocodataset.org/val2017/000000415990.jpg\n","tensor([[15.8444, 18.1585, 15.5235, 16.5112]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 415990: a photo of a dog\n","Generated Caption: a herd of cattle standing in a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 79%|███████▊  | 274/349 [07:20<02:08,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog in a field with a herd of cattle \n","\n","CLIP Similarity Score for image 415990: 0.30567631125450134\n","http://images.cocodataset.org/val2017/000000101420.jpg\n","tensor([[25.4391, 20.1501, 20.9195, 18.9476]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 101420: a photo of a cat\n","Generated Caption: a cat is laying on a couch in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 79%|███████▉  | 275/349 [07:21<02:03,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sleeping on a couch \n","\n","CLIP Similarity Score for image 101420: 0.268841028213501\n","http://images.cocodataset.org/val2017/000000460841.jpg\n","tensor([[24.4537, 21.2066, 21.8655, 16.1698]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 460841: a photo of a cat\n","Generated Caption: a person is looking at a cell phone \n"]},{"output_type":"stream","name":"stderr","text":["\r 79%|███████▉  | 276/349 [07:23<01:57,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a toilet \n","\n","CLIP Similarity Score for image 460841: 0.2486383318901062\n","http://images.cocodataset.org/val2017/000000367082.jpg\n","tensor([[20.9762, 26.8318, 19.1390, 19.1241]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 367082: a photo of a dog\n","Generated Caption: a dog is sitting on a pillow in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 79%|███████▉  | 277/349 [07:24<01:53,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a bed \n","\n","CLIP Similarity Score for image 367082: 0.2845040559768677\n","http://images.cocodataset.org/val2017/000000273642.jpg\n","tensor([[19.7245, 25.5338, 18.4726, 17.7742]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 273642: a photo of a dog\n","Generated Caption: a dog holding a remote control in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|███████▉  | 278/349 [07:26<01:50,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a remote control \n","\n","CLIP Similarity Score for image 273642: 0.34914830327033997\n","http://images.cocodataset.org/val2017/000000078420.jpg\n","tensor([[24.0366, 17.9236, 17.3367, 16.3015]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 78420: a photo of a cat\n","Generated Caption: a cat laying on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|███████▉  | 279/349 [07:27<01:53,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a laptop \n","\n","CLIP Similarity Score for image 78420: 0.28919628262519836\n","http://images.cocodataset.org/val2017/000000018833.jpg\n","tensor([[26.7308, 21.1545, 19.6271, 19.9276]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 18833: a photo of a cat\n","Generated Caption: a cat laying on a rug with shoes \n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 280/349 [07:29<01:55,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a rug \n","\n","CLIP Similarity Score for image 18833: 0.24376584589481354\n","http://images.cocodataset.org/val2017/000000389933.jpg\n","tensor([[21.2654, 27.8213, 21.0174, 19.2455]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 389933: a photo of a dog\n","Generated Caption: a brown dog sitting on a couch \n"]},{"output_type":"stream","name":"stderr","text":["\r 81%|████████  | 281/349 [07:31<01:49,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a couch \n","\n","CLIP Similarity Score for image 389933: 0.2741742432117462\n","http://images.cocodataset.org/val2017/000000253386.jpg\n","tensor([[21.3818, 28.7314, 20.7266, 19.4688]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 253386: a photo of a dog\n","Generated Caption: a black dog with a black collar and a black dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 81%|████████  | 282/349 [07:32<01:46,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a black collar \n","\n","CLIP Similarity Score for image 253386: 0.31531181931495667\n","http://images.cocodataset.org/val2017/000000329447.jpg\n","tensor([[17.9633, 22.7535, 15.5464, 17.2273]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 329447: a photo of a dog\n","Generated Caption: two cows and a calf standing in a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 81%|████████  | 283/349 [07:34<01:45,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a cow in a field \n","\n","CLIP Similarity Score for image 329447: 0.3094513416290283\n","http://images.cocodataset.org/val2017/000000170278.jpg\n","tensor([[21.7572, 27.5198, 20.4296, 19.4110]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 170278: a photo of a dog\n","Generated Caption: a dog laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 81%|████████▏ | 284/349 [07:35<01:42,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 170278: 0.3191554844379425\n","http://images.cocodataset.org/val2017/000000262938.jpg\n","tensor([[19.9693, 24.3442, 18.0010, 17.3987]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 262938: a photo of a dog\n","Generated Caption: a cat sitting on a table next to a book \n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 285/349 [07:37<01:41,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a book \n","\n","CLIP Similarity Score for image 262938: 0.2853337824344635\n","http://images.cocodataset.org/val2017/000000236592.jpg\n","tensor([[22.0325, 25.8680, 20.2232, 17.5049]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 236592: a photo of a dog\n","Generated Caption: a cat is looking out of an open oven \n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 286/349 [07:38<01:38,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog looking inside a oven \n","\n","CLIP Similarity Score for image 236592: 0.3714454770088196\n","http://images.cocodataset.org/val2017/000000479155.jpg\n","tensor([[20.0839, 25.3262, 18.8236, 18.7442]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 479155: a photo of a dog\n","Generated Caption: a woman walking down a street with a bunch of fruit \n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 287/349 [07:40<01:41,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a woman walking through a market \n","\n","CLIP Similarity Score for image 479155: 0.30830585956573486\n","http://images.cocodataset.org/val2017/000000269113.jpg\n","tensor([[14.9577, 22.7154, 16.2565, 16.0594]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 269113: a photo of a dog\n","Generated Caption: a dog running across a field with a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 83%|████████▎ | 288/349 [07:43<01:50,  1.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog running with a frisbee in its mouth \n","\n","CLIP Similarity Score for image 269113: 0.28117451071739197\n","http://images.cocodataset.org/val2017/000000255664.jpg\n","tensor([[22.0821, 28.1781, 22.0441, 20.3241]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 255664: a photo of a dog\n","Generated Caption: a dog catching a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 83%|████████▎ | 289/349 [07:44<01:48,  1.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog catching a frisbee in a field \n","\n","CLIP Similarity Score for image 255664: 0.3408297002315521\n","http://images.cocodataset.org/val2017/000000357459.jpg\n","tensor([[17.5354, 23.7005, 18.7585, 17.6892]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 357459: a photo of a dog\n","Generated Caption: a dog is playing with a frisbee in the grass \n"]},{"output_type":"stream","name":"stderr","text":["\r 83%|████████▎ | 290/349 [07:46<01:44,  1.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog playing with a frisbee \n","\n","CLIP Similarity Score for image 357459: 0.3615765869617462\n","http://images.cocodataset.org/val2017/000000555005.jpg\n","tensor([[21.8593, 26.4032, 20.7891, 18.9249]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 555005: a photo of a dog\n","Generated Caption: a man in a black shirt and tie \n"]},{"output_type":"stream","name":"stderr","text":["\r 83%|████████▎ | 291/349 [07:48<01:39,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a shirt on \n","\n","CLIP Similarity Score for image 555005: 0.26118168234825134\n","http://images.cocodataset.org/val2017/000000562561.jpg\n","tensor([[18.8345, 26.1322, 19.1772, 18.4355]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 562561: a photo of a dog\n","Generated Caption: a dog is standing on a leash with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▎ | 292/349 [07:49<01:35,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a cat playing together \n","\n","CLIP Similarity Score for image 562561: 0.27950215339660645\n","http://images.cocodataset.org/val2017/000000355905.jpg\n","tensor([[20.9570, 27.6379, 21.0639, 19.3840]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 355905: a photo of a dog\n","Generated Caption: a dog standing on the beach with its head down \n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▍ | 293/349 [07:51<01:30,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on the beach \n","\n","CLIP Similarity Score for image 355905: 0.3010649085044861\n","http://images.cocodataset.org/val2017/000000464522.jpg\n","tensor([[20.1704, 27.3689, 20.4255, 19.5820]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 464522: a photo of a dog\n","Generated Caption: a dog standing on a grassy field \n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▍ | 294/349 [07:52<01:26,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a collar on \n","\n","CLIP Similarity Score for image 464522: 0.29037246108055115\n","http://images.cocodataset.org/val2017/000000022892.jpg\n","tensor([[23.8747, 22.1836, 18.2741, 17.7838]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 22892: a photo of a cat\n","Generated Caption: a cat looking at a cat sitting on a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 85%|████████▍ | 295/349 [07:54<01:33,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat and a cat looking at each other \n","\n","CLIP Similarity Score for image 22892: 0.26113268733024597\n","http://images.cocodataset.org/val2017/000000447200.jpg\n","tensor([[18.2866, 23.9183, 16.9901, 17.7121]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 447200: a photo of a dog\n","Generated Caption: two dogs are standing next to each other \n"]},{"output_type":"stream","name":"stderr","text":["\r 85%|████████▍ | 296/349 [07:56<01:31,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a cat on a sidewalk \n","\n","CLIP Similarity Score for image 447200: 0.2851557433605194\n","http://images.cocodataset.org/val2017/000000221693.jpg\n","tensor([[19.3826, 25.9499, 19.2696, 18.2472]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 221693: a photo of a dog\n","Generated Caption: a dog with a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 85%|████████▌ | 297/349 [07:58<01:28,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog playing with a frisbee \n","\n","CLIP Similarity Score for image 221693: 0.3292151391506195\n","http://images.cocodataset.org/val2017/000000486479.jpg\n","tensor([[21.4072, 26.5885, 19.7008, 18.5535]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 486479: a photo of a dog\n","Generated Caption: a dog laying on a blanket on the floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 85%|████████▌ | 298/349 [07:59<01:24,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 486479: 0.2785729169845581\n","http://images.cocodataset.org/val2017/000000283412.jpg\n","tensor([[21.7234, 26.8703, 19.8939, 19.0094]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 283412: a photo of a dog\n","Generated Caption: a white dog sitting on a table with a white plate \n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 299/349 [08:01<01:22,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a stuffed animal \n","\n","CLIP Similarity Score for image 283412: 0.2649427056312561\n","http://images.cocodataset.org/val2017/000000367195.jpg\n","tensor([[22.5079, 27.9389, 21.7720, 19.9400]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 367195: a photo of a dog\n","Generated Caption: a dog laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 300/349 [08:02<01:17,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 367195: 0.296048104763031\n","http://images.cocodataset.org/val2017/000000331075.jpg\n","tensor([[18.7062, 26.5356, 20.4364, 18.1717]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 331075: a photo of a dog\n","Generated Caption: a dog standing on top of a beach \n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 301/349 [08:04<01:15,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a cat on a beach \n","\n","CLIP Similarity Score for image 331075: 0.2637737989425659\n","http://images.cocodataset.org/val2017/000000324158.jpg\n","tensor([[21.0920, 25.0162, 21.8369, 20.3449]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 324158: a photo of a dog\n","Generated Caption: a man on a skateboard is going down a road \n"]},{"output_type":"stream","name":"stderr","text":["\r 87%|████████▋ | 302/349 [08:05<01:15,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a leash walking down a road \n","\n","CLIP Similarity Score for image 324158: 0.2761473059654236\n","http://images.cocodataset.org/val2017/000000064868.jpg\n","tensor([[25.0317, 26.3079, 20.4970, 18.8610]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 64868: a photo of a dog\n","Generated Caption: a person is petting a cat on a stove \n"]},{"output_type":"stream","name":"stderr","text":["\r 87%|████████▋ | 303/349 [08:07<01:19,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog being fed a piece of food \n","\n","CLIP Similarity Score for image 64868: 0.2858120799064636\n","http://images.cocodataset.org/val2017/000000309938.jpg\n","tensor([[21.2357, 25.5632, 18.4242, 18.5144]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 309938: a photo of a dog\n","Generated Caption: a small dog is sitting on a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 87%|████████▋ | 304/349 [08:09<01:14,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a stuffed animal \n","\n","CLIP Similarity Score for image 309938: 0.2707829177379608\n","http://images.cocodataset.org/val2017/000000263463.jpg\n","tensor([[21.2410, 27.0648, 21.1006, 25.1084]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 263463: a photo of a dog\n","Generated Caption: a dog is looking out the window of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 87%|████████▋ | 305/349 [08:10<01:10,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog in a mirror \n","\n","CLIP Similarity Score for image 263463: 0.30231454968452454\n","http://images.cocodataset.org/val2017/000000564280.jpg\n","tensor([[18.4085, 24.1820, 18.2987, 17.0522]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 564280: a photo of a dog\n","Generated Caption: a dog laying on a couch with a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 306/349 [08:12<01:07,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a laptop \n","\n","CLIP Similarity Score for image 564280: 0.3177882432937622\n","http://images.cocodataset.org/val2017/000000022192.jpg\n","tensor([[19.3024, 24.9425, 18.4051, 18.8047]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 22192: a photo of a dog\n","Generated Caption: a dog laying on the ground next to a pile of trash \n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 307/349 [08:13<01:05,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a blanket \n","\n","CLIP Similarity Score for image 22192: 0.26413217186927795\n","http://images.cocodataset.org/val2017/000000385997.jpg\n","tensor([[24.4520, 24.4483, 21.2388, 20.9165]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 385997: a photo of a cat\n","Generated Caption: a cat sitting on a chair in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 308/349 [08:15<01:03,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a chair \n","\n","CLIP Similarity Score for image 385997: 0.23166963458061218\n","http://images.cocodataset.org/val2017/000000463522.jpg\n","tensor([[17.7771, 19.0613, 17.9419, 20.4020]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 463522: a photo of a car\n","Generated Caption: a horse drawn carriage on a city street \n"]},{"output_type":"stream","name":"stderr","text":["\r 89%|████████▊ | 309/349 [08:16<01:01,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a horse drawn carriage \n","\n","CLIP Similarity Score for image 463522: 0.29103177785873413\n","http://images.cocodataset.org/val2017/000000125405.jpg\n","tensor([[15.7892, 23.8100, 16.4379, 16.9330]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 125405: a photo of a dog\n","Generated Caption: a dog standing on a grassy field with a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 89%|████████▉ | 310/349 [08:18<01:04,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a frisbee in its mouth \n","\n","CLIP Similarity Score for image 125405: 0.27061012387275696\n","http://images.cocodataset.org/val2017/000000560880.jpg\n","tensor([[19.5707, 22.1624, 18.0009, 20.9867]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 560880: a photo of a dog\n","Generated Caption: people are standing around a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 89%|████████▉ | 311/349 [08:20<01:05,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a man in a field \n","\n","CLIP Similarity Score for image 560880: 0.2743526101112366\n","http://images.cocodataset.org/val2017/000000333772.jpg\n","tensor([[25.6711, 19.3230, 19.6041, 18.4756]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 333772: a photo of a cat\n","Generated Caption: a cat laying on top of a computer desk \n"]},{"output_type":"stream","name":"stderr","text":["\r 89%|████████▉ | 312/349 [08:22<01:00,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a computer desk \n","\n","CLIP Similarity Score for image 333772: 0.3204951584339142\n","http://images.cocodataset.org/val2017/000000364636.jpg\n","tensor([[19.1343, 27.0187, 19.4983, 18.3871]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 364636: a photo of a dog\n","Generated Caption: a dog with a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|████████▉ | 313/349 [08:23<01:00,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog with a frisbee in its mouth \n","\n","CLIP Similarity Score for image 364636: 0.29835787415504456\n","http://images.cocodataset.org/val2017/000000072813.jpg\n","tensor([[20.6043, 26.3375, 19.6435, 19.2670]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 72813: a photo of a dog\n","Generated Caption: a dog laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|████████▉ | 314/349 [08:25<00:56,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 72813: 0.30535927414894104\n","http://images.cocodataset.org/val2017/000000090003.jpg\n","tensor([[15.5991, 22.2100, 16.5837, 17.1736]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 90003: a photo of a dog\n","Generated Caption: a dog running in the grass with a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 315/349 [08:27<00:55,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog running in a field \n","\n","CLIP Similarity Score for image 90003: 0.22975556552410126\n","http://images.cocodataset.org/val2017/000000052891.jpg\n","tensor([[18.1588, 25.4646, 19.3840, 16.9488]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 52891: a photo of a dog\n","Generated Caption: a dog is playing with a frisbee in the sand \n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████ | 316/349 [08:28<00:55,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog playing with a frisbee \n","\n","CLIP Similarity Score for image 52891: 0.2948470413684845\n","http://images.cocodataset.org/val2017/000000547502.jpg\n","tensor([[15.0053, 22.4653, 16.0795, 16.0567]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 547502: a photo of a dog\n","Generated Caption: a dog chasing a dog in a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████ | 317/349 [08:30<00:53,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a dog in a field \n","\n","CLIP Similarity Score for image 547502: 0.2292681783437729\n","http://images.cocodataset.org/val2017/000000289702.jpg\n","tensor([[18.0163, 23.7922, 18.0050, 17.0113]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 289702: a photo of a dog\n","Generated Caption: a dog standing in a room next to a pile of trash \n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████ | 318/349 [08:32<00:57,  1.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog and a dog's reflection in a mirror \n","\n","CLIP Similarity Score for image 289702: 0.2663217782974243\n","http://images.cocodataset.org/val2017/000000369541.jpg\n","tensor([[17.7209, 26.2954, 25.2105, 18.1061]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 369541: a photo of a dog\n","Generated Caption: a dog jumping up to catch a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████▏| 319/349 [08:34<00:55,  1.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog catching a frisbee in its mouth \n","\n","CLIP Similarity Score for image 369541: 0.3611200749874115\n","http://images.cocodataset.org/val2017/000000355240.jpg\n","tensor([[20.0238, 26.3635, 19.6562, 21.3589]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 355240: a photo of a dog\n","Generated Caption: a dog sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 320/349 [08:36<00:50,  1.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a window sill \n","\n","CLIP Similarity Score for image 355240: 0.284946471452713\n","http://images.cocodataset.org/val2017/000000017029.jpg\n","tensor([[20.4696, 25.8943, 21.7093, 19.0569]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 17029: a photo of a dog\n","Generated Caption: a black cat jumping up to catch a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 321/349 [08:38<00:50,  1.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog jumping in the air to catch a frisbee \n","\n","CLIP Similarity Score for image 17029: 0.38532575964927673\n","http://images.cocodataset.org/val2017/000000151962.jpg\n","tensor([[19.6109, 26.4484, 19.7206, 24.2953]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 151962: a photo of a dog\n","Generated Caption: a dog is looking out the window of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 322/349 [08:39<00:47,  1.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog in the reflection of a car mirror \n","\n","CLIP Similarity Score for image 151962: 0.32397520542144775\n","http://images.cocodataset.org/val2017/000000421455.jpg\n","tensor([[20.9929, 25.4843, 20.4295, 24.4796]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 421455: a photo of a dog\n","Generated Caption: a person taking a picture of themselves in a car mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 323/349 [08:41<00:45,  1.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog in the reflection of a car mirror \n","\n","CLIP Similarity Score for image 421455: 0.31968024373054504\n","http://images.cocodataset.org/val2017/000000134112.jpg\n","tensor([[20.1631, 24.7047, 18.5094, 17.1289]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 134112: a photo of a dog\n","Generated Caption: a dog is sitting on a bed with a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 324/349 [08:43<00:42,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a laptop \n","\n","CLIP Similarity Score for image 134112: 0.33993273973464966\n","http://images.cocodataset.org/val2017/000000134882.jpg\n","tensor([[24.7959, 19.7603, 19.4000, 17.9814]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 134882: a photo of a cat\n","Generated Caption: a bed with a white sheet and a black and white cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 325/349 [08:45<00:43,  1.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bed \n","\n","CLIP Similarity Score for image 134882: 0.280862033367157\n","http://images.cocodataset.org/val2017/000000061333.jpg\n","tensor([[28.0680, 22.2465, 21.3002, 20.7235]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 61333: a photo of a cat\n","Generated Caption: a cat laying on a blanket on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 326/349 [08:47<00:41,  1.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bed \n","\n","CLIP Similarity Score for image 61333: 0.3124151825904846\n","http://images.cocodataset.org/val2017/000000202445.jpg\n","tensor([[24.9765, 18.8830, 18.8722, 17.5772]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 202445: a photo of a cat\n","Generated Caption: a cat is laying on a bed with a picture of a cat on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▎| 327/349 [08:48<00:41,  1.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat laying on a bed \n","\n","CLIP Similarity Score for image 202445: 0.2732568085193634\n","http://images.cocodataset.org/val2017/000000318238.jpg\n","tensor([[14.9688, 20.9939, 14.1080, 15.6411]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 318238: a photo of a dog\n","Generated Caption: a dog laying on a bed with a person \n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▍| 328/349 [08:50<00:39,  1.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 318238: 0.24183389544487\n","http://images.cocodataset.org/val2017/000000159458.jpg\n","tensor([[19.7034, 21.3115, 17.8087, 16.1745]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 159458: a photo of a dog\n","Generated Caption: a bed with a stuffed animal on top of it \n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▍| 329/349 [08:52<00:35,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 159458: 0.2732863128185272\n","http://images.cocodataset.org/val2017/000000193674.jpg\n","tensor([[17.4633, 20.6381, 19.6917, 18.2666]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 193674: a photo of a dog\n","Generated Caption: a man in a wet suit is standing on a surfboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 95%|█████████▍| 330/349 [08:54<00:33,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on a surfboard in the ocean \n","\n","CLIP Similarity Score for image 193674: 0.28677716851234436\n","http://images.cocodataset.org/val2017/000000494913.jpg\n","tensor([[19.2873, 19.1062, 18.4235, 18.5908]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 494913: a photo of a cat\n","Generated Caption: a living room with a couch, television, and a coffee table \n"]},{"output_type":"stream","name":"stderr","text":["\r 95%|█████████▍| 331/349 [08:55<00:31,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a couch in a living room \n","\n","CLIP Similarity Score for image 494913: 0.21659357845783234\n","http://images.cocodataset.org/val2017/000000371749.jpg\n","tensor([[21.5522, 22.9222, 20.1096, 17.3505]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 371749: a photo of a dog\n","Generated Caption: a person holding a wii remote in their hand \n"]},{"output_type":"stream","name":"stderr","text":["\r 95%|█████████▌| 332/349 [08:57<00:30,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog playing with a wii controller \n","\n","CLIP Similarity Score for image 371749: 0.37772923707962036\n","http://images.cocodataset.org/val2017/000000121586.jpg\n","tensor([[20.7528, 20.1766, 19.3351, 15.9390]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 121586: a photo of a cat\n","Generated Caption: a television is on in a room with a book shelf \n"]},{"output_type":"stream","name":"stderr","text":["\r 95%|█████████▌| 333/349 [08:59<00:28,  1.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a tv \n","\n","CLIP Similarity Score for image 121586: 0.2431049942970276\n","http://images.cocodataset.org/val2017/000000435299.jpg\n","tensor([[27.8197, 22.0403, 22.3010, 20.3492]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 435299: a photo of a cat\n","Generated Caption: a cat is laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▌| 334/349 [09:01<00:25,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a bed \n","\n","CLIP Similarity Score for image 435299: 0.3170135021209717\n","http://images.cocodataset.org/val2017/000000366611.jpg\n","tensor([[17.3415, 23.5622, 17.1833, 17.7490]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 366611: a photo of a dog\n","Generated Caption: a dog is standing on the ground by a grassy area \n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▌| 335/349 [09:02<00:23,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog on the ground \n","\n","CLIP Similarity Score for image 366611: 0.2531684935092926\n","http://images.cocodataset.org/val2017/000000530624.jpg\n","tensor([[20.6088, 24.3868, 18.4809, 17.0035]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 530624: a photo of a dog\n","Generated Caption: a cat laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▋| 336/349 [09:04<00:21,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog laying on a bed \n","\n","CLIP Similarity Score for image 530624: 0.2784810960292816\n","http://images.cocodataset.org/val2017/000000427034.jpg\n","tensor([[19.3195, 25.1662, 19.2811, 17.6133]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 427034: a photo of a dog\n","Generated Caption: a dog is sitting on a lamp post \n"]},{"output_type":"stream","name":"stderr","text":["\r 97%|█████████▋| 337/349 [09:05<00:19,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a lamp \n","\n","CLIP Similarity Score for image 427034: 0.2636001408100128\n","http://images.cocodataset.org/val2017/000000139684.jpg\n","tensor([[19.3339, 18.7704, 18.3781, 17.0980]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 139684: a photo of a cat\n","Generated Caption: a living room with a couch, television, and a coffee table \n"]},{"output_type":"stream","name":"stderr","text":["\r 97%|█████████▋| 338/349 [09:07<00:17,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a couch \n","\n","CLIP Similarity Score for image 139684: 0.2155308723449707\n","http://images.cocodataset.org/val2017/000000057760.jpg\n","tensor([[17.3438, 19.7060, 21.2292, 19.7120]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 57760: a photo of a bird\n","Generated Caption: people on a beach with a beach ball \n"]},{"output_type":"stream","name":"stderr","text":["\r 97%|█████████▋| 339/349 [09:09<00:16,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a bird-like group of people on a beach \n","\n","CLIP Similarity Score for image 57760: 0.3004379868507385\n","http://images.cocodataset.org/val2017/000000543581.jpg\n","tensor([[21.0853, 25.1502, 19.7740, 18.3959]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 543581: a photo of a dog\n","Generated Caption: a living room with a couch, chair, and a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 97%|█████████▋| 340/349 [09:10<00:14,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting on a couch \n","\n","CLIP Similarity Score for image 543581: 0.2920953631401062\n","http://images.cocodataset.org/val2017/000000471789.jpg\n","tensor([[17.5007, 18.6879, 22.2334, 19.9535]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 471789: a photo of a bird\n","Generated Caption: a man flying a kite in a park \n"]},{"output_type":"stream","name":"stderr","text":["\r 98%|█████████▊| 341/349 [09:12<00:13,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a bird flying over a park \n","\n","CLIP Similarity Score for image 471789: 0.2850562632083893\n","http://images.cocodataset.org/val2017/000000117908.jpg\n","tensor([[23.8986, 21.1441, 20.5307, 17.2894]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 117908: a photo of a cat\n","Generated Caption: a cat is looking at the refrigerator door \n"]},{"output_type":"stream","name":"stderr","text":["\r 98%|█████████▊| 342/349 [09:14<00:11,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat in a refrigerator \n","\n","CLIP Similarity Score for image 117908: 0.2967582643032074\n","http://images.cocodataset.org/val2017/000000366141.jpg\n","tensor([[20.6200, 20.1515, 19.0527, 17.7151]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 366141: a photo of a cat\n","Generated Caption: a living room with a couch, chair, and table \n"]},{"output_type":"stream","name":"stderr","text":["\r 98%|█████████▊| 343/349 [09:15<00:09,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a couch in a living room \n","\n","CLIP Similarity Score for image 366141: 0.2672273516654968\n","http://images.cocodataset.org/val2017/000000290843.jpg\n","tensor([[26.3306, 20.7002, 20.9790, 17.8929]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 290843: a photo of a cat\n","Generated Caption: a cat sitting on top of a bed next to a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 99%|█████████▊| 344/349 [09:17<00:08,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a bed \n","\n","CLIP Similarity Score for image 290843: 0.24818113446235657\n","http://images.cocodataset.org/val2017/000000014007.jpg\n","tensor([[26.6305, 20.8391, 20.9731, 18.7517]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 14007: a photo of a cat\n","Generated Caption: a cat is standing on a ledge in a bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r 99%|█████████▉| 345/349 [09:18<00:06,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat on a ledge \n","\n","CLIP Similarity Score for image 14007: 0.2895844876766205\n","http://images.cocodataset.org/val2017/000000371699.jpg\n","tensor([[17.2005, 19.6629, 18.9730, 17.2388]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 371699: a photo of a dog\n","Generated Caption: a woman sitting at a table with a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 99%|█████████▉| 346/349 [09:20<00:04,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a dog sitting at a table \n","\n","CLIP Similarity Score for image 371699: 0.21899470686912537\n","http://images.cocodataset.org/val2017/000000084362.jpg\n","tensor([[24.9015, 20.4697, 19.2853, 20.8185]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 84362: a photo of a cat\n","Generated Caption: a man sitting on a couch watching a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 99%|█████████▉| 347/349 [09:22<00:03,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on a couch \n","\n","CLIP Similarity Score for image 84362: 0.2789967656135559\n","http://images.cocodataset.org/val2017/000000243344.jpg\n","tensor([[26.6006, 20.1601, 21.8511, 17.3980]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 243344: a photo of a cat\n","Generated Caption: a cat sitting on top of a refrigerator \n"]},{"output_type":"stream","name":"stderr","text":["\r100%|█████████▉| 348/349 [09:23<00:01,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a cat sitting on top of a refrigerator \n","\n","CLIP Similarity Score for image 243344: 0.36297494173049927\n","http://images.cocodataset.org/val2017/000000395801.jpg\n","tensor([[16.1920, 18.5106, 17.5410, 21.4058]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 395801: a photo of a car\n","Generated Caption: a man riding a skateboard down a street \n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 349/349 [09:25<00:00,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on a street with people walking \n","\n","CLIP Similarity Score for image 395801: 0.24241767823696136\n","\n","Average CLIP Similarity Score for 349 images: 0.29296260623843073\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Test on single image"],"metadata":{"id":"WKqJJ3TkiyPo"}},{"cell_type":"code","source":["# load image\n","# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","# url = \"http://images.cocodataset.org/val2017/000000227044.jpg\"\n","url = \"http://images.cocodataset.org/val2017/000000284623.jpg\"\n","image = Image.open(requests.get(url, stream=True).raw)\n","\n","# Use CLIP to compute similarity scores for predefined captions\n","# candidate_captions = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of an animal\"]\n","# candidate_captions = [\"a photo of a cat\", \"a photo of a white cat\", \"a photo of a white cloud\", \"a photo of a black cloud\"]\n","candidate_captions = [\"a photo of a cat\", \"a photo of a black cat\", \"a photo of a black car\"]\n","inputs = clip_processor(text=candidate_captions, images=image, return_tensors=\"pt\", padding=True)\n","outputs = clip_model(**inputs)\n","\n","\n","# Get the similarity scores\n","logits_per_image = outputs.logits_per_image\n","print(logits_per_image)\n","probs = logits_per_image.softmax(dim=1)\n","best_caption_index = torch.argmax(probs).item()\n","print(f\"Best caption by CLIP: {candidate_captions[best_caption_index]}\")\n","\n","\n","# Generate a detailed caption using the captioning model\n","pixel_values = clip_processor(images=image, return_tensors=\"pt\").pixel_values\n","attention_mask = torch.ones(pixel_values.shape[:2], dtype=torch.long)\n","\n","generated_ids = caption_model.generate(\n","    pixel_values,\n","    attention_mask=attention_mask,  # Explicitly pass the attention mask\n","    max_length=50,                 # Set a custom maximum length for the output\n","    pad_token_id=caption_tokenizer.pad_token_id  # Use the tokenizer's pad token ID\n",")\n","generated_caption = caption_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","print(f\"Generated Caption: {generated_caption}\")\n","\n","\n","# CLIP의 Best Caption을 Caption Generation 모델의 입력에 포함\n","best_caption = candidate_captions[best_caption_index]\n","input_ids = caption_tokenizer(best_caption, return_tensors=\"pt\").input_ids\n","\n","generated_ids = caption_model.generate(\n","    pixel_values,\n","    input_ids=input_ids,  # CLIP의 Best Caption 반영\n","    attention_mask=attention_mask,\n","    max_length=50,\n","    pad_token_id=caption_tokenizer.pad_token_id\n",")\n","generated_caption = caption_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","print(f\"Generated Caption with CLIP influence: {generated_caption}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"epVY8R1UvefT","executionInfo":{"status":"ok","timestamp":1733053142254,"user_tz":-540,"elapsed":9610,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"c0a5dcf7-5de1-4b2f-d65d-9cab7ff8104b"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[26.8981, 29.5249, 22.4578]], grad_fn=<TBackward0>)\n","Best caption by CLIP: a photo of a black cat\n","Generated Caption: a cat sitting on a sink next to a mirror \n","Generated Caption with CLIP influence: a photo of a black cat sitting on a sink \n"]}]},{"cell_type":"markdown","source":["### 생성된 caption에 대해 CLIP score 계산"],"metadata":{"id":"JHT8k32TjGlI"}},{"cell_type":"code","source":["# 이미지 임베딩 생성\n","pixel_values = clip_processor(images=image, return_tensors=\"pt\").pixel_values\n","image_features = clip_model.get_image_features(pixel_values)\n","\n","# 캡션 임베딩 생성\n","print(generated_caption)\n","text_inputs = clip_processor(text=[generated_caption], return_tensors=\"pt\", padding=True)\n","text_features = clip_model.get_text_features(**text_inputs)\n","\n","# 임베딩 정규화 (벡터 길이를 1로 만듦)\n","image_features = F.normalize(image_features, p=2, dim=1)\n","text_features = F.normalize(text_features, p=2, dim=1)\n","\n","# 유사도 점수 계산 (코사인 유사도)\n","similarity_score = (image_features @ text_features.T).item()\n","print(f\"CLIP Similarity Score: {similarity_score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAXdR4sdZwzv","executionInfo":{"status":"ok","timestamp":1733026239134,"user_tz":-540,"elapsed":1100,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"f1a52f6e-8e54-47b7-df0c-2f69d82493d5"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["a photo of a cat and a cat laying on a blanket \n","CLIP Similarity Score: 0.267513632774353\n"]}]},{"cell_type":"markdown","source":["# CLIP model 수정"],"metadata":{"id":"YSXHBt-2jPv3"}},{"cell_type":"code","source":["clip_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6CJzEp79v891","executionInfo":{"status":"ok","timestamp":1733144307905,"user_tz":-540,"elapsed":19,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"1028507b-7951-4778-d5ab-5c401051669f"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CLIPModel(\n","  (text_model): CLIPTextTransformer(\n","    (embeddings): CLIPTextEmbeddings(\n","      (token_embedding): Embedding(49408, 512)\n","      (position_embedding): Embedding(77, 512)\n","    )\n","    (encoder): CLIPEncoder(\n","      (layers): ModuleList(\n","        (0-11): 12 x CLIPEncoderLayer(\n","          (self_attn): CLIPSdpaAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): CLIPMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (vision_model): CLIPVisionTransformer(\n","    (embeddings): CLIPVisionEmbeddings(\n","      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","      (position_embedding): Embedding(197, 768)\n","    )\n","    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (encoder): CLIPEncoder(\n","      (layers): ModuleList(\n","        (0-11): 12 x CLIPEncoderLayer(\n","          (self_attn): CLIPSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): CLIPMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n","  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["for name, module in clip_model.named_modules():\n","  print(name, module)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"8rrLsZciviow","executionInfo":{"status":"ok","timestamp":1733141874815,"user_tz":-540,"elapsed":14,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"408ad5c0-66e5-4178-9b4d-c6cbbde8fa28"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":[" CLIPModel(\n","  (text_model): CLIPTextTransformer(\n","    (embeddings): CLIPTextEmbeddings(\n","      (token_embedding): Embedding(49408, 512)\n","      (position_embedding): Embedding(77, 512)\n","    )\n","    (encoder): CLIPEncoder(\n","      (layers): ModuleList(\n","        (0-11): 12 x CLIPEncoderLayer(\n","          (self_attn): CLIPSdpaAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): CLIPMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (vision_model): CLIPVisionTransformer(\n","    (embeddings): CLIPVisionEmbeddings(\n","      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","      (position_embedding): Embedding(197, 768)\n","    )\n","    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (encoder): CLIPEncoder(\n","      (layers): ModuleList(\n","        (0-11): 12 x CLIPEncoderLayer(\n","          (self_attn): CLIPSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): CLIPMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n","  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",")\n","text_model CLIPTextTransformer(\n","  (embeddings): CLIPTextEmbeddings(\n","    (token_embedding): Embedding(49408, 512)\n","    (position_embedding): Embedding(77, 512)\n","  )\n","  (encoder): CLIPEncoder(\n","    (layers): ModuleList(\n","      (0-11): 12 x CLIPEncoderLayer(\n","        (self_attn): CLIPSdpaAttention(\n","          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): CLIPMLP(\n","          (activation_fn): QuickGELUActivation()\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.embeddings CLIPTextEmbeddings(\n","  (token_embedding): Embedding(49408, 512)\n","  (position_embedding): Embedding(77, 512)\n",")\n","text_model.embeddings.token_embedding Embedding(49408, 512)\n","text_model.embeddings.position_embedding Embedding(77, 512)\n","text_model.encoder CLIPEncoder(\n","  (layers): ModuleList(\n","    (0-11): 12 x CLIPEncoderLayer(\n","      (self_attn): CLIPSdpaAttention(\n","        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","      )\n","      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      (mlp): CLIPMLP(\n","        (activation_fn): QuickGELUActivation()\n","        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","      )\n","      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n",")\n","text_model.encoder.layers ModuleList(\n","  (0-11): 12 x CLIPEncoderLayer(\n","    (self_attn): CLIPSdpaAttention(\n","      (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","    )\n","    (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","    (mlp): CLIPMLP(\n","      (activation_fn): QuickGELUActivation()\n","      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","    )\n","    (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n",")\n","text_model.encoder.layers.0 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.0.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.0.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.0.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.0.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.0.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.0.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.0.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.0.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.0.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.0.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.0.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.1 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.1.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.1.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.1.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.1.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.1.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.1.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.1.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.1.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.1.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.1.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.1.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.2 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.2.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.2.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.2.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.2.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.2.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.2.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.2.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.2.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.2.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.2.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.2.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.3 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.3.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.3.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.3.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.3.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.3.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.3.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.3.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.3.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.3.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.3.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.3.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.4 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.4.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.4.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.4.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.4.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.4.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.4.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.4.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.4.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.4.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.4.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.4.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.5 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.5.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.5.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.5.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.5.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.5.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.5.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.5.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.5.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.5.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.5.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.5.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.6 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.6.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.6.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.6.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.6.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.6.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.6.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.6.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.6.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.6.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.6.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.6.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.7 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.7.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.7.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.7.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.7.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.7.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.7.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.7.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.7.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.7.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.7.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.7.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.8 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.8.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.8.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.8.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.8.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.8.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.8.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.8.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.8.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.8.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.8.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.8.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.9 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.9.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.9.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.9.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.9.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.9.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.9.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.9.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.9.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.9.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.9.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.9.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.10 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.10.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.10.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.10.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.10.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.10.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.10.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.10.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.10.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.10.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.10.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.10.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.11 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",")\n","text_model.encoder.layers.11.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.11.self_attn.k_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.11.self_attn.v_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.11.self_attn.q_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.11.self_attn.out_proj Linear(in_features=512, out_features=512, bias=True)\n","text_model.encoder.layers.11.layer_norm1 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.encoder.layers.11.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",")\n","text_model.encoder.layers.11.mlp.activation_fn QuickGELUActivation()\n","text_model.encoder.layers.11.mlp.fc1 Linear(in_features=512, out_features=2048, bias=True)\n","text_model.encoder.layers.11.mlp.fc2 Linear(in_features=2048, out_features=512, bias=True)\n","text_model.encoder.layers.11.layer_norm2 LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","text_model.final_layer_norm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","vision_model CLIPVisionTransformer(\n","  (embeddings): CLIPVisionEmbeddings(\n","    (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","    (position_embedding): Embedding(197, 768)\n","  )\n","  (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (encoder): CLIPEncoder(\n","    (layers): ModuleList(\n","      (0-11): 12 x CLIPEncoderLayer(\n","        (self_attn): CLIPSdpaAttention(\n","          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): CLIPMLP(\n","          (activation_fn): QuickGELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.embeddings CLIPVisionEmbeddings(\n","  (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","  (position_embedding): Embedding(197, 768)\n",")\n","vision_model.embeddings.patch_embedding Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","vision_model.embeddings.position_embedding Embedding(197, 768)\n","vision_model.pre_layrnorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder CLIPEncoder(\n","  (layers): ModuleList(\n","    (0-11): 12 x CLIPEncoderLayer(\n","      (self_attn): CLIPSdpaAttention(\n","        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (mlp): CLIPMLP(\n","        (activation_fn): QuickGELUActivation()\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n",")\n","vision_model.encoder.layers ModuleList(\n","  (0-11): 12 x CLIPEncoderLayer(\n","    (self_attn): CLIPSdpaAttention(\n","      (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","      (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","      (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","      (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","    )\n","    (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (mlp): CLIPMLP(\n","      (activation_fn): QuickGELUActivation()\n","      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","    )\n","    (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n",")\n","vision_model.encoder.layers.0 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.0.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.0.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.0.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.0.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.0.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.0.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.0.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.0.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.0.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.0.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.0.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.1 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.1.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.1.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.1.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.1.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.1.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.1.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.1.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.1.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.1.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.1.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.1.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.2 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.2.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.2.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.2.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.2.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.2.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.2.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.2.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.2.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.2.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.2.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.2.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.3 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.3.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.3.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.3.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.3.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.3.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.3.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.3.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.3.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.3.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.3.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.3.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.4 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.4.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.4.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.4.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.4.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.4.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.4.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.4.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.4.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.4.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.4.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.4.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.5 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.5.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.5.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.5.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.5.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.5.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.5.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.5.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.5.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.5.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.5.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.5.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.6 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.6.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.6.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.6.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.6.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.6.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.6.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.6.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.6.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.6.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.6.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.6.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.7 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.7.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.7.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.7.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.7.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.7.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.7.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.7.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.7.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.7.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.7.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.7.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.8 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.8.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.8.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.8.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.8.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.8.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.8.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.8.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.8.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.8.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.8.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.8.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.9 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.9.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.9.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.9.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.9.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.9.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.9.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.9.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.9.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.9.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.9.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.9.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.10 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.10.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.10.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.10.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.10.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.10.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.10.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.10.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.10.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.10.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.10.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.10.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.11 CLIPEncoderLayer(\n","  (self_attn): CLIPSdpaAttention(\n","    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (mlp): CLIPMLP(\n","    (activation_fn): QuickGELUActivation()\n","    (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","    (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","  )\n","  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","vision_model.encoder.layers.11.self_attn CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.11.self_attn.k_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.11.self_attn.v_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.11.self_attn.q_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.11.self_attn.out_proj Linear(in_features=768, out_features=768, bias=True)\n","vision_model.encoder.layers.11.layer_norm1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.encoder.layers.11.mlp CLIPMLP(\n","  (activation_fn): QuickGELUActivation()\n","  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",")\n","vision_model.encoder.layers.11.mlp.activation_fn QuickGELUActivation()\n","vision_model.encoder.layers.11.mlp.fc1 Linear(in_features=768, out_features=3072, bias=True)\n","vision_model.encoder.layers.11.mlp.fc2 Linear(in_features=3072, out_features=768, bias=True)\n","vision_model.encoder.layers.11.layer_norm2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","vision_model.post_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","visual_projection Linear(in_features=768, out_features=512, bias=False)\n","text_projection Linear(in_features=512, out_features=512, bias=False)\n"]}]},{"cell_type":"markdown","source":["### CLIP vision model의 self attention layer 속 forward 함수 코드 얻기"],"metadata":{"id":"khSi9E9ljVVb"}},{"cell_type":"code","source":["self_attn_layer1 = clip_model.vision_model.encoder.layers[0].self_attn\n","print(self_attn_layer1)\n","\n","# k_proj와 q_proj 확인\n","print(\"Key Projection:\", self_attn_layer1.k_proj)\n","print(\"Query Projection:\", self_attn_layer1.q_proj.weight)\n","\n","# Forward 함수 확인\n","import inspect\n","forward_code = inspect.getsource(self_attn_layer1.forward)\n","print(forward_code)\n","\n","# 파일에 저장\n","file_name = \"self_attention_layer1_forward.py\"\n","with open(file_name, \"w\") as f:\n","    f.write(forward_code)\n","\n","print(f\"Forward code saved to {file_name}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"A9kUFbqQvoTf","executionInfo":{"status":"ok","timestamp":1733142414231,"user_tz":-540,"elapsed":614,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"5a30319e-ed5d-480a-c7f8-9fadefb3decf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["CLIPSdpaAttention(\n","  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",")\n","Key Projection: Linear(in_features=768, out_features=768, bias=True)\n","Query Projection: Parameter containing:\n","tensor([[-0.0002,  0.0268, -0.0010,  ..., -0.0179, -0.0128,  0.0003],\n","        [ 0.0638,  0.0078,  0.0008,  ..., -0.0288, -0.0222, -0.0140],\n","        [-0.0345,  0.0409, -0.0006,  ...,  0.0058,  0.0077, -0.0119],\n","        ...,\n","        [-0.0252, -0.0552, -0.0038,  ...,  0.0020, -0.0012, -0.0017],\n","        [-0.0281,  0.0301, -0.0010,  ..., -0.0094, -0.0043, -0.0051],\n","        [ 0.0312, -0.0452, -0.0044,  ..., -0.0125, -0.0114,  0.0095]],\n","       requires_grad=True)\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        causal_attention_mask: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = False,\n","    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n","        if output_attentions:\n","            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n","            logger.warning_once(\n","                \"CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not \"\n","                \"support `output_attentions=True`. Falling back to the manual attention implementation, but specifying \"\n","                \"the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can \"\n","                'be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n","            )\n","            return super().forward(\n","                hidden_states=hidden_states,\n","                attention_mask=attention_mask,\n","                causal_attention_mask=causal_attention_mask,\n","                output_attentions=output_attentions,\n","            )\n","\n","        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n","        if attention_mask is not None and causal_attention_mask is not None:\n","            attn_mask = attention_mask + causal_attention_mask\n","        elif causal_attention_mask is not None:\n","            attn_mask = causal_attention_mask\n","        else:\n","            attn_mask = attention_mask\n","\n","        bsz, tgt_len, embed_dim = hidden_states.size()\n","\n","        query_states = self.q_proj(hidden_states)\n","        key_states = self.k_proj(hidden_states)\n","        value_states = self.v_proj(hidden_states)\n","\n","        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n","        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n","        if not is_torch_greater_or_equal_than_2_2 and query_states.device.type == \"cuda\" and attn_mask is not None:\n","            query_states = query_states.contiguous()\n","            key_states = key_states.contiguous()\n","            value_states = value_states.contiguous()\n","\n","        # CLIP text model uses both `causal_attention_mask` and `attention_mask` sequentially.\n","        attn_output = torch.nn.functional.scaled_dot_product_attention(\n","            query_states,\n","            key_states,\n","            value_states,\n","            attn_mask=attn_mask,\n","            dropout_p=self.dropout if self.training else 0.0,\n","            scale=self.scale,\n","        )\n","\n","        attn_output = attn_output.transpose(1, 2)\n","        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n","\n","        attn_output = self.out_proj(attn_output)\n","\n","        return attn_output, None\n","\n","Forward code saved to self_attention_layer1_forward.py\n"]}]},{"cell_type":"markdown","source":["###Self Attention Layer의 forward 함수 수정\n","\n","attention layer의 query, key 부분을 identity matrix로 대체.\n","이로 인해 output은 value값이 그대로 반영됨"],"metadata":{"id":"30kPlxXjS2sY"}},{"cell_type":"code","source":["from typing import Optional, Tuple\n","from types import MethodType\n","from packaging import version\n","is_torch_greater_or_equal_than_2_2 = version.parse(torch.__version__) >= version.parse(\"2.2.0\")\n","\n","\n","def forward_modified(\n","    self,\n","    hidden_states: torch.Tensor,\n","    attention_mask: Optional[torch.Tensor] = None,\n","    causal_attention_mask: Optional[torch.Tensor] = None,\n","    output_attentions: Optional[bool] = False,\n",") -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n","    if output_attentions:\n","        # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n","        logger.warning_once(\n","            \"CLIPModel is using CLIPSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not \"\n","            \"support `output_attentions=True`. Falling back to the manual attention implementation, but specifying \"\n","            \"the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can \"\n","            'be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n","        )\n","        return super().forward(\n","            hidden_states=hidden_states,\n","            attention_mask=attention_mask,\n","            causal_attention_mask=causal_attention_mask,\n","            output_attentions=output_attentions,\n","        )\n","\n","    # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n","    if attention_mask is not None and causal_attention_mask is not None:\n","        attn_mask = attention_mask + causal_attention_mask\n","    elif causal_attention_mask is not None:\n","        attn_mask = causal_attention_mask\n","    else:\n","        attn_mask = attention_mask\n","\n","    bsz, tgt_len, embed_dim = hidden_states.size()\n","\n","    query_states = self.q_proj(hidden_states)\n","    key_states = self.k_proj(hidden_states)\n","    value_states = self.v_proj(hidden_states)\n","\n","\n","    # print(query_states.size())\n","    # print(key_states.size())\n","    # print(value_states.size())\n","\n","\n","    query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","    key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","    value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n","    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n","    if not is_torch_greater_or_equal_than_2_2 and query_states.device.type == \"cuda\" and attn_mask is not None:\n","        query_states = query_states.contiguous()\n","        key_states = key_states.contiguous()\n","        value_states = value_states.contiguous()\n","\n","    # CLIP text model uses both `causal_attention_mask` and `attention_mask` sequentially.\n","    '''attn_output = torch.nn.functional.scaled_dot_product_attention(\n","        query_states,\n","        key_states,\n","        value_states,\n","        attn_mask=attn_mask,\n","        dropout_p=self.dropout if self.training else 0.0,\n","        scale=self.scale,\n","    )'''\n","    attn_output = value_states ## 수정\n","    attn_output = attn_output.transpose(1, 2)\n","    attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n","\n","    attn_output = self.out_proj(attn_output)\n","\n","    return attn_output, None"],"metadata":{"id":"aD49EK8X9vQ3","executionInfo":{"status":"ok","timestamp":1733316737636,"user_tz":-540,"elapsed":330,"user":{"displayName":"김유진","userId":"04225960051398709816"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#**Modified CLIP**"],"metadata":{"id":"kZpy9wHATBnm"}},{"cell_type":"code","source":["# Load the CLIP model and processor\n","modified_clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","modified_clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"],"metadata":{"id":"VmSdYhlqV49p","executionInfo":{"status":"ok","timestamp":1733318495766,"user_tz":-540,"elapsed":1494,"user":{"displayName":"김유진","userId":"04225960051398709816"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# 수정된 forward 함수를 Layer들에 적용 [customize!!]\n","for i in range(6,9):\n","  self_attn_layer_i = modified_clip_model.vision_model.encoder.layers[i].self_attn\n","  self_attn_layer_i.forward = MethodType(forward_modified, self_attn_layer_i)"],"metadata":{"id":"IKpfoadKBvoX","executionInfo":{"status":"ok","timestamp":1733318503498,"user_tz":-540,"elapsed":5,"user":{"displayName":"김유진","userId":"04225960051398709816"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### MSCOCO Dataset 결과"],"metadata":{"id":"KaNa89GMkJ8J"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# Initialize variables for average score calculation\n","total_similarity_score = 0.0\n","processed_images = 0\n","num_images = total_images  # Number of images to process (adjust as needed)\n","\n","# Candidate captions\n","# candidate_captions = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of an animal\", \"a photo of a chair\"]\n","candidate_captions = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a bird\", \"a photo of a car\"]\n","\n","# Loop through filtered images\n","for image_id, captions in tqdm(list(captions_by_image.items())[:num_images]):\n","    # Load image from URL\n","    image_url = images_info[image_id][\"coco_url\"]\n","    image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n","    print(image_url)\n","    # plt.imshow(image)\n","    # plt.title(f\"Image ID: {image_id}\")\n","    # plt.axis(\"off\")\n","    # plt.show()\n","\n","    # CLIP score for candidate captions\n","    inputs = modified_clip_processor(text=candidate_captions, images=image, return_tensors=\"pt\", padding=True)\n","    outputs = modified_clip_model(**inputs)\n","\n","    # Compute similarity scores\n","    logits_per_image = outputs.logits_per_image\n","    print(logits_per_image)\n","    probs = logits_per_image.softmax(dim=1)\n","    best_caption_index = torch.argmax(probs).item()\n","    best_caption = candidate_captions[best_caption_index]\n","    print(f\"Best caption by CLIP for image {image_id}: {best_caption}\")\n","\n","    # # Calculate CLIP similarity score for best caption\n","    # pixel_values = clip_processor(images=image, return_tensors=\"pt\").pixel_values\n","    # image_features = clip_model.get_image_features(pixel_values)\n","\n","    # text_inputs = clip_processor(text=[best_caption], return_tensors=\"pt\", padding=True)\n","    # text_features = clip_model.get_text_features(**text_inputs)\n","\n","    # # Normalize embeddings\n","    # image_features = F.normalize(image_features, p=2, dim=1)\n","    # text_features = F.normalize(text_features, p=2, dim=1)\n","\n","    # # Cosine similarity\n","    # similarity_score = (image_features @ text_features.T).item()\n","    # print(f\"CLIP Similarity Score for image {image_id}: {similarity_score}\")\n","\n","    # Generate a detailed caption using the captioning model\n","    pixel_values = modified_clip_processor(images=image, return_tensors=\"pt\").pixel_values\n","    attention_mask = torch.ones(pixel_values.shape[:2], dtype=torch.long)\n","\n","    generated_ids = caption_model.generate(\n","        pixel_values,\n","        attention_mask=attention_mask,  # Explicitly pass the attention mask\n","        max_length=50,                 # Set a custom maximum length for the output\n","        pad_token_id=caption_tokenizer.pad_token_id  # Use the tokenizer's pad token ID\n","    )\n","    generated_caption = caption_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","    print(f\"Generated Caption: {generated_caption}\")\n","\n","    # CLIP의 Best Caption을 Caption Generation 모델의 입력에 포함\n","    best_caption = candidate_captions[best_caption_index]\n","    input_ids = caption_tokenizer(best_caption, return_tensors=\"pt\").input_ids\n","\n","    generated_ids = caption_model.generate(\n","        pixel_values,\n","        input_ids=input_ids,  # CLIP의 Best Caption 반영\n","        attention_mask=attention_mask,\n","        max_length=50,\n","        pad_token_id=caption_tokenizer.pad_token_id\n","    )\n","    generated_caption = caption_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","    print(f\"Generated Caption with CLIP influence: {generated_caption}\\n\")\n","\n","    # Calculate CLIP similarity score for generated caption with clip influence\n","    pixel_values = clip_processor(images=image, return_tensors=\"pt\").pixel_values\n","    image_features = clip_model.get_image_features(pixel_values)\n","\n","    text_inputs = clip_processor(text=[generated_caption], return_tensors=\"pt\", padding=True)\n","    text_features = clip_model.get_text_features(**text_inputs)\n","\n","    # Normalize embeddings\n","    image_features = F.normalize(image_features, p=2, dim=1)\n","    text_features = F.normalize(text_features, p=2, dim=1)\n","\n","    # Cosine similarity\n","    similarity_score = (image_features @ text_features.T).item()\n","    print(f\"CLIP Similarity Score for image {image_id}: {similarity_score}\")\n","\n","\n","    # Update total score and processed image count\n","    total_similarity_score += similarity_score\n","    processed_images += 1\n","\n","# Calculate and print the average CLIP similarity score\n","average_similarity_score = total_similarity_score / processed_images\n","print(f\"\\nAverage CLIP Similarity Score for {processed_images} images: {average_similarity_score}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"5QEB_1YmD4Sq","executionInfo":{"status":"ok","timestamp":1733319082259,"user_tz":-540,"elapsed":576305,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"77b1b970-ace6-41bb-961b-181b725e46fd"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/349 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["http://images.cocodataset.org/val2017/000000046378.jpg\n","tensor([[22.5199, 24.4677, 23.9899, 26.6227]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 46378: a photo of a car\n","Generated Caption: a cat with a bite out of it's mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 1/349 [00:01<09:26,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 46378: 0.21293078362941742\n","http://images.cocodataset.org/val2017/000000172330.jpg\n","tensor([[22.3142, 24.6054, 24.0991, 26.5793]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 172330: a photo of a car\n","Generated Caption: a cat standing on the side of a road \n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 2/349 [00:03<09:13,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the hood \n","\n","CLIP Similarity Score for image 172330: 0.28767314553260803\n","http://images.cocodataset.org/val2017/000000223747.jpg\n","tensor([[22.9552, 25.1239, 24.5796, 26.8849]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 223747: a photo of a car\n","Generated Caption: a person laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 3/349 [00:04<08:29,  1.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a person sleeping \n","\n","CLIP Similarity Score for image 223747: 0.25637322664260864\n","http://images.cocodataset.org/val2017/000000482917.jpg\n","tensor([[22.3332, 24.4680, 23.9590, 26.4748]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 482917: a photo of a car\n","Generated Caption: a person laying on the floor with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 4/349 [00:06<08:37,  1.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a person laying on the floor \n","\n","CLIP Similarity Score for image 482917: 0.21514996886253357\n","http://images.cocodataset.org/val2017/000000065485.jpg\n","tensor([[22.4087, 24.3693, 23.7326, 26.7463]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 65485: a photo of a car\n","Generated Caption: a red truck parked in a parking lot \n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|▏         | 5/349 [00:07<08:50,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a red and white truck \n","\n","CLIP Similarity Score for image 65485: 0.24997827410697937\n","http://images.cocodataset.org/val2017/000000255965.jpg\n","tensor([[22.5480, 24.5270, 24.2127, 26.5874]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 255965: a photo of a car\n","Generated Caption: a cat sitting on a ledge looking at the camera \n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 6/349 [00:09<08:58,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the ground \n","\n","CLIP Similarity Score for image 255965: 0.25154781341552734\n","http://images.cocodataset.org/val2017/000000010363.jpg\n","tensor([[21.9545, 24.0144, 23.3246, 26.3595]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 10363: a photo of a car\n","Generated Caption: a cat sitting on top of a car hood \n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 7/349 [00:10<08:57,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the hood \n","\n","CLIP Similarity Score for image 10363: 0.3113110661506653\n","http://images.cocodataset.org/val2017/000000558073.jpg\n","tensor([[22.5268, 24.5303, 24.0107, 26.7138]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 558073: a photo of a car\n","Generated Caption: a cat sitting on a wall next to a window \n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 8/349 [00:12<09:40,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting on the floor \n","\n","CLIP Similarity Score for image 558073: 0.2460295706987381\n","http://images.cocodataset.org/val2017/000000565391.jpg\n","tensor([[22.3093, 24.3170, 23.7115, 26.3637]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 565391: a photo of a car\n","Generated Caption: a black and white bird is sitting on a car \n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 9/349 [00:14<09:25,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a bird on the hood \n","\n","CLIP Similarity Score for image 565391: 0.27331575751304626\n","http://images.cocodataset.org/val2017/000000304560.jpg\n","tensor([[22.4879, 24.3561, 23.9795, 26.1419]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 304560: a photo of a car\n","Generated Caption: a cat sitting in the grass looking at the camera \n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 10/349 [00:16<09:22,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a black cat \n","\n","CLIP Similarity Score for image 304560: 0.29808810353279114\n","http://images.cocodataset.org/val2017/000000153217.jpg\n","tensor([[22.4006, 24.4569, 24.0666, 26.6288]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 153217: a photo of a car\n","Generated Caption: a black dog is running on a leash \n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 11/349 [00:17<09:00,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the back \n","\n","CLIP Similarity Score for image 153217: 0.22158221900463104\n","http://images.cocodataset.org/val2017/000000209747.jpg\n","tensor([[22.7334, 24.8983, 24.2849, 26.7478]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 209747: a photo of a car\n","Generated Caption: a cat sitting in a sink with a toothbrush in it's mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 12/349 [00:19<09:00,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat in it \n","\n","CLIP Similarity Score for image 209747: 0.2237672209739685\n","http://images.cocodataset.org/val2017/000000134096.jpg\n","tensor([[22.5987, 24.6462, 23.9804, 26.3037]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 134096: a photo of a car\n","Generated Caption: a cat sitting on a sink in a bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▎         | 13/349 [00:20<09:08,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting on the sink \n","\n","CLIP Similarity Score for image 134096: 0.2981395125389099\n","http://images.cocodataset.org/val2017/000000163155.jpg\n","tensor([[22.5793, 24.7359, 24.2484, 26.5437]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 163155: a photo of a car\n","Generated Caption: a cat sitting on a wooden bench \n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 14/349 [00:22<08:49,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a wooden bench \n","\n","CLIP Similarity Score for image 163155: 0.22117695212364197\n","http://images.cocodataset.org/val2017/000000360943.jpg\n","tensor([[22.0974, 24.3263, 23.5851, 26.6070]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 360943: a photo of a car\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 15/349 [00:24<09:01,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a cat sitting on the sill \n","\n","CLIP Similarity Score for image 360943: 0.29505661129951477\n","http://images.cocodataset.org/val2017/000000284623.jpg\n","tensor([[22.8471, 24.7029, 24.3486, 26.5316]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 284623: a photo of a car\n","Generated Caption: a cat sitting on a sink next to a mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 16/349 [00:26<09:46,  1.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the floor \n","\n","CLIP Similarity Score for image 284623: 0.22293995320796967\n","http://images.cocodataset.org/val2017/000000554002.jpg\n","tensor([[22.0279, 24.1353, 23.6213, 26.2203]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 554002: a photo of a car\n","Generated Caption: a woman walking down a sidewalk with a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 17/349 [00:27<09:28,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a person walking on a sidewalk \n","\n","CLIP Similarity Score for image 554002: 0.22216738760471344\n","http://images.cocodataset.org/val2017/000000023272.jpg\n","tensor([[21.9783, 24.0366, 23.6747, 26.3164]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 23272: a photo of a car\n","Generated Caption: a dog is sitting on the hood of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 18/349 [00:29<09:15,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the hood \n","\n","CLIP Similarity Score for image 23272: 0.30087584257125854\n","http://images.cocodataset.org/val2017/000000067213.jpg\n","tensor([[22.4513, 24.8344, 24.1800, 26.6495]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 67213: a photo of a car\n","Generated Caption: a dog jumping in the air over a swimming pool \n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 19/349 [00:31<09:12,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog jumping in the air \n","\n","CLIP Similarity Score for image 67213: 0.3303055763244629\n","http://images.cocodataset.org/val2017/000000170893.jpg\n","tensor([[22.5918, 24.6406, 23.9785, 26.7339]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 170893: a photo of a car\n","Generated Caption: a dog is sitting on the toilet in the bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 20/349 [00:32<08:57,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in the bathroom \n","\n","CLIP Similarity Score for image 170893: 0.2952195405960083\n","http://images.cocodataset.org/val2017/000000107087.jpg\n","tensor([[22.3326, 24.3268, 23.7995, 26.4706]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 107087: a photo of a car\n","Generated Caption: a cat sitting in the hood of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 21/349 [00:34<08:54,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the hood \n","\n","CLIP Similarity Score for image 107087: 0.3235827386379242\n","http://images.cocodataset.org/val2017/000000366884.jpg\n","tensor([[21.8537, 23.8170, 23.3167, 25.9143]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 366884: a photo of a car\n","Generated Caption: a cat is sitting on a rug in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▋         | 22/349 [00:35<08:50,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car door and a cat \n","\n","CLIP Similarity Score for image 366884: 0.19362872838974\n","http://images.cocodataset.org/val2017/000000139099.jpg\n","tensor([[22.4490, 24.3559, 23.8005, 26.2425]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 139099: a photo of a car\n","Generated Caption: a man in a hat is on a bike \n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 23/349 [00:37<09:23,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with people on it \n","\n","CLIP Similarity Score for image 139099: 0.23479725420475006\n","http://images.cocodataset.org/val2017/000000404484.jpg\n","tensor([[22.0719, 24.3468, 23.6803, 26.4462]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 404484: a photo of a car\n","Generated Caption: a dog standing on a rug in front of a window \n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 24/349 [00:39<09:19,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the floor \n","\n","CLIP Similarity Score for image 404484: 0.243840754032135\n","http://images.cocodataset.org/val2017/000000227044.jpg\n","tensor([[22.4547, 24.4830, 23.6897, 26.6147]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 227044: a photo of a car\n","Generated Caption: a cat sitting on a sink in a bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 25/349 [00:41<09:08,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting on the sink \n","\n","CLIP Similarity Score for image 227044: 0.30325037240982056\n","http://images.cocodataset.org/val2017/000000472375.jpg\n","tensor([[22.3247, 24.4812, 23.7120, 26.6300]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 472375: a photo of a car\n","Generated Caption: a dog is sitting on a motorcycle \n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 26/349 [00:42<08:45,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the back \n","\n","CLIP Similarity Score for image 472375: 0.20187219977378845\n","http://images.cocodataset.org/val2017/000000101762.jpg\n","tensor([[21.5253, 23.6353, 23.2697, 25.9923]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 101762: a photo of a car\n","Generated Caption: a cat is standing on a bicycle tire \n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 27/349 [00:44<08:37,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the floor \n","\n","CLIP Similarity Score for image 101762: 0.2785238027572632\n","http://images.cocodataset.org/val2017/000000419974.jpg\n","tensor([[21.8564, 23.8484, 23.2242, 25.9763]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 419974: a photo of a car\n","Generated Caption: a man in a kitchen cutting a cake \n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 28/349 [00:45<08:28,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a man cutting a cake \n","\n","CLIP Similarity Score for image 419974: 0.23608392477035522\n","http://images.cocodataset.org/val2017/000000167122.jpg\n","tensor([[22.0652, 24.2258, 23.4508, 26.5387]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 167122: a photo of a car\n","Generated Caption: a car is driving down a street at night \n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 29/349 [00:47<08:26,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on a street at night \n","\n","CLIP Similarity Score for image 167122: 0.2832256257534027\n","http://images.cocodataset.org/val2017/000000365207.jpg\n","tensor([[21.9783, 24.1315, 23.3834, 26.4561]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 365207: a photo of a car\n","Generated Caption: a mirror reflecting a car's reflection in the side view mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▊         | 30/349 [00:49<09:06,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car mirror with a reflection of a dog \n","\n","CLIP Similarity Score for image 365207: 0.32966262102127075\n","http://images.cocodataset.org/val2017/000000209613.jpg\n","tensor([[22.0233, 24.4019, 24.0799, 26.2898]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 209613: a photo of a car\n","Generated Caption: sheep standing on top of a grass covered field \n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 31/349 [00:51<09:26,  1.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car herd of sheep \n","\n","CLIP Similarity Score for image 209613: 0.3063301146030426\n","http://images.cocodataset.org/val2017/000000386457.jpg\n","tensor([[22.2005, 24.2965, 23.7780, 26.2742]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 386457: a photo of a car\n","Generated Caption: a cat looking out of a window \n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 32/349 [00:52<08:41,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car door opening \n","\n","CLIP Similarity Score for image 386457: 0.23433685302734375\n","http://images.cocodataset.org/val2017/000000077396.jpg\n","tensor([[21.7273, 23.7254, 22.8048, 26.1229]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 77396: a photo of a car\n","Generated Caption: a cat is watching a television on the floor \n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 33/349 [00:54<08:32,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and cat on a television \n","\n","CLIP Similarity Score for image 77396: 0.32360613346099854\n","http://images.cocodataset.org/val2017/000000312340.jpg\n","tensor([[22.5030, 24.3302, 23.9037, 26.3862]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 312340: a photo of a car\n","Generated Caption: a cat sitting in a tree with a window \n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|▉         | 34/349 [00:55<08:27,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting in the window \n","\n","CLIP Similarity Score for image 312340: 0.2717823088169098\n","http://images.cocodataset.org/val2017/000000530099.jpg\n","tensor([[22.8187, 24.7254, 24.3025, 26.4554]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 530099: a photo of a car\n","Generated Caption: a cat sitting on a car window sill \n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 35/349 [00:57<08:09,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the hood \n","\n","CLIP Similarity Score for image 530099: 0.3418656885623932\n","http://images.cocodataset.org/val2017/000000149222.jpg\n","tensor([[22.1315, 24.1753, 23.4920, 26.1733]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 149222: a photo of a car\n","Generated Caption: a computer monitor and keyboard on a desk \n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 36/349 [00:58<07:39,  1.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a computer monitor \n","\n","CLIP Similarity Score for image 149222: 0.2876583933830261\n","http://images.cocodataset.org/val2017/000000329219.jpg\n","tensor([[22.0300, 24.0398, 23.3940, 26.1926]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 329219: a photo of a car\n","Generated Caption: a man standing next to a dog on a kitchen floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 37/349 [01:00<08:04,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog standing in the middle \n","\n","CLIP Similarity Score for image 329219: 0.23532336950302124\n","http://images.cocodataset.org/val2017/000000245764.jpg\n","tensor([[22.5138, 24.4499, 24.0364, 26.5157]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 245764: a photo of a car\n","Generated Caption: a cat is sitting on a toilet seat \n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 38/349 [01:01<08:08,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the toilet seat \n","\n","CLIP Similarity Score for image 245764: 0.32078444957733154\n","http://images.cocodataset.org/val2017/000000131273.jpg\n","tensor([[22.9167, 24.8706, 24.3299, 26.8595]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 131273: a photo of a car\n","Generated Caption: a dog sitting in the driver's seat of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 39/349 [01:03<09:02,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in the driver seat \n","\n","CLIP Similarity Score for image 131273: 0.29593372344970703\n","http://images.cocodataset.org/val2017/000000169076.jpg\n","tensor([[22.1690, 24.2306, 23.6114, 26.5351]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 169076: a photo of a car\n","Generated Caption: a black and white dog looking at a tv \n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█▏        | 40/349 [01:05<08:42,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog looking at it \n","\n","CLIP Similarity Score for image 169076: 0.23445071280002594\n","http://images.cocodataset.org/val2017/000000466156.jpg\n","tensor([[22.1755, 24.3436, 23.7747, 26.3596]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 466156: a photo of a car\n","Generated Caption: a cat standing on top of a car hood \n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 41/349 [01:06<08:20,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on the street \n","\n","CLIP Similarity Score for image 466156: 0.25997990369796753\n","http://images.cocodataset.org/val2017/000000279278.jpg\n","tensor([[22.3096, 24.2756, 24.0102, 26.2302]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 279278: a photo of a car\n","Generated Caption: a woman is standing on a skateboard with a skateboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 42/349 [01:08<08:31,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a girl on a skateboard \n","\n","CLIP Similarity Score for image 279278: 0.33424800634384155\n","http://images.cocodataset.org/val2017/000000579321.jpg\n","tensor([[22.4856, 24.4749, 24.0111, 26.3801]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 579321: a photo of a car\n","Generated Caption: a dog laying on the ground with its head on a person's shoulder \n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 43/349 [01:10<08:48,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on the ground \n","\n","CLIP Similarity Score for image 579321: 0.25105682015419006\n","http://images.cocodataset.org/val2017/000000289343.jpg\n","tensor([[22.2399, 24.3906, 23.6606, 26.3852]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 289343: a photo of a car\n","Generated Caption: a man riding a skateboard down a sidewalk \n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 44/349 [01:12<08:25,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on a street \n","\n","CLIP Similarity Score for image 289343: 0.22455836832523346\n","http://images.cocodataset.org/val2017/000000498286.jpg\n","tensor([[22.8535, 24.7490, 24.5268, 26.4476]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 498286: a photo of a car\n","Generated Caption: a dog with a tag on its face \n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 45/349 [01:13<08:05,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog \n","\n","CLIP Similarity Score for image 498286: 0.22989022731781006\n","http://images.cocodataset.org/val2017/000000494869.jpg\n","tensor([[21.9127, 23.9388, 23.4244, 26.0540]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 494869: a photo of a car\n","Generated Caption: a woman and a child are in a kitchen \n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 46/349 [01:15<08:52,  1.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a woman in a kitchen \n","\n","CLIP Similarity Score for image 494869: 0.3107788562774658\n","http://images.cocodataset.org/val2017/000000129756.jpg\n","tensor([[21.8084, 24.0080, 23.7599, 25.9966]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 129756: a photo of a car\n","Generated Caption: a man standing in a field with sheep \n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 47/349 [01:17<08:49,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a herd of sheep \n","\n","CLIP Similarity Score for image 129756: 0.301022469997406\n","http://images.cocodataset.org/val2017/000000568690.jpg\n","tensor([[22.2636, 24.2960, 23.6162, 26.3218]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 568690: a photo of a car\n","Generated Caption: a cat sitting on top of a toilet bowl \n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 48/349 [01:18<08:36,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting on the tank \n","\n","CLIP Similarity Score for image 568690: 0.23935681581497192\n","http://images.cocodataset.org/val2017/000000061471.jpg\n","tensor([[22.2948, 24.3600, 23.6591, 26.3018]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 61471: a photo of a car\n","Generated Caption: a dog is standing in the bathroom with its mouth open \n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 49/349 [01:20<08:31,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in the bathroom \n","\n","CLIP Similarity Score for image 61471: 0.29650160670280457\n","http://images.cocodataset.org/val2017/000000047121.jpg\n","tensor([[22.3191, 24.4713, 23.6516, 26.6477]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 47121: a photo of a car\n","Generated Caption: a cat drinking water from a sink \n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 50/349 [01:22<08:02,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat drinking water \n","\n","CLIP Similarity Score for image 47121: 0.26985377073287964\n","http://images.cocodataset.org/val2017/000000007386.jpg\n","tensor([[21.5784, 23.7436, 23.1605, 26.1567]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 7386: a photo of a car\n","Generated Caption: a motorcycle with a dog on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 51/349 [01:23<07:57,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a motorcycle on it \n","\n","CLIP Similarity Score for image 7386: 0.2785927355289459\n","http://images.cocodataset.org/val2017/000000267300.jpg\n","tensor([[21.9700, 24.0590, 23.5553, 26.3393]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 267300: a photo of a car\n","Generated Caption: a dog laying on a bed with a plate of food \n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 52/349 [01:25<07:59,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 267300: 0.16229011118412018\n","http://images.cocodataset.org/val2017/000000327769.jpg\n","tensor([[22.0981, 24.2774, 23.3795, 26.3948]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 327769: a photo of a car\n","Generated Caption: a cat laying in a bathroom sink \n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 53/349 [01:26<07:46,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat laying in the sink \n","\n","CLIP Similarity Score for image 327769: 0.3304702639579773\n","http://images.cocodataset.org/val2017/000000176778.jpg\n","tensor([[22.1679, 24.2383, 23.6476, 26.3321]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 176778: a photo of a car\n","Generated Caption: a white toilet sitting next to a bath tub \n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 54/349 [01:28<08:27,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a toilet in a bathroom \n","\n","CLIP Similarity Score for image 176778: 0.28536456823349\n","http://images.cocodataset.org/val2017/000000260925.jpg\n","tensor([[22.4999, 24.4922, 23.9839, 26.6566]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 260925: a photo of a car\n","Generated Caption: a cat is laying on the hood of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 55/349 [01:30<08:15,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the hood \n","\n","CLIP Similarity Score for image 260925: 0.3044099807739258\n","http://images.cocodataset.org/val2017/000000520301.jpg\n","tensor([[22.7769, 24.6392, 24.1911, 26.6946]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 520301: a photo of a car\n","Generated Caption: a dog is sitting in the window of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 56/349 [01:32<08:04,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the back \n","\n","CLIP Similarity Score for image 520301: 0.27093830704689026\n","http://images.cocodataset.org/val2017/000000205834.jpg\n","tensor([[22.7442, 24.6845, 24.0737, 26.7229]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 205834: a photo of a car\n","Generated Caption: a dog is sitting in the dirt with a bowl \n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▋        | 57/349 [01:33<07:55,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in it \n","\n","CLIP Similarity Score for image 205834: 0.2191830426454544\n","http://images.cocodataset.org/val2017/000000491216.jpg\n","tensor([[21.7919, 23.7091, 23.1697, 26.1897]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 491216: a photo of a car\n","Generated Caption: a cat is standing on the floor in a kitchen \n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 58/349 [01:35<07:47,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a cat in a room \n","\n","CLIP Similarity Score for image 491216: 0.2574011981487274\n","http://images.cocodataset.org/val2017/000000109055.jpg\n","tensor([[21.7635, 23.8044, 23.6453, 26.3870]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 109055: a photo of a car\n","Generated Caption: a cat is standing on a bicycle \n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 59/349 [01:36<07:38,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the front \n","\n","CLIP Similarity Score for image 109055: 0.2565654218196869\n","http://images.cocodataset.org/val2017/000000501523.jpg\n","tensor([[22.2011, 24.1933, 23.3974, 26.3521]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 501523: a photo of a car\n","Generated Caption: a cat is sitting in a bathroom sink \n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 60/349 [01:38<07:45,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a black cat sitting on top of it \n","\n","CLIP Similarity Score for image 501523: 0.24887099862098694\n","http://images.cocodataset.org/val2017/000000125850.jpg\n","tensor([[22.4783, 24.4237, 24.0401, 26.4186]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 125850: a photo of a car\n","Generated Caption: a cat laying in a bowl on a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 61/349 [01:39<07:45,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat in it \n","\n","CLIP Similarity Score for image 125850: 0.23748016357421875\n","http://images.cocodataset.org/val2017/000000213445.jpg\n","tensor([[22.1225, 24.1360, 23.6001, 26.2411]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 213445: a photo of a car\n","Generated Caption: a cat sitting on a table in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 62/349 [01:41<07:48,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting in a room \n","\n","CLIP Similarity Score for image 213445: 0.1975766271352768\n","http://images.cocodataset.org/val2017/000000466339.jpg\n","tensor([[22.6210, 24.5389, 24.0223, 26.4071]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 466339: a photo of a car\n","Generated Caption: a door is open to a room with a wooden floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 63/349 [01:43<07:52,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car door with a door open \n","\n","CLIP Similarity Score for image 466339: 0.26446080207824707\n","http://images.cocodataset.org/val2017/000000456292.jpg\n","tensor([[22.2366, 24.1725, 23.6597, 26.2378]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 456292: a photo of a car\n","Generated Caption: a cat standing on a wall next to a graffiti covered wall \n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 64/349 [01:44<07:48,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car painted with graffiti \n","\n","CLIP Similarity Score for image 456292: 0.2332640141248703\n","http://images.cocodataset.org/val2017/000000225184.jpg\n","tensor([[22.4759, 25.0702, 24.3741, 26.6033]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 225184: a photo of a car\n","Generated Caption: a herd of sheep standing on top of a lush green field \n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▊        | 65/349 [01:46<07:55,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and sheep in a field \n","\n","CLIP Similarity Score for image 225184: 0.2945556342601776\n","http://images.cocodataset.org/val2017/000000176857.jpg\n","tensor([[22.2067, 24.2586, 23.5866, 26.2427]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 176857: a photo of a car\n","Generated Caption: a woman is sitting on a bench in a museum \n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 66/349 [01:48<07:41,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with people standing around \n","\n","CLIP Similarity Score for image 176857: 0.20300041139125824\n","http://images.cocodataset.org/val2017/000000402473.jpg\n","tensor([[23.4973, 25.0845, 24.4847, 26.7827]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 402473: a photo of a car\n","Generated Caption: a cat laying on a wall next to a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 67/349 [01:49<07:30,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and cat laying on a wall \n","\n","CLIP Similarity Score for image 402473: 0.2635745406150818\n","http://images.cocodataset.org/val2017/000000211042.jpg\n","tensor([[22.1540, 24.3375, 23.5877, 26.2124]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 211042: a photo of a car\n","Generated Caption: a cat standing on a toilet in a bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 68/349 [01:51<07:33,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting on the floor \n","\n","CLIP Similarity Score for image 211042: 0.21292585134506226\n","http://images.cocodataset.org/val2017/000000361621.jpg\n","tensor([[22.8986, 24.7378, 24.2664, 26.5100]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 361621: a photo of a car\n","Generated Caption: a cat is looking at the camera \n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|█▉        | 69/349 [01:52<07:20,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the back \n","\n","CLIP Similarity Score for image 361621: 0.21906903386116028\n","http://images.cocodataset.org/val2017/000000078823.jpg\n","tensor([[22.4331, 24.2502, 23.8637, 26.5084]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 78823: a photo of a car\n","Generated Caption: a dog sitting on top of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 70/349 [01:54<07:55,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the hood \n","\n","CLIP Similarity Score for image 78823: 0.3268486261367798\n","http://images.cocodataset.org/val2017/000000061108.jpg\n","tensor([[22.2139, 24.1187, 23.6140, 26.4137]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 61108: a photo of a car\n","Generated Caption: a bike parked next to a bench \n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 71/349 [01:56<07:40,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car parked next to a bike rack \n","\n","CLIP Similarity Score for image 61108: 0.2632635533809662\n","http://images.cocodataset.org/val2017/000000554291.jpg\n","tensor([[22.6036, 24.7660, 24.2832, 26.5792]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 554291: a photo of a car\n","Generated Caption: a cat is playing with a toy in a bowl \n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 72/349 [01:58<07:32,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat in it \n","\n","CLIP Similarity Score for image 554291: 0.21600058674812317\n","http://images.cocodataset.org/val2017/000000068078.jpg\n","tensor([[22.3907, 24.5750, 24.0792, 26.5705]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 68078: a photo of a car\n","Generated Caption: a bathroom with a sink, toilet and mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 73/349 [01:59<07:13,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car wash basin and sink \n","\n","CLIP Similarity Score for image 68078: 0.2692001461982727\n","http://images.cocodataset.org/val2017/000000424162.jpg\n","tensor([[21.5461, 23.7024, 23.1908, 26.0262]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 424162: a photo of a car\n","Generated Caption: a woman riding a bike with a dog on a leash \n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 74/349 [02:01<07:24,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a woman riding a bike \n","\n","CLIP Similarity Score for image 424162: 0.2864609658718109\n","http://images.cocodataset.org/val2017/000000157807.jpg\n","tensor([[22.3911, 24.3182, 23.7266, 26.3766]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 157807: a photo of a car\n","Generated Caption: a cat standing on top of a toilet bowl \n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██▏       | 75/349 [02:02<07:24,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the top of it \n","\n","CLIP Similarity Score for image 157807: 0.2037702351808548\n","http://images.cocodataset.org/val2017/000000060835.jpg\n","tensor([[21.7163, 24.0145, 23.4205, 26.1897]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 60835: a photo of a car\n","Generated Caption: a dog is standing in a cage with a fence \n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 76/349 [02:04<07:29,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in the cage \n","\n","CLIP Similarity Score for image 60835: 0.2812301516532898\n","http://images.cocodataset.org/val2017/000000240940.jpg\n","tensor([[21.9001, 24.0246, 23.6229, 26.1121]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 240940: a photo of a car\n","Generated Caption: a cat sitting on the floor watching television \n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 77/349 [02:06<07:26,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the floor \n","\n","CLIP Similarity Score for image 240940: 0.28105881810188293\n","http://images.cocodataset.org/val2017/000000118515.jpg\n","tensor([[22.7298, 24.7786, 24.3428, 26.6325]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 118515: a photo of a car\n","Generated Caption: a small kitten sitting on a wooden bench \n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 78/349 [02:07<07:41,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a wooden bench \n","\n","CLIP Similarity Score for image 118515: 0.21957823634147644\n","http://images.cocodataset.org/val2017/000000119233.jpg\n","tensor([[22.2480, 24.4127, 23.8266, 26.3841]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 119233: a photo of a car\n","Generated Caption: a cat laying on top of a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 79/349 [02:09<07:31,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 119233: 0.2804657220840454\n","http://images.cocodataset.org/val2017/000000377575.jpg\n","tensor([[21.9167, 24.0074, 23.6005, 26.2927]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 377575: a photo of a car\n","Generated Caption: a dog wearing a santa clause hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 80/349 [02:11<07:11,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a bear on it \n","\n","CLIP Similarity Score for image 377575: 0.18750569224357605\n","http://images.cocodataset.org/val2017/000000291664.jpg\n","tensor([[22.2264, 24.4791, 23.8535, 26.3312]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 291664: a photo of a car\n","Generated Caption: a dog is standing next to a fire hydrant \n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 81/349 [02:12<07:08,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a dog \n","\n","CLIP Similarity Score for image 291664: 0.22460559010505676\n","http://images.cocodataset.org/val2017/000000222235.jpg\n","tensor([[21.9424, 23.9982, 23.6668, 26.2948]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 222235: a photo of a car\n","Generated Caption: a cat sitting in a tree next to a plant \n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 82/349 [02:14<07:10,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat in it \n","\n","CLIP Similarity Score for image 222235: 0.20144841074943542\n","http://images.cocodataset.org/val2017/000000117374.jpg\n","tensor([[21.9365, 24.2097, 23.7239, 26.1362]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 117374: a photo of a car\n","Generated Caption: a cat is sitting on a tree branch \n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 83/349 [02:15<06:58,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a cat \n","\n","CLIP Similarity Score for image 117374: 0.25429579615592957\n","http://images.cocodataset.org/val2017/000000080666.jpg\n","tensor([[22.3755, 24.3318, 23.6494, 26.4262]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 80666: a photo of a car\n","Generated Caption: a cat sitting on a sidewalk next to a building \n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 84/349 [02:17<07:00,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 80666: 0.24383942782878876\n","http://images.cocodataset.org/val2017/000000088951.jpg\n","tensor([[22.1910, 24.2228, 23.8853, 26.2578]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 88951: a photo of a car\n","Generated Caption: a man sitting on a bench next to a tree \n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 85/349 [02:19<07:15,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a park bench \n","\n","CLIP Similarity Score for image 88951: 0.24626529216766357\n","http://images.cocodataset.org/val2017/000000117525.jpg\n","tensor([[22.2128, 24.3337, 23.7601, 26.4445]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 117525: a photo of a car\n","Generated Caption: a man with a dog wearing a christmas hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▍       | 86/349 [02:21<07:43,  1.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a christmas tree \n","\n","CLIP Similarity Score for image 117525: 0.2160353809595108\n","http://images.cocodataset.org/val2017/000000029393.jpg\n","tensor([[22.0443, 24.1259, 23.5307, 26.2385]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 29393: a photo of a car\n","Generated Caption: a dog standing on top of a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▍       | 87/349 [02:22<07:27,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on top of it \n","\n","CLIP Similarity Score for image 29393: 0.23084238171577454\n","http://images.cocodataset.org/val2017/000000305343.jpg\n","tensor([[22.2941, 24.4192, 23.9089, 26.7187]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 305343: a photo of a car\n","Generated Caption: two paintings of a man and a woman \n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 88/349 [02:24<07:21,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a painting of a man and a bird \n","\n","CLIP Similarity Score for image 305343: 0.14654093980789185\n","http://images.cocodataset.org/val2017/000000532530.jpg\n","tensor([[22.1527, 24.2044, 23.4730, 26.2197]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 532530: a photo of a car\n","Generated Caption: a sign that says \"no parking\" on a street \n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 89/349 [02:25<07:07,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on a street \n","\n","CLIP Similarity Score for image 532530: 0.20036351680755615\n","http://images.cocodataset.org/val2017/000000224200.jpg\n","tensor([[21.8121, 23.9280, 23.3070, 26.2920]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 224200: a photo of a car\n","Generated Caption: a cat sitting on the sidewalk next to a fire hydrant \n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 90/349 [02:27<07:11,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the hood \n","\n","CLIP Similarity Score for image 224200: 0.17578592896461487\n","http://images.cocodataset.org/val2017/000000361571.jpg\n","tensor([[22.3211, 24.7288, 24.3138, 26.5231]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 361571: a photo of a car\n","Generated Caption: a dog wearing a hat and a hat hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 91/349 [02:29<07:08,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it's head \n","\n","CLIP Similarity Score for image 361571: 0.21887290477752686\n","http://images.cocodataset.org/val2017/000000452891.jpg\n","tensor([[22.0463, 24.0970, 23.6010, 26.2909]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 452891: a photo of a car\n","Generated Caption: a dog sitting on a bench with a leash \n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▋       | 92/349 [02:30<06:54,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a bench \n","\n","CLIP Similarity Score for image 452891: 0.22072479128837585\n","http://images.cocodataset.org/val2017/000000375493.jpg\n","tensor([[22.2689, 24.3331, 23.6913, 26.2843]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 375493: a photo of a car\n","Generated Caption: a woman in a dress standing next to a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 93/349 [02:32<07:13,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog \n","\n","CLIP Similarity Score for image 375493: 0.21758078038692474\n","http://images.cocodataset.org/val2017/000000432553.jpg\n","tensor([[22.7311, 24.7091, 24.1827, 26.7321]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 432553: a photo of a car\n","Generated Caption: a dog is standing on a leash \n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 94/349 [02:34<07:15,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 432553: 0.20744214951992035\n","http://images.cocodataset.org/val2017/000000546829.jpg\n","tensor([[21.8427, 23.9099, 23.4690, 26.1444]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 546829: a photo of a car\n","Generated Caption: a dog sitting on a bench in the woods \n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 95/349 [02:35<06:59,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a bench \n","\n","CLIP Similarity Score for image 546829: 0.24065887928009033\n","http://images.cocodataset.org/val2017/000000261161.jpg\n","tensor([[22.1787, 24.2315, 23.7651, 26.4310]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 261161: a photo of a car\n","Generated Caption: a dog sitting on a bench next to a tree \n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 96/349 [02:37<07:00,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the ground \n","\n","CLIP Similarity Score for image 261161: 0.2302202433347702\n","http://images.cocodataset.org/val2017/000000475732.jpg\n","tensor([[22.7777, 24.6509, 24.0584, 26.4540]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 475732: a photo of a car\n","Generated Caption: a cat wearing a hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 97/349 [02:39<06:37,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on top \n","\n","CLIP Similarity Score for image 475732: 0.21115410327911377\n","http://images.cocodataset.org/val2017/000000372819.jpg\n","tensor([[22.1484, 24.6990, 24.4149, 26.5483]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 372819: a photo of a car\n","Generated Caption: a dog running with a group of people \n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 98/349 [02:40<06:34,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and dog playing with a dog \n","\n","CLIP Similarity Score for image 372819: 0.24593167006969452\n","http://images.cocodataset.org/val2017/000000053529.jpg\n","tensor([[22.0015, 24.2378, 23.8144, 26.3226]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 53529: a photo of a car\n","Generated Caption: a dog is looking at a camera while standing in the street \n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 99/349 [02:42<06:45,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the hood \n","\n","CLIP Similarity Score for image 53529: 0.26636624336242676\n","http://images.cocodataset.org/val2017/000000108244.jpg\n","tensor([[22.2873, 24.3328, 23.5510, 26.6212]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 108244: a photo of a car\n","Generated Caption: a cat laying in a box on top of a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▊       | 100/349 [02:44<06:50,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat inside of it \n","\n","CLIP Similarity Score for image 108244: 0.21141405403614044\n","http://images.cocodataset.org/val2017/000000076417.jpg\n","tensor([[22.6185, 24.6506, 24.0147, 26.7376]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 76417: a photo of a car\n","Generated Caption: a dog sitting in the drivers seat of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 101/349 [02:45<06:58,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the dashboard \n","\n","CLIP Similarity Score for image 76417: 0.3109862506389618\n","http://images.cocodataset.org/val2017/000000320554.jpg\n","tensor([[21.6719, 23.7721, 23.2420, 26.1215]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 320554: a photo of a car\n","Generated Caption: a cat sitting on a bench in the sun \n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 102/349 [02:47<07:02,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting in the shade \n","\n","CLIP Similarity Score for image 320554: 0.22455811500549316\n","http://images.cocodataset.org/val2017/000000277584.jpg\n","tensor([[21.8958, 23.8676, 23.4870, 26.2933]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 277584: a photo of a car\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|██▉       | 103/349 [02:49<07:01,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a cat sitting on the window sill \n","\n","CLIP Similarity Score for image 277584: 0.30469343066215515\n","http://images.cocodataset.org/val2017/000000089271.jpg\n","tensor([[22.3928, 24.5584, 24.0268, 26.4676]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 89271: a photo of a car\n","Generated Caption: a cat wearing a hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|██▉       | 104/349 [02:50<06:42,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 89271: 0.2182348519563675\n","http://images.cocodataset.org/val2017/000000286708.jpg\n","tensor([[22.9214, 24.8409, 24.3388, 26.6564]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 286708: a photo of a car\n","Generated Caption: a cat wearing a hat and a bow tie \n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 105/349 [02:52<06:34,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 286708: 0.20537203550338745\n","http://images.cocodataset.org/val2017/000000279145.jpg\n","tensor([[21.8805, 23.5438, 23.3201, 25.9191]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 279145: a photo of a car\n","Generated Caption: a bench in a garden with plants growing out of it \n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 106/349 [02:53<06:35,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting in a garden \n","\n","CLIP Similarity Score for image 279145: 0.26519447565078735\n","http://images.cocodataset.org/val2017/000000236166.jpg\n","tensor([[22.1956, 24.2505, 23.8857, 26.3995]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 236166: a photo of a car\n","Generated Caption: a man wearing a hat and a hat hat \n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 107/349 [02:55<06:26,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 236166: 0.1730557233095169\n","http://images.cocodataset.org/val2017/000000288685.jpg\n","tensor([[22.3442, 24.4542, 23.9667, 26.2071]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 288685: a photo of a car\n","Generated Caption: a dog is running in a field with a group of people \n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 108/349 [02:57<06:47,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog and a group of people \n","\n","CLIP Similarity Score for image 288685: 0.15690571069717407\n","http://images.cocodataset.org/val2017/000000311190.jpg\n","tensor([[22.5045, 24.5764, 23.9487, 26.6056]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 311190: a photo of a car\n","Generated Caption: a dog wearing a hat and a collar \n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 109/349 [02:59<07:05,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the hood \n","\n","CLIP Similarity Score for image 311190: 0.2698235511779785\n","http://images.cocodataset.org/val2017/000000413395.jpg\n","tensor([[23.0208, 24.8608, 24.2903, 26.8424]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 413395: a photo of a car\n","Generated Caption: a man holding a cat on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 110/349 [03:00<06:45,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat and a man \n","\n","CLIP Similarity Score for image 413395: 0.2779559791088104\n","http://images.cocodataset.org/val2017/000000446522.jpg\n","tensor([[22.0761, 23.9453, 23.1976, 26.1053]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 446522: a photo of a car\n","Generated Caption: a dog laying on a chair in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 111/349 [03:02<06:37,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on the back \n","\n","CLIP Similarity Score for image 446522: 0.23006553947925568\n","http://images.cocodataset.org/val2017/000000089880.jpg\n","tensor([[22.1427, 24.4548, 24.0060, 26.3390]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 89880: a photo of a car\n","Generated Caption: a dog with a red collar and a black and white dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 112/349 [03:04<06:29,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and dog with a dog collar \n","\n","CLIP Similarity Score for image 89880: 0.22384324669837952\n","http://images.cocodataset.org/val2017/000000171611.jpg\n","tensor([[22.4131, 24.4668, 24.1500, 26.4353]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 171611: a photo of a car\n","Generated Caption: a boat is docked at a pier with people on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 113/349 [03:05<06:28,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and boat on a lake \n","\n","CLIP Similarity Score for image 171611: 0.23085343837738037\n","http://images.cocodataset.org/val2017/000000312213.jpg\n","tensor([[22.7860, 24.7187, 24.0617, 26.9335]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 312213: a photo of a car\n","Generated Caption: a cat sitting on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 114/349 [03:07<06:15,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 312213: 0.2819163501262665\n","http://images.cocodataset.org/val2017/000000512836.jpg\n","tensor([[21.9582, 24.3583, 23.7796, 26.0950]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 512836: a photo of a car\n","Generated Caption: a woman holding an umbrella while walking in the snow \n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 115/349 [03:08<06:17,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog and a snowman \n","\n","CLIP Similarity Score for image 512836: 0.2550865709781647\n","http://images.cocodataset.org/val2017/000000222317.jpg\n","tensor([[22.3017, 24.3320, 23.7889, 26.5270]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 222317: a photo of a car\n","Generated Caption: a dog laying on a couch with a stuffed animal \n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 116/349 [03:10<06:32,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and dog on a couch \n","\n","CLIP Similarity Score for image 222317: 0.28440093994140625\n","http://images.cocodataset.org/val2017/000000181969.jpg\n","tensor([[22.7438, 24.8993, 24.3895, 26.4053]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 181969: a photo of a car\n","Generated Caption: a dog laying on a couch with a pillow \n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▎      | 117/349 [03:12<06:49,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on it \n","\n","CLIP Similarity Score for image 181969: 0.214345321059227\n","http://images.cocodataset.org/val2017/000000574315.jpg\n","tensor([[22.3673, 24.5766, 23.8261, 26.6443]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 574315: a photo of a car\n","Generated Caption: a cat sitting on top of a computer keyboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 118/349 [03:14<06:33,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 574315: 0.30336612462997437\n","http://images.cocodataset.org/val2017/000000177015.jpg\n","tensor([[22.4574, 24.6147, 23.9430, 26.5666]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 177015: a photo of a car\n","Generated Caption: a man is sitting on a couch with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 119/349 [03:15<06:23,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a man on a laptop \n","\n","CLIP Similarity Score for image 177015: 0.28675970435142517\n","http://images.cocodataset.org/val2017/000000210099.jpg\n","tensor([[22.1234, 24.1799, 23.8173, 26.3399]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 210099: a photo of a car\n","Generated Caption: a cat sitting on a chair in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 120/349 [03:17<06:21,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting on the back of it \n","\n","CLIP Similarity Score for image 210099: 0.23536911606788635\n","http://images.cocodataset.org/val2017/000000574810.jpg\n","tensor([[22.3449, 24.1611, 23.8527, 26.1460]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 574810: a photo of a car\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▍      | 121/349 [03:19<06:16,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting on the window sill \n","\n","CLIP Similarity Score for image 574810: 0.2718275189399719\n","http://images.cocodataset.org/val2017/000000197528.jpg\n","tensor([[22.3674, 24.2739, 23.8139, 26.2461]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 197528: a photo of a car\n","Generated Caption: a cat looking out a window \n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▍      | 122/349 [03:20<06:06,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a cat looking out \n","\n","CLIP Similarity Score for image 197528: 0.27892810106277466\n","http://images.cocodataset.org/val2017/000000116825.jpg\n","tensor([[22.4872, 24.6936, 24.0169, 26.7347]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 116825: a photo of a car\n","Generated Caption: a cat laying on a table next to a remote \n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▌      | 123/349 [03:22<05:54,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 116825: 0.193869486451149\n","http://images.cocodataset.org/val2017/000000246454.jpg\n","tensor([[22.5673, 24.6258, 23.9685, 26.6258]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 246454: a photo of a car\n","Generated Caption: a woman petting a dog on the street \n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 124/349 [03:24<06:26,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a woman petting a dog \n","\n","CLIP Similarity Score for image 246454: 0.2592431306838989\n","http://images.cocodataset.org/val2017/000000517832.jpg\n","tensor([[22.3517, 24.2733, 23.5549, 26.5957]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 517832: a photo of a car\n","Generated Caption: a dog sitting on a chair with a pillow \n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 125/349 [03:25<06:18,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a chair \n","\n","CLIP Similarity Score for image 517832: 0.23457218706607819\n","http://images.cocodataset.org/val2017/000000401991.jpg\n","tensor([[22.0474, 24.0163, 23.3766, 26.1235]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 401991: a photo of a car\n","Generated Caption: a dog laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 126/349 [03:27<06:07,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a dog laying on a bed \n","\n","CLIP Similarity Score for image 401991: 0.2522938847541809\n","http://images.cocodataset.org/val2017/000000126110.jpg\n","tensor([[22.4522, 24.6742, 24.1247, 26.3722]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 126110: a photo of a car\n","Generated Caption: a dog is looking out of a truck window \n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▋      | 127/349 [03:28<05:48,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in the window \n","\n","CLIP Similarity Score for image 126110: 0.2681330144405365\n","http://images.cocodataset.org/val2017/000000079229.jpg\n","tensor([[22.0456, 24.5773, 24.0376, 26.4191]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 79229: a photo of a car\n","Generated Caption: a man riding a horse in a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 128/349 [03:30<05:45,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a dog on a field \n","\n","CLIP Similarity Score for image 79229: 0.26766762137413025\n","http://images.cocodataset.org/val2017/000000520531.jpg\n","tensor([[22.4562, 24.2440, 23.6734, 26.4969]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 520531: a photo of a car\n","Generated Caption: a cat sitting on a desk next to a computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 129/349 [03:31<05:38,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a desk \n","\n","CLIP Similarity Score for image 520531: 0.24542376399040222\n","http://images.cocodataset.org/val2017/000000179392.jpg\n","tensor([[22.1858, 24.4086, 23.7574, 26.3845]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 179392: a photo of a car\n","Generated Caption: a black cat sitting on a desk with a lamp \n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 130/349 [03:33<05:35,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the hood \n","\n","CLIP Similarity Score for image 179392: 0.1957089900970459\n","http://images.cocodataset.org/val2017/000000145781.jpg\n","tensor([[22.8687, 24.9005, 24.3876, 26.7137]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 145781: a photo of a car\n","Generated Caption: a dog laying on the ground with a bottle of water \n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 131/349 [03:34<05:42,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a bottle of water \n","\n","CLIP Similarity Score for image 145781: 0.1994064301252365\n","http://images.cocodataset.org/val2017/000000551815.jpg\n","tensor([[22.9908, 24.9726, 24.5265, 26.6573]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 551815: a photo of a car\n","Generated Caption: a cat laying on top of a pile of stuffed animals \n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 132/349 [03:36<06:08,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and cat laying on a blanket \n","\n","CLIP Similarity Score for image 551815: 0.2677982449531555\n","http://images.cocodataset.org/val2017/000000316015.jpg\n","tensor([[22.5181, 24.5489, 23.8932, 26.6706]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 316015: a photo of a car\n","Generated Caption: a cat sitting on a desk next to a computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 133/349 [03:38<05:59,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a desk \n","\n","CLIP Similarity Score for image 316015: 0.2489849328994751\n","http://images.cocodataset.org/val2017/000000525247.jpg\n","tensor([[22.7930, 24.7295, 24.1304, 26.7159]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 525247: a photo of a car\n","Generated Caption: a cat sitting on top of a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 134/349 [03:40<05:53,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 525247: 0.28260090947151184\n","http://images.cocodataset.org/val2017/000000030494.jpg\n","tensor([[22.0250, 24.2816, 23.8485, 26.3595]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 30494: a photo of a car\n","Generated Caption: a cat is running through the woods \n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▊      | 135/349 [03:41<05:46,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the ground \n","\n","CLIP Similarity Score for image 30494: 0.17210158705711365\n","http://images.cocodataset.org/val2017/000000025560.jpg\n","tensor([[22.3445, 24.3751, 23.6698, 26.3522]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 25560: a photo of a car\n","Generated Caption: a cat sitting on top of a television \n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 136/349 [03:43<05:39,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the top of it \n","\n","CLIP Similarity Score for image 25560: 0.19923411309719086\n","http://images.cocodataset.org/val2017/000000329319.jpg\n","tensor([[22.5042, 24.5069, 23.9739, 26.6202]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 329319: a photo of a car\n","Generated Caption: a cat sitting on a wooden bench \n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 137/349 [03:44<05:28,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 329319: 0.20306162536144257\n","http://images.cocodataset.org/val2017/000000416256.jpg\n","tensor([[22.2013, 24.2437, 23.7915, 26.3252]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 416256: a photo of a car\n","Generated Caption: a cat laying on top of a computer keyboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|███▉      | 138/349 [03:46<05:27,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 416256: 0.2772402763366699\n","http://images.cocodataset.org/val2017/000000292330.jpg\n","tensor([[22.1563, 24.5705, 23.8625, 26.0588]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 292330: a photo of a car\n","Generated Caption: a dog running with a baseball bat on a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|███▉      | 139/349 [03:47<05:34,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog running \n","\n","CLIP Similarity Score for image 292330: 0.20368486642837524\n","http://images.cocodataset.org/val2017/000000235399.jpg\n","tensor([[21.7597, 23.7532, 23.2303, 26.1091]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 235399: a photo of a car\n","Generated Caption: a dog laying on the bed of a truck \n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 140/349 [03:49<05:53,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting in a garage \n","\n","CLIP Similarity Score for image 235399: 0.2144792377948761\n","http://images.cocodataset.org/val2017/000000565962.jpg\n","tensor([[22.1800, 24.4802, 23.9669, 26.2264]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 565962: a photo of a car\n","Generated Caption: a black bear is sitting in the middle of a forest \n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 141/349 [03:51<05:39,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a bear in the back \n","\n","CLIP Similarity Score for image 565962: 0.25157901644706726\n","http://images.cocodataset.org/val2017/000000364297.jpg\n","tensor([[22.2081, 24.2172, 23.5882, 26.4553]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 364297: a photo of a car\n","Generated Caption: a cat is laying on a computer keyboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 142/349 [03:52<05:25,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 364297: 0.29497024416923523\n","http://images.cocodataset.org/val2017/000000077595.jpg\n","tensor([[22.4501, 24.5200, 23.9574, 26.4352]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 77595: a photo of a car\n","Generated Caption: a cat laying on a bed with a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 143/349 [03:54<05:23,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 77595: 0.22528474032878876\n","http://images.cocodataset.org/val2017/000000219578.jpg\n","tensor([[22.5602, 24.6841, 24.0760, 26.6052]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 219578: a photo of a car\n","Generated Caption: a cat laying on a couch next to a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████▏     | 144/349 [03:55<05:23,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a cat \n","\n","CLIP Similarity Score for image 219578: 0.24556173384189606\n","http://images.cocodataset.org/val2017/000000407083.jpg\n","tensor([[22.0529, 24.1884, 23.6729, 26.2817]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 407083: a photo of a car\n","Generated Caption: a dog is sitting in the driver's seat of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 145/349 [03:57<05:32,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in the driver seat \n","\n","CLIP Similarity Score for image 407083: 0.29826676845550537\n","http://images.cocodataset.org/val2017/000000411665.jpg\n","tensor([[22.9503, 25.0003, 24.5451, 26.8656]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 411665: a photo of a car\n","Generated Caption: a cat is looking at itself in a mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 146/349 [03:59<05:19,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car mirror with a cat \n","\n","CLIP Similarity Score for image 411665: 0.32218676805496216\n","http://images.cocodataset.org/val2017/000000078565.jpg\n","tensor([[21.9802, 24.3067, 23.5604, 26.0738]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 78565: a photo of a car\n","Generated Caption: people are on the beach with sailboats \n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 147/349 [04:01<05:37,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and sailboats in the water \n","\n","CLIP Similarity Score for image 78565: 0.314492791891098\n","http://images.cocodataset.org/val2017/000000261706.jpg\n","tensor([[22.4683, 24.7507, 24.1203, 26.4549]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 261706: a photo of a car\n","Generated Caption: a cat is sitting on a couch with a remote \n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 148/349 [04:02<05:47,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the floor \n","\n","CLIP Similarity Score for image 261706: 0.21944627165794373\n","http://images.cocodataset.org/val2017/000000297830.jpg\n","tensor([[22.4771, 24.3231, 23.9573, 26.2886]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 297830: a photo of a car\n","Generated Caption: a dog with a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 149/349 [04:04<05:43,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the back \n","\n","CLIP Similarity Score for image 297830: 0.1926630437374115\n","http://images.cocodataset.org/val2017/000000425390.jpg\n","tensor([[22.5325, 24.5421, 23.9044, 26.4063]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 425390: a photo of a car\n","Generated Caption: a cat laying on top of a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 150/349 [04:06<05:32,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 425390: 0.2866775393486023\n","http://images.cocodataset.org/val2017/000000309484.jpg\n","tensor([[22.7209, 24.7034, 24.0679, 26.6359]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 309484: a photo of a car\n","Generated Caption: a dog chewing on a person's hand \n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 151/349 [04:07<05:26,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog chewing on a toothbrush \n","\n","CLIP Similarity Score for image 309484: 0.30231988430023193\n","http://images.cocodataset.org/val2017/000000433134.jpg\n","tensor([[22.8694, 24.8890, 24.3960, 26.6817]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 433134: a photo of a car\n","Generated Caption: a cat is standing on the grass with its eyes closed \n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▎     | 152/349 [04:09<05:23,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the grass \n","\n","CLIP Similarity Score for image 433134: 0.2678245007991791\n","http://images.cocodataset.org/val2017/000000241326.jpg\n","tensor([[21.7599, 23.8324, 23.1191, 26.0840]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 241326: a photo of a car\n","Generated Caption: a black cat laying on a couch next to a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 153/349 [04:11<05:25,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat laying on the couch \n","\n","CLIP Similarity Score for image 241326: 0.2884056866168976\n","http://images.cocodataset.org/val2017/000000107226.jpg\n","tensor([[22.0500, 24.2601, 23.6359, 26.3040]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 107226: a photo of a car\n","Generated Caption: a dog is standing in the grass with a group of people \n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 154/349 [04:12<05:19,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 107226: 0.14466746151447296\n","http://images.cocodataset.org/val2017/000000094336.jpg\n","tensor([[22.6683, 24.5836, 24.0037, 26.7319]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 94336: a photo of a car\n","Generated Caption: a cat sitting in a sink \n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 155/349 [04:14<05:18,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting in the sink \n","\n","CLIP Similarity Score for image 94336: 0.3159799575805664\n","http://images.cocodataset.org/val2017/000000555705.jpg\n","tensor([[22.5411, 24.5131, 23.8736, 26.4059]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 555705: a photo of a car\n","Generated Caption: a cat laying on a wooden floor next to a shoe \n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▍     | 156/349 [04:16<05:30,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car cat and a cat laying on a shoe \n","\n","CLIP Similarity Score for image 555705: 0.3530362546443939\n","http://images.cocodataset.org/val2017/000000161609.jpg\n","tensor([[22.3241, 24.3712, 23.8141, 26.4974]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 161609: a photo of a car\n","Generated Caption: a dog wearing a hat is walking down the stairs \n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▍     | 157/349 [04:17<05:24,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 161609: 0.21231772005558014\n","http://images.cocodataset.org/val2017/000000462728.jpg\n","tensor([[22.5424, 24.8029, 24.2859, 26.4749]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 462728: a photo of a car\n","Generated Caption: a person riding a surfboard on top of a wave \n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▌     | 158/349 [04:19<05:24,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car riding a wave on a surfboard \n","\n","CLIP Similarity Score for image 462728: 0.28059396147727966\n","http://images.cocodataset.org/val2017/000000524280.jpg\n","tensor([[22.1937, 24.2174, 23.8321, 26.1180]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 524280: a photo of a car\n","Generated Caption: a cat with a green eyes staring at the camera \n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 159/349 [04:21<05:14,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 524280: 0.23376218974590302\n","http://images.cocodataset.org/val2017/000000251572.jpg\n","tensor([[22.6179, 24.7550, 24.2133, 26.6754]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 251572: a photo of a car\n","Generated Caption: a woman laying on a couch with a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 160/349 [04:22<05:08,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on it \n","\n","CLIP Similarity Score for image 251572: 0.2481003850698471\n","http://images.cocodataset.org/val2017/000000181859.jpg\n","tensor([[22.2608, 24.3824, 23.8263, 26.4998]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 181859: a photo of a car\n","Generated Caption: a cat laying in a sink in a bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 161/349 [04:24<05:04,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting in the sink \n","\n","CLIP Similarity Score for image 181859: 0.30859431624412537\n","http://images.cocodataset.org/val2017/000000001675.jpg\n","tensor([[22.5321, 24.4756, 24.1138, 26.6703]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 1675: a photo of a car\n","Generated Caption: a cat sitting on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▋     | 162/349 [04:25<04:55,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the front \n","\n","CLIP Similarity Score for image 1675: 0.2265966683626175\n","http://images.cocodataset.org/val2017/000000190140.jpg\n","tensor([[22.4246, 24.4966, 24.0218, 26.4577]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 190140: a photo of a car\n","Generated Caption: a dog on a boat with a man on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 47%|████▋     | 163/349 [04:27<05:00,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and dog on a boat \n","\n","CLIP Similarity Score for image 190140: 0.30157509446144104\n","http://images.cocodataset.org/val2017/000000182805.jpg\n","tensor([[22.2725, 24.3707, 23.8596, 26.3645]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 182805: a photo of a car\n","Generated Caption: a woman holding a dog under her arm \n"]},{"output_type":"stream","name":"stderr","text":["\r 47%|████▋     | 164/349 [04:29<05:19,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog and a woman \n","\n","CLIP Similarity Score for image 182805: 0.237585186958313\n","http://images.cocodataset.org/val2017/000000138492.jpg\n","tensor([[22.8401, 24.9606, 24.4344, 26.5567]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 138492: a photo of a car\n","Generated Caption: a black and white dog catching a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 47%|████▋     | 165/349 [04:31<05:22,  1.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a frisbee in its mouth \n","\n","CLIP Similarity Score for image 138492: 0.272440642118454\n","http://images.cocodataset.org/val2017/000000291490.jpg\n","tensor([[22.8510, 24.8378, 24.3921, 26.8138]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 291490: a photo of a car\n","Generated Caption: a cat is sitting on a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 166/349 [04:32<05:02,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 291490: 0.22253096103668213\n","http://images.cocodataset.org/val2017/000000071226.jpg\n","tensor([[21.9698, 24.0157, 23.3858, 26.1673]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 71226: a photo of a car\n","Generated Caption: a dog laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 167/349 [04:34<04:58,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on it \n","\n","CLIP Similarity Score for image 71226: 0.2015424221754074\n","http://images.cocodataset.org/val2017/000000131938.jpg\n","tensor([[22.7244, 24.3921, 24.0433, 26.4621]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 131938: a photo of a car\n","Generated Caption: a cat wearing a tie sitting on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 168/349 [04:35<04:53,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 131938: 0.22163288295269012\n","http://images.cocodataset.org/val2017/000000434996.jpg\n","tensor([[22.2519, 24.4432, 23.6726, 26.4603]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 434996: a photo of a car\n","Generated Caption: a cat laying on a blanket next to stuffed animals \n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 169/349 [04:37<04:45,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sleeping on it \n","\n","CLIP Similarity Score for image 434996: 0.26616138219833374\n","http://images.cocodataset.org/val2017/000000398810.jpg\n","tensor([[22.4109, 24.4714, 23.8071, 26.5024]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 398810: a photo of a car\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 49%|████▊     | 170/349 [04:38<04:40,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a cat \n","\n","CLIP Similarity Score for image 398810: 0.28275489807128906\n","http://images.cocodataset.org/val2017/000000237864.jpg\n","tensor([[21.9170, 23.9936, 23.6584, 26.1177]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 237864: a photo of a car\n","Generated Caption: a small elephant walking across a dirt field \n"]},{"output_type":"stream","name":"stderr","text":["\r 49%|████▉     | 171/349 [04:40<04:57,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and elephant in a field \n","\n","CLIP Similarity Score for image 237864: 0.2844322919845581\n","http://images.cocodataset.org/val2017/000000139872.jpg\n","tensor([[22.0887, 24.3142, 23.9597, 26.1371]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 139872: a photo of a car\n","Generated Caption: a dog is playing with a frisbee in the grass \n"]},{"output_type":"stream","name":"stderr","text":["\r 49%|████▉     | 172/349 [04:42<05:12,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the ground \n","\n","CLIP Similarity Score for image 139872: 0.24376599490642548\n","http://images.cocodataset.org/val2017/000000106389.jpg\n","tensor([[22.1051, 23.9857, 23.4607, 26.2588]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 106389: a photo of a car\n","Generated Caption: a cat sitting on a chair in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|████▉     | 173/349 [04:44<05:01,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the floor \n","\n","CLIP Similarity Score for image 106389: 0.2359003871679306\n","http://images.cocodataset.org/val2017/000000140203.jpg\n","tensor([[22.5965, 24.6548, 23.9311, 26.8136]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 140203: a photo of a car\n","Generated Caption: a white and blue truck parked in a parking lot \n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|████▉     | 174/349 [04:45<04:54,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car parked in a parking lot \n","\n","CLIP Similarity Score for image 140203: 0.25806668400764465\n","http://images.cocodataset.org/val2017/000000081766.jpg\n","tensor([[22.2401, 24.1458, 23.8915, 26.2919]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 81766: a photo of a car\n","Generated Caption: a black dog sitting on a wooden bench \n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 175/349 [04:47<04:47,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a wooden bench \n","\n","CLIP Similarity Score for image 81766: 0.23182076215744019\n","http://images.cocodataset.org/val2017/000000318908.jpg\n","tensor([[22.7267, 24.6561, 24.1913, 26.5853]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 318908: a photo of a car\n","Generated Caption: a small white dog wearing a pink bow tie \n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 176/349 [04:49<04:38,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a white dog on it \n","\n","CLIP Similarity Score for image 318908: 0.2029895782470703\n","http://images.cocodataset.org/val2017/000000131131.jpg\n","tensor([[22.7993, 24.6036, 24.0372, 26.6729]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 131131: a photo of a car\n","Generated Caption: a cat is sitting on a computer screen \n"]},{"output_type":"stream","name":"stderr","text":["\r 51%|█████     | 177/349 [04:50<04:32,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the screen \n","\n","CLIP Similarity Score for image 131131: 0.25419357419013977\n","http://images.cocodataset.org/val2017/000000004795.jpg\n","tensor([[22.8720, 24.7596, 24.3437, 26.6068]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 4795: a photo of a car\n","Generated Caption: a cat sitting on a computer desk \n"]},{"output_type":"stream","name":"stderr","text":["\r 51%|█████     | 178/349 [04:52<04:28,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 4795: 0.2688145339488983\n","http://images.cocodataset.org/val2017/000000115885.jpg\n","tensor([[22.5134, 24.5707, 23.8534, 26.7913]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 115885: a photo of a car\n","Generated Caption: a cat is laying on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 51%|█████▏    | 179/349 [04:54<04:54,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the lap of a person \n","\n","CLIP Similarity Score for image 115885: 0.2351934015750885\n","http://images.cocodataset.org/val2017/000000490171.jpg\n","tensor([[22.6715, 24.8235, 24.3612, 26.3422]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 490171: a photo of a car\n","Generated Caption: a dog on a surfboard in the water \n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 180/349 [04:55<04:51,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on a beach with a dog \n","\n","CLIP Similarity Score for image 490171: 0.25099101662635803\n","http://images.cocodataset.org/val2017/000000219485.jpg\n","tensor([[21.8451, 23.7548, 23.3944, 26.0199]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 219485: a photo of a car\n","Generated Caption: a window with a sign on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 181/349 [04:57<04:35,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a building \n","\n","CLIP Similarity Score for image 219485: 0.2827305197715759\n","http://images.cocodataset.org/val2017/000000078426.jpg\n","tensor([[22.3639, 24.4227, 24.0122, 26.4741]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 78426: a photo of a car\n","Generated Caption: a cat sitting on a desk next to a book \n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 182/349 [04:58<04:27,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 78426: 0.20520570874214172\n","http://images.cocodataset.org/val2017/000000049269.jpg\n","tensor([[22.2718, 24.3404, 23.9311, 26.2854]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 49269: a photo of a car\n","Generated Caption: a brown and white dog and a black and white dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 183/349 [05:00<04:22,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a dog \n","\n","CLIP Similarity Score for image 49269: 0.2532452642917633\n","http://images.cocodataset.org/val2017/000000515025.jpg\n","tensor([[22.4879, 24.3904, 23.7510, 26.4821]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 515025: a photo of a car\n","Generated Caption: a man is standing on a table with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 53%|█████▎    | 184/349 [05:02<04:23,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 515025: 0.21056786179542542\n","http://images.cocodataset.org/val2017/000000403817.jpg\n","tensor([[22.6261, 24.7766, 23.9380, 26.4960]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 403817: a photo of a car\n","Generated Caption: a cat sitting on a computer desk \n"]},{"output_type":"stream","name":"stderr","text":["\r 53%|█████▎    | 185/349 [05:03<04:18,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the screen \n","\n","CLIP Similarity Score for image 403817: 0.2509930431842804\n","http://images.cocodataset.org/val2017/000000424545.jpg\n","tensor([[22.5955, 24.4403, 23.9324, 26.6503]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 424545: a photo of a car\n","Generated Caption: a cat is sitting on a counter looking at something \n"]},{"output_type":"stream","name":"stderr","text":["\r 53%|█████▎    | 186/349 [05:05<04:19,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the floor \n","\n","CLIP Similarity Score for image 424545: 0.24933892488479614\n","http://images.cocodataset.org/val2017/000000545826.jpg\n","tensor([[22.2229, 24.3364, 23.6876, 26.2634]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 545826: a photo of a car\n","Generated Caption: a cat is playing with a toy on a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▎    | 187/349 [05:06<04:25,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on top of it \n","\n","CLIP Similarity Score for image 545826: 0.17767730355262756\n","http://images.cocodataset.org/val2017/000000187236.jpg\n","tensor([[22.6261, 24.6683, 24.0244, 26.5392]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 187236: a photo of a car\n","Generated Caption: a cat is laying on the floor looking at something \n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 188/349 [05:08<04:31,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the floor \n","\n","CLIP Similarity Score for image 187236: 0.21021240949630737\n","http://images.cocodataset.org/val2017/000000343076.jpg\n","tensor([[22.7493, 24.5929, 24.1642, 26.6564]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 343076: a photo of a car\n","Generated Caption: a cat sitting on top of a wooden table \n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 189/349 [05:10<04:41,  1.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 343076: 0.21496132016181946\n","http://images.cocodataset.org/val2017/000000049810.jpg\n","tensor([[22.9933, 24.9624, 24.3378, 26.6652]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 49810: a photo of a car\n","Generated Caption: a cat sitting on a wooden bench \n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 190/349 [05:12<04:33,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a wooden bench \n","\n","CLIP Similarity Score for image 49810: 0.20288363099098206\n","http://images.cocodataset.org/val2017/000000014831.jpg\n","tensor([[22.9838, 24.5695, 24.1195, 26.5559]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 14831: a photo of a car\n","Generated Caption: a cat is sleeping on a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▍    | 191/349 [05:13<04:21,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the back \n","\n","CLIP Similarity Score for image 14831: 0.2286299169063568\n","http://images.cocodataset.org/val2017/000000186296.jpg\n","tensor([[22.4625, 24.3731, 24.1195, 26.4219]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 186296: a photo of a car\n","Generated Caption: a cat laying on the ground next to a pair of shoes \n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▌    | 192/349 [05:15<04:22,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the ground \n","\n","CLIP Similarity Score for image 186296: 0.24647736549377441\n","http://images.cocodataset.org/val2017/000000416330.jpg\n","tensor([[22.7906, 24.7406, 24.2641, 26.6173]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 416330: a photo of a car\n","Generated Caption: a cat is laying on a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▌    | 193/349 [05:17<04:11,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 416330: 0.20020847022533417\n","http://images.cocodataset.org/val2017/000000357941.jpg\n","tensor([[22.7457, 24.7301, 24.1640, 26.6374]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 357941: a photo of a car\n","Generated Caption: a tv with a cat on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 194/349 [05:18<03:58,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a tv on \n","\n","CLIP Similarity Score for image 357941: 0.28455060720443726\n","http://images.cocodataset.org/val2017/000000063552.jpg\n","tensor([[22.5431, 24.6896, 23.9927, 26.7368]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 63552: a photo of a car\n","Generated Caption: a cat is looking at itself in a mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 195/349 [05:20<04:16,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car mirror with a cat \n","\n","CLIP Similarity Score for image 63552: 0.328111857175827\n","http://images.cocodataset.org/val2017/000000015497.jpg\n","tensor([[22.8084, 24.7397, 24.2053, 26.4927]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 15497: a photo of a car\n","Generated Caption: a cat laying on a desk next to a mouse \n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 196/349 [05:21<04:13,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 15497: 0.26720020174980164\n","http://images.cocodataset.org/val2017/000000399560.jpg\n","tensor([[22.1376, 24.3791, 23.5853, 26.6647]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 399560: a photo of a car\n","Generated Caption: a cat is sitting in a bowl on the floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▋    | 197/349 [05:23<04:07,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat in the middle \n","\n","CLIP Similarity Score for image 399560: 0.21353580057621002\n","http://images.cocodataset.org/val2017/000000098839.jpg\n","tensor([[22.5494, 24.4981, 23.8997, 26.5714]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 98839: a photo of a car\n","Generated Caption: a cat sitting on a television screen \n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 198/349 [05:25<04:01,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the floor \n","\n","CLIP Similarity Score for image 98839: 0.21305795013904572\n","http://images.cocodataset.org/val2017/000000278463.jpg\n","tensor([[22.8295, 24.5660, 23.9772, 26.7324]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 278463: a photo of a car\n","Generated Caption: a laptop computer sitting on top of a wooden table \n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 199/349 [05:26<04:01,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a laptop on a table \n","\n","CLIP Similarity Score for image 278463: 0.30318406224250793\n","http://images.cocodataset.org/val2017/000000575357.jpg\n","tensor([[22.6583, 24.7780, 24.2186, 26.4179]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 575357: a photo of a car\n","Generated Caption: a dog running through a field with a red tag \n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 200/349 [05:28<04:00,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in the back \n","\n","CLIP Similarity Score for image 575357: 0.2244676649570465\n","http://images.cocodataset.org/val2017/000000366199.jpg\n","tensor([[22.0777, 24.5095, 23.8313, 26.4044]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 366199: a photo of a car\n","Generated Caption: a cat sleeping on a blanket on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 201/349 [05:29<04:00,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sleeping on the bed \n","\n","CLIP Similarity Score for image 366199: 0.23776759207248688\n","http://images.cocodataset.org/val2017/000000286422.jpg\n","tensor([[22.3698, 24.1970, 23.7358, 26.2901]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 286422: a photo of a car\n","Generated Caption: a white boat with a black and white sail on a lake \n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 202/349 [05:31<04:06,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a sailboat on a lake \n","\n","CLIP Similarity Score for image 286422: 0.26071658730506897\n","http://images.cocodataset.org/val2017/000000112798.jpg\n","tensor([[22.5254, 24.3746, 23.8405, 26.4613]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 112798: a photo of a car\n","Generated Caption: a cat laying on a table with a lot of clutter \n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 203/349 [05:33<04:26,  1.83s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat laying on top of it \n","\n","CLIP Similarity Score for image 112798: 0.22316771745681763\n","http://images.cocodataset.org/val2017/000000407960.jpg\n","tensor([[21.9927, 24.2461, 23.7188, 26.4948]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 407960: a photo of a car\n","Generated Caption: a cat sitting on the floor looking out the window \n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 204/349 [05:35<04:11,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on the floor \n","\n","CLIP Similarity Score for image 407960: 0.18390582501888275\n","http://images.cocodataset.org/val2017/000000572388.jpg\n","tensor([[22.9556, 24.9695, 24.3225, 26.4282]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 572388: a photo of a car\n","Generated Caption: a stuffed bear with a stuffed bear in it's mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 59%|█████▊    | 205/349 [05:37<04:01,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car stuffed animal and a bear \n","\n","CLIP Similarity Score for image 572388: 0.23602887988090515\n","http://images.cocodataset.org/val2017/000000051008.jpg\n","tensor([[22.7089, 24.7762, 24.1445, 26.7323]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 51008: a photo of a car\n","Generated Caption: a cat is sitting on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 59%|█████▉    | 206/349 [05:38<03:54,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 51008: 0.2767648994922638\n","http://images.cocodataset.org/val2017/000000387383.jpg\n","tensor([[22.8837, 24.8398, 24.4065, 26.6515]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 387383: a photo of a car\n","Generated Caption: a cat laying on a bed with a cat on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 59%|█████▉    | 207/349 [05:40<03:55,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car bed with a cat laying on top of it \n","\n","CLIP Similarity Score for image 387383: 0.2723241150379181\n","http://images.cocodataset.org/val2017/000000311789.jpg\n","tensor([[22.9127, 24.8838, 24.3601, 26.5934]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 311789: a photo of a car\n","Generated Caption: a cat laying on a wooden floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|█████▉    | 208/349 [05:41<03:46,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat laying on it \n","\n","CLIP Similarity Score for image 311789: 0.20917972922325134\n","http://images.cocodataset.org/val2017/000000080153.jpg\n","tensor([[22.7944, 24.9245, 24.7021, 26.5907]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 80153: a photo of a car\n","Generated Caption: a man riding skis on top of a snow covered slope \n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|█████▉    | 209/349 [05:43<03:47,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a person on skis \n","\n","CLIP Similarity Score for image 80153: 0.28463754057884216\n","http://images.cocodataset.org/val2017/000000189806.jpg\n","tensor([[22.3302, 24.3568, 23.8501, 26.5498]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 189806: a photo of a car\n","Generated Caption: a cat is sitting on the floor next to a cat laying on the floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 210/349 [05:45<04:07,  1.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and cat on the floor \n","\n","CLIP Similarity Score for image 189806: 0.25864505767822266\n","http://images.cocodataset.org/val2017/000000392818.jpg\n","tensor([[22.7019, 24.5517, 24.3374, 26.8086]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 392818: a photo of a car\n","Generated Caption: a dog wearing a collar and tie \n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 211/349 [05:47<04:00,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 392818: 0.21419338881969452\n","http://images.cocodataset.org/val2017/000000084650.jpg\n","tensor([[22.5326, 24.4546, 23.9145, 26.5656]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 84650: a photo of a car\n","Generated Caption: a cat laying on a suitcase on the floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 61%|██████    | 212/349 [05:48<03:53,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat laying on top of it \n","\n","CLIP Similarity Score for image 84650: 0.20733432471752167\n","http://images.cocodataset.org/val2017/000000509403.jpg\n","tensor([[22.4386, 24.5070, 24.2340, 26.3350]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 509403: a photo of a car\n","Generated Caption: a man and a child playing with a frisbee in a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 61%|██████    | 213/349 [05:50<03:57,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a man playing frisbee \n","\n","CLIP Similarity Score for image 509403: 0.2679555118083954\n","http://images.cocodataset.org/val2017/000000193162.jpg\n","tensor([[22.3393, 24.6712, 24.2746, 26.3139]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 193162: a photo of a car\n","Generated Caption: a man is standing in a field with a cow \n"]},{"output_type":"stream","name":"stderr","text":["\r 61%|██████▏   | 214/349 [05:52<03:47,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cow \n","\n","CLIP Similarity Score for image 193162: 0.2729133367538452\n","http://images.cocodataset.org/val2017/000000432468.jpg\n","tensor([[23.0680, 24.9433, 24.3058, 26.8096]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 432468: a photo of a car\n","Generated Caption: a cat is sitting on a suitcase \n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 215/349 [05:53<03:35,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on top \n","\n","CLIP Similarity Score for image 432468: 0.22980941832065582\n","http://images.cocodataset.org/val2017/000000185250.jpg\n","tensor([[22.2825, 24.4785, 23.9478, 26.4058]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 185250: a photo of a car\n","Generated Caption: a man in a baseball uniform is holding a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 216/349 [05:55<03:40,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a man in a dog costume \n","\n","CLIP Similarity Score for image 185250: 0.21913200616836548\n","http://images.cocodataset.org/val2017/000000119828.jpg\n","tensor([[22.1737, 24.5058, 23.7359, 26.4901]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 119828: a photo of a car\n","Generated Caption: a black cat sitting on a desk next to a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 217/349 [05:57<03:41,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the desk \n","\n","CLIP Similarity Score for image 119828: 0.3051949441432953\n","http://images.cocodataset.org/val2017/000000416170.jpg\n","tensor([[21.9903, 23.8449, 23.5574, 25.7663]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 416170: a photo of a car\n","Generated Caption: a window with a metal frame and a fence \n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▏   | 218/349 [05:58<03:41,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a fence \n","\n","CLIP Similarity Score for image 416170: 0.2427751123905182\n","http://images.cocodataset.org/val2017/000000080949.jpg\n","tensor([[22.7386, 24.6182, 24.0986, 26.7634]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 80949: a photo of a car\n","Generated Caption: a cat laying on top of a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 63%|██████▎   | 219/349 [06:00<03:40,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 80949: 0.2782633304595947\n","http://images.cocodataset.org/val2017/000000155291.jpg\n","tensor([[22.7919, 24.8576, 24.2663, 26.6830]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 155291: a photo of a car\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 63%|██████▎   | 220/349 [06:02<03:29,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a cat \n","\n","CLIP Similarity Score for image 155291: 0.26262640953063965\n","http://images.cocodataset.org/val2017/000000174231.jpg\n","tensor([[22.3616, 24.4936, 24.0328, 26.7097]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 174231: a photo of a car\n","Generated Caption: a cat laying on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 63%|██████▎   | 221/349 [06:03<03:22,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on top of it \n","\n","CLIP Similarity Score for image 174231: 0.23024983704090118\n","http://images.cocodataset.org/val2017/000000039769.jpg\n","tensor([[23.0737, 24.9090, 24.3063, 26.7014]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 39769: a photo of a car\n","Generated Caption: a cat laying on a blanket with a cat on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▎   | 222/349 [06:05<03:21,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and cat laying on a blanket \n","\n","CLIP Similarity Score for image 39769: 0.23445209860801697\n","http://images.cocodataset.org/val2017/000000494634.jpg\n","tensor([[22.4928, 24.5085, 23.8918, 26.3155]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 494634: a photo of a car\n","Generated Caption: a cat is laying on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 223/349 [06:06<03:17,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 494634: 0.27708637714385986\n","http://images.cocodataset.org/val2017/000000469067.jpg\n","tensor([[22.3766, 24.1975, 23.8933, 26.1505]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 469067: a photo of a car\n","Generated Caption: a woman laying on a bed with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 224/349 [06:08<03:17,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a woman laying on a bed \n","\n","CLIP Similarity Score for image 469067: 0.26971760392189026\n","http://images.cocodataset.org/val2017/000000385205.jpg\n","tensor([[22.8558, 24.6636, 24.3705, 26.5002]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 385205: a photo of a car\n","Generated Caption: a cat laying on a rug next to a stuffed animal \n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 225/349 [06:09<03:17,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat laying on it \n","\n","CLIP Similarity Score for image 385205: 0.22081692516803741\n","http://images.cocodataset.org/val2017/000000198641.jpg\n","tensor([[22.5053, 24.6077, 23.8909, 26.7273]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 198641: a photo of a car\n","Generated Caption: a computer desk with a laptop and a monitor \n"]},{"output_type":"stream","name":"stderr","text":["\r 65%|██████▍   | 226/349 [06:12<03:35,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a computer on a desk \n","\n","CLIP Similarity Score for image 198641: 0.298595130443573\n","http://images.cocodataset.org/val2017/000000549220.jpg\n","tensor([[22.5621, 24.5663, 24.0227, 26.6462]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 549220: a photo of a car\n","Generated Caption: a dog is sitting on a skateboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 65%|██████▌   | 227/349 [06:13<03:26,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 549220: 0.23620003461837769\n","http://images.cocodataset.org/val2017/000000338901.jpg\n","tensor([[22.2044, 24.2707, 23.6597, 26.3244]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 338901: a photo of a car\n","Generated Caption: a dog laying on a couch with a pillow \n"]},{"output_type":"stream","name":"stderr","text":["\r 65%|██████▌   | 228/349 [06:15<03:17,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a couch \n","\n","CLIP Similarity Score for image 338901: 0.24280421435832977\n","http://images.cocodataset.org/val2017/000000297085.jpg\n","tensor([[21.8464, 23.9649, 23.1803, 26.0633]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 297085: a photo of a car\n","Generated Caption: a tv is on in a room with a large screen \n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 229/349 [06:16<03:15,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and tv on a tv stand \n","\n","CLIP Similarity Score for image 297085: 0.2898179590702057\n","http://images.cocodataset.org/val2017/000000532575.jpg\n","tensor([[22.6198, 24.5789, 24.0989, 26.4102]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 532575: a photo of a car\n","Generated Caption: a dog is looking out the window of a boat \n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 230/349 [06:18<03:12,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the back \n","\n","CLIP Similarity Score for image 532575: 0.20733153820037842\n","http://images.cocodataset.org/val2017/000000409867.jpg\n","tensor([[22.2023, 24.2977, 23.6537, 26.1996]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 409867: a photo of a car\n","Generated Caption: a window with a bunch of colorful kites in it \n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▌   | 231/349 [06:19<03:11,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a kitty in it \n","\n","CLIP Similarity Score for image 409867: 0.26850178837776184\n","http://images.cocodataset.org/val2017/000000474164.jpg\n","tensor([[21.9152, 24.1557, 23.6139, 26.3133]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 474164: a photo of a car\n","Generated Caption: a dog sitting in the back of a truck \n"]},{"output_type":"stream","name":"stderr","text":["\r 66%|██████▋   | 232/349 [06:21<03:13,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in the back seat \n","\n","CLIP Similarity Score for image 474164: 0.28078070282936096\n","http://images.cocodataset.org/val2017/000000225670.jpg\n","tensor([[21.9635, 24.4081, 23.7708, 26.4024]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 225670: a photo of a car\n","Generated Caption: a skateboarder jumping over a basketball \n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 233/349 [06:23<03:11,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car jumping in the air with a skateboard \n","\n","CLIP Similarity Score for image 225670: 0.23136931657791138\n","http://images.cocodataset.org/val2017/000000287649.jpg\n","tensor([[22.3646, 24.2465, 23.9016, 26.4379]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 287649: a photo of a car\n","Generated Caption: a cat sitting on a desk next to a computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 234/349 [06:25<03:13,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a desk \n","\n","CLIP Similarity Score for image 287649: 0.23746928572654724\n","http://images.cocodataset.org/val2017/000000511398.jpg\n","tensor([[22.2922, 24.2812, 23.8936, 26.1937]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 511398: a photo of a car\n","Generated Caption: a dog laying on the ground next to a towel \n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 235/349 [06:26<03:15,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on it \n","\n","CLIP Similarity Score for image 511398: 0.20130124688148499\n","http://images.cocodataset.org/val2017/000000269314.jpg\n","tensor([[21.8885, 24.0028, 23.8695, 26.2294]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 269314: a photo of a car\n","Generated Caption: a banana tree with a bunch of bananas hanging from it \n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 236/349 [06:28<03:13,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a bunch of bananas hanging from it \n","\n","CLIP Similarity Score for image 269314: 0.2577815353870392\n","http://images.cocodataset.org/val2017/000000533536.jpg\n","tensor([[22.4624, 24.6597, 24.2505, 26.5723]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 533536: a photo of a car\n","Generated Caption: a cat sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 237/349 [06:30<03:08,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a cat on the window sill \n","\n","CLIP Similarity Score for image 533536: 0.1934107542037964\n","http://images.cocodataset.org/val2017/000000412240.jpg\n","tensor([[22.2048, 24.3456, 23.6561, 26.5176]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 412240: a photo of a car\n","Generated Caption: a cat laying on a table next to a book \n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 238/349 [06:31<03:04,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the floor \n","\n","CLIP Similarity Score for image 412240: 0.2601729929447174\n","http://images.cocodataset.org/val2017/000000492284.jpg\n","tensor([[22.1819, 24.1669, 23.9611, 26.3035]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 492284: a photo of a car\n","Generated Caption: a woman standing next to a rock wall with a sheep \n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 239/349 [06:33<02:59,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cow \n","\n","CLIP Similarity Score for image 492284: 0.1874198019504547\n","http://images.cocodataset.org/val2017/000000458255.jpg\n","tensor([[22.0753, 24.2680, 23.4924, 26.2653]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 458255: a photo of a car\n","Generated Caption: a cat laying on a bed with a person \n"]},{"output_type":"stream","name":"stderr","text":["\r 69%|██████▉   | 240/349 [06:35<02:58,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the bed \n","\n","CLIP Similarity Score for image 458255: 0.2867933213710785\n","http://images.cocodataset.org/val2017/000000264441.jpg\n","tensor([[22.6530, 24.6610, 24.2583, 26.5846]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 264441: a photo of a car\n","Generated Caption: a white fluffy cat laying on a chair \n"]},{"output_type":"stream","name":"stderr","text":["\r 69%|██████▉   | 241/349 [06:36<02:54,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a white cat on it \n","\n","CLIP Similarity Score for image 264441: 0.22171562910079956\n","http://images.cocodataset.org/val2017/000000478393.jpg\n","tensor([[21.9430, 24.1458, 23.6056, 26.2834]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 478393: a photo of a car\n","Generated Caption: a cat and a dog laying on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 69%|██████▉   | 242/349 [06:38<02:55,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and cat on a bed \n","\n","CLIP Similarity Score for image 478393: 0.2664353549480438\n","http://images.cocodataset.org/val2017/000000540928.jpg\n","tensor([[21.5620, 23.7705, 23.1613, 25.9107]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 540928: a photo of a car\n","Generated Caption: a cat sitting on a chair next to a toy elephant \n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|██████▉   | 243/349 [06:39<02:53,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 540928: 0.19187504053115845\n","http://images.cocodataset.org/val2017/000000231831.jpg\n","tensor([[22.7998, 24.7756, 24.1124, 26.4913]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 231831: a photo of a car\n","Generated Caption: a cat sitting on a table next to a table cloth \n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|██████▉   | 244/349 [06:41<02:51,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the table \n","\n","CLIP Similarity Score for image 231831: 0.27368760108947754\n","http://images.cocodataset.org/val2017/000000399655.jpg\n","tensor([[21.9301, 24.1660, 23.5150, 26.4321]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 399655: a photo of a car\n","Generated Caption: a dog wearing a red shirt and a black and white striped shirt \n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 245/349 [06:43<02:53,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 399655: 0.17665953934192657\n","http://images.cocodataset.org/val2017/000000489014.jpg\n","tensor([[22.4888, 24.5021, 23.8717, 26.6126]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 489014: a photo of a car\n","Generated Caption: a white and black dog standing on top of a sailboat \n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 246/349 [06:44<02:52,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the back \n","\n","CLIP Similarity Score for image 489014: 0.18542958796024323\n","http://images.cocodataset.org/val2017/000000149568.jpg\n","tensor([[21.9085, 24.1330, 23.7102, 26.3234]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 149568: a photo of a car\n","Generated Caption: a dog running with a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 71%|███████   | 247/349 [06:46<02:53,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog and a frisbee \n","\n","CLIP Similarity Score for image 149568: 0.2164475917816162\n","http://images.cocodataset.org/val2017/000000058111.jpg\n","tensor([[22.5252, 24.4829, 23.9496, 26.4666]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 58111: a photo of a car\n","Generated Caption: a cat is sitting on a counter top \n"]},{"output_type":"stream","name":"stderr","text":["\r 71%|███████   | 248/349 [06:48<02:44,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 58111: 0.1997046172618866\n","http://images.cocodataset.org/val2017/000000245576.jpg\n","tensor([[22.7812, 24.8063, 24.2084, 26.8081]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 245576: a photo of a car\n","Generated Caption: a cat is laying on a keyboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 71%|███████▏  | 249/349 [06:49<02:38,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 245576: 0.2818373739719391\n","http://images.cocodataset.org/val2017/000000534270.jpg\n","tensor([[22.1120, 24.3775, 23.6410, 26.3560]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 534270: a photo of a car\n","Generated Caption: a man and woman standing next to each other \n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 250/349 [06:51<02:46,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a man holding an umbrella \n","\n","CLIP Similarity Score for image 534270: 0.2605646252632141\n","http://images.cocodataset.org/val2017/000000476810.jpg\n","tensor([[22.5672, 24.5282, 24.0702, 26.6452]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 476810: a photo of a car\n","Generated Caption: a black cat laying on a table next to a remote control \n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 251/349 [06:53<02:49,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat laying on top of it \n","\n","CLIP Similarity Score for image 476810: 0.20660345256328583\n","http://images.cocodataset.org/val2017/000000347930.jpg\n","tensor([[22.2648, 24.3889, 23.8773, 26.3337]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 347930: a photo of a car\n","Generated Caption: a dog wearing a hat on a couch \n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 252/349 [06:54<02:41,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 347930: 0.20306925475597382\n","http://images.cocodataset.org/val2017/000000273232.jpg\n","tensor([[22.2833, 24.4537, 23.7972, 26.7645]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 273232: a photo of a car\n","Generated Caption: a man carrying a surfboard and a dog on a beach \n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 253/349 [06:56<02:41,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog and a surfboard \n","\n","CLIP Similarity Score for image 273232: 0.2927154302597046\n","http://images.cocodataset.org/val2017/000000082807.jpg\n","tensor([[22.0896, 23.9909, 23.2217, 26.0547]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 82807: a photo of a car\n","Generated Caption: a dog sitting on a table with a cup of coffee \n"]},{"output_type":"stream","name":"stderr","text":["\r 73%|███████▎  | 254/349 [06:58<02:40,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting at a table with a dog \n","\n","CLIP Similarity Score for image 82807: 0.27807554602622986\n","http://images.cocodataset.org/val2017/000000443303.jpg\n","tensor([[22.7915, 24.7002, 24.1174, 26.5863]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 443303: a photo of a car\n","Generated Caption: a cat is laying on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 73%|███████▎  | 255/349 [06:59<02:35,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat laying on top of it \n","\n","CLIP Similarity Score for image 443303: 0.19075541198253632\n","http://images.cocodataset.org/val2017/000000300913.jpg\n","tensor([[23.0349, 24.8687, 24.2948, 26.7671]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 300913: a photo of a car\n","Generated Caption: a cat is laying on a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 73%|███████▎  | 256/349 [07:01<02:27,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 300913: 0.22379137575626373\n","http://images.cocodataset.org/val2017/000000491757.jpg\n","tensor([[22.6009, 24.6254, 23.9560, 26.8082]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 491757: a photo of a car\n","Generated Caption: a cat is sitting on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▎  | 257/349 [07:02<02:26,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the bed \n","\n","CLIP Similarity Score for image 491757: 0.3136166036128998\n","http://images.cocodataset.org/val2017/000000350054.jpg\n","tensor([[21.9974, 24.1498, 23.2080, 26.0967]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 350054: a photo of a car\n","Generated Caption: a tv is on in a living room \n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 258/349 [07:04<02:34,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a christmas tree \n","\n","CLIP Similarity Score for image 350054: 0.21040993928909302\n","http://images.cocodataset.org/val2017/000000236784.jpg\n","tensor([[22.6463, 24.7320, 24.2439, 26.7199]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 236784: a photo of a car\n","Generated Caption: a dog laying on a couch with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 259/349 [07:06<02:30,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on it \n","\n","CLIP Similarity Score for image 236784: 0.19525916874408722\n","http://images.cocodataset.org/val2017/000000353970.jpg\n","tensor([[22.7267, 24.6910, 24.3027, 26.6966]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 353970: a photo of a car\n","Generated Caption: a cat is sitting on a chair \n"]},{"output_type":"stream","name":"stderr","text":["\r 74%|███████▍  | 260/349 [07:08<02:23,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the back \n","\n","CLIP Similarity Score for image 353970: 0.2200596034526825\n","http://images.cocodataset.org/val2017/000000143961.jpg\n","tensor([[22.2582, 24.2030, 23.7686, 26.1394]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 143961: a photo of a car\n","Generated Caption: a woman sitting on the grass with a rainbow umbrella \n"]},{"output_type":"stream","name":"stderr","text":["\r 75%|███████▍  | 261/349 [07:09<02:21,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and people sitting on the grass \n","\n","CLIP Similarity Score for image 143961: 0.25019463896751404\n","http://images.cocodataset.org/val2017/000000206831.jpg\n","tensor([[22.2493, 24.4859, 24.0825, 26.4093]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 206831: a photo of a car\n","Generated Caption: a dog with a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 75%|███████▌  | 262/349 [07:11<02:20,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in it's mouth \n","\n","CLIP Similarity Score for image 206831: 0.2363496720790863\n","http://images.cocodataset.org/val2017/000000570664.jpg\n","tensor([[22.5196, 24.3237, 23.7599, 26.5149]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 570664: a photo of a car\n","Generated Caption: a cat is sitting on a counter top \n"]},{"output_type":"stream","name":"stderr","text":["\r 75%|███████▌  | 263/349 [07:12<02:16,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on top of it \n","\n","CLIP Similarity Score for image 570664: 0.19902338087558746\n","http://images.cocodataset.org/val2017/000000166277.jpg\n","tensor([[22.4424, 24.0653, 23.5308, 26.4157]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 166277: a photo of a car\n","Generated Caption: a cat drinking water from a glass \n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 264/349 [07:14<02:14,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat drinking from a cup \n","\n","CLIP Similarity Score for image 166277: 0.26740944385528564\n","http://images.cocodataset.org/val2017/000000505573.jpg\n","tensor([[22.1598, 24.4271, 23.7963, 26.4041]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 505573: a photo of a car\n","Generated Caption: a dog wearing a red bow tie \n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 265/349 [07:15<02:11,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the hood \n","\n","CLIP Similarity Score for image 505573: 0.18001092970371246\n","http://images.cocodataset.org/val2017/000000338624.jpg\n","tensor([[22.0534, 24.0383, 23.5480, 26.2696]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 338624: a photo of a car\n","Generated Caption: a man walking down a sidewalk with a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 266/349 [07:17<02:15,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a dog walking down a sidewalk \n","\n","CLIP Similarity Score for image 338624: 0.29741400480270386\n","http://images.cocodataset.org/val2017/000000271728.jpg\n","tensor([[22.1838, 24.3580, 23.6993, 26.5949]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 271728: a photo of a car\n","Generated Caption: a cat sitting on a couch next to a remote control \n"]},{"output_type":"stream","name":"stderr","text":["\r 77%|███████▋  | 267/349 [07:19<02:19,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting on top of it \n","\n","CLIP Similarity Score for image 271728: 0.21304106712341309\n","http://images.cocodataset.org/val2017/000000405306.jpg\n","tensor([[23.2638, 25.1087, 24.6377, 26.8080]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 405306: a photo of a car\n","Generated Caption: a cat laying on a bed with a red blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 77%|███████▋  | 268/349 [07:21<02:14,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 405306: 0.20722976326942444\n","http://images.cocodataset.org/val2017/000000377000.jpg\n","tensor([[22.4724, 24.3870, 24.3239, 26.2498]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 377000: a photo of a car\n","Generated Caption: a cat is looking out the window of a window \n"]},{"output_type":"stream","name":"stderr","text":["\r 77%|███████▋  | 269/349 [07:22<02:11,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car window with a cat \n","\n","CLIP Similarity Score for image 377000: 0.23708494007587433\n","http://images.cocodataset.org/val2017/000000519764.jpg\n","tensor([[22.4337, 24.5653, 23.8427, 26.4668]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 519764: a photo of a car\n","Generated Caption: a cat sitting on a chair in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 77%|███████▋  | 270/349 [07:24<02:09,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat sitting on the seat \n","\n","CLIP Similarity Score for image 519764: 0.25559520721435547\n","http://images.cocodataset.org/val2017/000000375278.jpg\n","tensor([[22.7467, 24.6547, 24.2584, 26.6166]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 375278: a photo of a car\n","Generated Caption: a black cat is laying on a person's lap \n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 271/349 [07:25<02:07,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the hood \n","\n","CLIP Similarity Score for image 375278: 0.18543225526809692\n","http://images.cocodataset.org/val2017/000000295478.jpg\n","tensor([[22.2335, 24.3144, 24.0257, 26.1442]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 295478: a photo of a car\n","Generated Caption: a woman walking down a sidewalk with a bag \n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 272/349 [07:27<02:04,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a woman walking \n","\n","CLIP Similarity Score for image 295478: 0.257795125246048\n","http://images.cocodataset.org/val2017/000000554579.jpg\n","tensor([[22.3199, 24.2018, 23.6224, 26.4974]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 554579: a photo of a car\n","Generated Caption: a woman standing next to a man holding a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 273/349 [07:29<02:08,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a woman standing next to each other \n","\n","CLIP Similarity Score for image 554579: 0.20476944744586945\n","http://images.cocodataset.org/val2017/000000415990.jpg\n","tensor([[22.1267, 24.6626, 23.7565, 26.2059]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 415990: a photo of a car\n","Generated Caption: a herd of cattle standing in a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 79%|███████▊  | 274/349 [07:30<02:03,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car herd of cattle \n","\n","CLIP Similarity Score for image 415990: 0.2936345040798187\n","http://images.cocodataset.org/val2017/000000101420.jpg\n","tensor([[22.0590, 24.1421, 23.5828, 26.4625]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 101420: a photo of a car\n","Generated Caption: a cat is laying on a couch in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 79%|███████▉  | 275/349 [07:32<02:05,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the back \n","\n","CLIP Similarity Score for image 101420: 0.23629426956176758\n","http://images.cocodataset.org/val2017/000000460841.jpg\n","tensor([[22.3868, 24.3496, 23.8139, 26.4607]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 460841: a photo of a car\n","Generated Caption: a person is looking at a cell phone \n"]},{"output_type":"stream","name":"stderr","text":["\r 79%|███████▉  | 276/349 [07:34<02:00,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a person holding a cell phone \n","\n","CLIP Similarity Score for image 460841: 0.15180179476737976\n","http://images.cocodataset.org/val2017/000000367082.jpg\n","tensor([[22.0008, 23.9544, 23.4974, 26.2573]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 367082: a photo of a car\n","Generated Caption: a dog is sitting on a pillow in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 79%|███████▉  | 277/349 [07:35<01:58,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the bed \n","\n","CLIP Similarity Score for image 367082: 0.26175960898399353\n","http://images.cocodataset.org/val2017/000000273642.jpg\n","tensor([[22.8283, 24.9904, 24.2363, 26.5964]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 273642: a photo of a car\n","Generated Caption: a dog holding a remote control in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|███████▉  | 278/349 [07:37<01:54,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 273642: 0.18846037983894348\n","http://images.cocodataset.org/val2017/000000078420.jpg\n","tensor([[22.1807, 24.3446, 23.4319, 26.5263]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 78420: a photo of a car\n","Generated Caption: a cat laying on a laptop computer \n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|███████▉  | 279/349 [07:38<01:49,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the floor \n","\n","CLIP Similarity Score for image 78420: 0.20291291177272797\n","http://images.cocodataset.org/val2017/000000018833.jpg\n","tensor([[22.6420, 24.6592, 24.1688, 26.5778]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 18833: a photo of a car\n","Generated Caption: a cat laying on a rug with shoes \n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 280/349 [07:40<01:44,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 18833: 0.21352916955947876\n","http://images.cocodataset.org/val2017/000000389933.jpg\n","tensor([[22.9319, 24.9433, 24.2953, 26.5412]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 389933: a photo of a car\n","Generated Caption: a brown dog sitting on a couch \n"]},{"output_type":"stream","name":"stderr","text":["\r 81%|████████  | 281/349 [07:41<01:41,  1.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 389933: 0.20473583042621613\n","http://images.cocodataset.org/val2017/000000253386.jpg\n","tensor([[22.4553, 24.6557, 24.0911, 26.3358]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 253386: a photo of a car\n","Generated Caption: a black dog with a black collar and a black dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 81%|████████  | 282/349 [07:43<01:47,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog \n","\n","CLIP Similarity Score for image 253386: 0.2295524775981903\n","http://images.cocodataset.org/val2017/000000329447.jpg\n","tensor([[22.3507, 24.3707, 23.9926, 26.4920]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 329447: a photo of a car\n","Generated Caption: two cows and a calf standing in a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 81%|████████  | 283/349 [07:45<01:48,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a cow \n","\n","CLIP Similarity Score for image 329447: 0.2666189968585968\n","http://images.cocodataset.org/val2017/000000170278.jpg\n","tensor([[22.3390, 24.3654, 23.9905, 26.4489]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 170278: a photo of a car\n","Generated Caption: a dog laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 81%|████████▏ | 284/349 [07:46<01:46,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car bed with a dog laying on it \n","\n","CLIP Similarity Score for image 170278: 0.27995115518569946\n","http://images.cocodataset.org/val2017/000000262938.jpg\n","tensor([[22.3518, 24.4871, 23.8312, 26.6630]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 262938: a photo of a car\n","Generated Caption: a cat sitting on a table next to a book \n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 285/349 [07:48<01:43,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 262938: 0.14173011481761932\n","http://images.cocodataset.org/val2017/000000236592.jpg\n","tensor([[22.3487, 24.3201, 23.6251, 26.4612]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 236592: a photo of a car\n","Generated Caption: a cat is looking out of an open oven \n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 286/349 [07:50<01:41,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat inside of it \n","\n","CLIP Similarity Score for image 236592: 0.17082485556602478\n","http://images.cocodataset.org/val2017/000000479155.jpg\n","tensor([[22.7366, 24.6653, 24.0157, 26.4766]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 479155: a photo of a car\n","Generated Caption: a woman walking down a street with a bunch of fruit \n"]},{"output_type":"stream","name":"stderr","text":["\r 82%|████████▏ | 287/349 [07:51<01:40,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a bunch of fruit \n","\n","CLIP Similarity Score for image 479155: 0.21742409467697144\n","http://images.cocodataset.org/val2017/000000269113.jpg\n","tensor([[22.3363, 24.7697, 24.1239, 26.4052]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 269113: a photo of a car\n","Generated Caption: a dog running across a field with a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 83%|████████▎ | 288/349 [07:53<01:41,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and dog running in a field \n","\n","CLIP Similarity Score for image 269113: 0.2553756833076477\n","http://images.cocodataset.org/val2017/000000255664.jpg\n","tensor([[22.3829, 24.6853, 24.0260, 26.3805]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 255664: a photo of a car\n","Generated Caption: a dog catching a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 83%|████████▎ | 289/349 [07:55<01:40,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog running in the grass \n","\n","CLIP Similarity Score for image 255664: 0.25846850872039795\n","http://images.cocodataset.org/val2017/000000357459.jpg\n","tensor([[22.2588, 24.3907, 23.9847, 26.0962]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 357459: a photo of a car\n","Generated Caption: a dog is playing with a frisbee in the grass \n"]},{"output_type":"stream","name":"stderr","text":["\r 83%|████████▎ | 290/349 [07:57<01:43,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in the grass \n","\n","CLIP Similarity Score for image 357459: 0.2089613974094391\n","http://images.cocodataset.org/val2017/000000555005.jpg\n","tensor([[22.0514, 24.2715, 23.4284, 26.2445]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 555005: a photo of a car\n","Generated Caption: a man in a black shirt and tie \n"]},{"output_type":"stream","name":"stderr","text":["\r 83%|████████▎ | 291/349 [07:58<01:40,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a man in a black shirt \n","\n","CLIP Similarity Score for image 555005: 0.16101676225662231\n","http://images.cocodataset.org/val2017/000000562561.jpg\n","tensor([[22.7336, 24.8449, 24.0337, 26.6697]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 562561: a photo of a car\n","Generated Caption: a dog is standing on a leash with a cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▎ | 292/349 [08:00<01:37,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the leash \n","\n","CLIP Similarity Score for image 562561: 0.2032977044582367\n","http://images.cocodataset.org/val2017/000000355905.jpg\n","tensor([[22.0081, 24.2759, 23.6608, 26.0420]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 355905: a photo of a car\n","Generated Caption: a dog standing on the beach with its head down \n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▍ | 293/349 [08:01<01:30,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on the beach \n","\n","CLIP Similarity Score for image 355905: 0.21019797027111053\n","http://images.cocodataset.org/val2017/000000464522.jpg\n","tensor([[22.1313, 24.2743, 23.9423, 26.1931]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 464522: a photo of a car\n","Generated Caption: a dog standing on a grassy field \n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▍ | 294/349 [08:03<01:26,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car dog with a dog tag \n","\n","CLIP Similarity Score for image 464522: 0.27516302466392517\n","http://images.cocodataset.org/val2017/000000022892.jpg\n","tensor([[22.2277, 24.4017, 23.9194, 26.3081]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 22892: a photo of a car\n","Generated Caption: a cat looking at a cat sitting on a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 85%|████████▍ | 295/349 [08:04<01:23,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a cat \n","\n","CLIP Similarity Score for image 22892: 0.23727823793888092\n","http://images.cocodataset.org/val2017/000000447200.jpg\n","tensor([[22.5896, 24.5516, 24.4330, 26.5589]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 447200: a photo of a car\n","Generated Caption: two dogs are standing next to each other \n"]},{"output_type":"stream","name":"stderr","text":["\r 85%|████████▍ | 296/349 [08:06<01:21,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and dog with a cat \n","\n","CLIP Similarity Score for image 447200: 0.21623706817626953\n","http://images.cocodataset.org/val2017/000000221693.jpg\n","tensor([[22.2842, 24.8578, 24.1835, 26.4683]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 221693: a photo of a car\n","Generated Caption: a dog with a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 85%|████████▌ | 297/349 [08:07<01:20,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in the grass \n","\n","CLIP Similarity Score for image 221693: 0.21462957561016083\n","http://images.cocodataset.org/val2017/000000486479.jpg\n","tensor([[22.2962, 24.5024, 23.9032, 26.3666]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 486479: a photo of a car\n","Generated Caption: a dog laying on a blanket on the floor \n"]},{"output_type":"stream","name":"stderr","text":["\r 85%|████████▌ | 298/349 [08:10<01:26,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on the floor \n","\n","CLIP Similarity Score for image 486479: 0.22899113595485687\n","http://images.cocodataset.org/val2017/000000283412.jpg\n","tensor([[22.2935, 24.6087, 23.9460, 26.5208]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 283412: a photo of a car\n","Generated Caption: a white dog sitting on a table with a white plate \n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 299/349 [08:11<01:26,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a white dog \n","\n","CLIP Similarity Score for image 283412: 0.24580442905426025\n","http://images.cocodataset.org/val2017/000000367195.jpg\n","tensor([[22.8631, 24.8247, 24.2711, 26.7308]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 367195: a photo of a car\n","Generated Caption: a dog laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 300/349 [08:13<01:21,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on it \n","\n","CLIP Similarity Score for image 367195: 0.22272886335849762\n","http://images.cocodataset.org/val2017/000000331075.jpg\n","tensor([[22.3980, 24.5449, 24.2314, 26.2573]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 331075: a photo of a car\n","Generated Caption: a dog standing on top of a beach \n"]},{"output_type":"stream","name":"stderr","text":["\r 86%|████████▌ | 301/349 [08:14<01:17,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and dog on the beach \n","\n","CLIP Similarity Score for image 331075: 0.2592480182647705\n","http://images.cocodataset.org/val2017/000000324158.jpg\n","tensor([[22.1717, 24.1911, 23.7121, 26.4943]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 324158: a photo of a car\n","Generated Caption: a man on a skateboard is going down a road \n"]},{"output_type":"stream","name":"stderr","text":["\r 87%|████████▋ | 302/349 [08:16<01:15,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on a road with a dog \n","\n","CLIP Similarity Score for image 324158: 0.2673117220401764\n","http://images.cocodataset.org/val2017/000000064868.jpg\n","tensor([[22.1630, 24.2513, 23.7717, 26.3821]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 64868: a photo of a car\n","Generated Caption: a person is petting a cat on a stove \n"]},{"output_type":"stream","name":"stderr","text":["\r 87%|████████▋ | 303/349 [08:17<01:13,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the stove \n","\n","CLIP Similarity Score for image 64868: 0.3214505612850189\n","http://images.cocodataset.org/val2017/000000309938.jpg\n","tensor([[22.7419, 24.4628, 24.0794, 26.6434]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 309938: a photo of a car\n","Generated Caption: a small dog is sitting on a table \n"]},{"output_type":"stream","name":"stderr","text":["\r 87%|████████▋ | 304/349 [08:19<01:10,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 309938: 0.1858932077884674\n","http://images.cocodataset.org/val2017/000000263463.jpg\n","tensor([[22.2832, 24.1815, 23.7040, 26.3623]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 263463: a photo of a car\n","Generated Caption: a dog is looking out the window of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 87%|████████▋ | 305/349 [08:21<01:10,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car mirror with a dog in the reflection \n","\n","CLIP Similarity Score for image 263463: 0.3498072326183319\n","http://images.cocodataset.org/val2017/000000564280.jpg\n","tensor([[22.4487, 24.5231, 23.8686, 26.6777]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 564280: a photo of a car\n","Generated Caption: a dog laying on a couch with a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 306/349 [08:22<01:09,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it \n","\n","CLIP Similarity Score for image 564280: 0.1890544593334198\n","http://images.cocodataset.org/val2017/000000022192.jpg\n","tensor([[22.2695, 24.2918, 23.8734, 26.4536]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 22192: a photo of a car\n","Generated Caption: a dog laying on the ground next to a pile of trash \n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 307/349 [08:24<01:11,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on it \n","\n","CLIP Similarity Score for image 22192: 0.20975542068481445\n","http://images.cocodataset.org/val2017/000000385997.jpg\n","tensor([[22.3015, 24.1895, 23.4945, 26.3119]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 385997: a photo of a car\n","Generated Caption: a cat sitting on a chair in a room \n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 308/349 [08:26<01:07,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting in a room \n","\n","CLIP Similarity Score for image 385997: 0.2630649209022522\n","http://images.cocodataset.org/val2017/000000463522.jpg\n","tensor([[22.3625, 24.4208, 23.9083, 26.6361]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 463522: a photo of a car\n","Generated Caption: a horse drawn carriage on a city street \n"]},{"output_type":"stream","name":"stderr","text":["\r 89%|████████▊ | 309/349 [08:27<01:03,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a horse drawn carriage \n","\n","CLIP Similarity Score for image 463522: 0.29103177785873413\n","http://images.cocodataset.org/val2017/000000125405.jpg\n","tensor([[22.0617, 24.2286, 23.9012, 26.1611]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 125405: a photo of a car\n","Generated Caption: a dog standing on a grassy field with a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 89%|████████▉ | 310/349 [08:29<01:03,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the ground \n","\n","CLIP Similarity Score for image 125405: 0.21460899710655212\n","http://images.cocodataset.org/val2017/000000560880.jpg\n","tensor([[22.1918, 24.3824, 23.7433, 26.4229]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 560880: a photo of a car\n","Generated Caption: people are standing around a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 89%|████████▉ | 311/349 [08:30<00:59,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and people in a field \n","\n","CLIP Similarity Score for image 560880: 0.27625617384910583\n","http://images.cocodataset.org/val2017/000000333772.jpg\n","tensor([[22.3144, 24.2534, 23.5833, 26.3291]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 333772: a photo of a car\n","Generated Caption: a cat laying on top of a computer desk \n"]},{"output_type":"stream","name":"stderr","text":["\r 89%|████████▉ | 312/349 [08:32<00:58,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the keyboard \n","\n","CLIP Similarity Score for image 333772: 0.30063048005104065\n","http://images.cocodataset.org/val2017/000000364636.jpg\n","tensor([[22.4188, 24.5859, 24.1195, 26.1602]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 364636: a photo of a car\n","Generated Caption: a dog with a frisbee in its mouth \n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|████████▉ | 313/349 [08:34<00:58,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on it's back \n","\n","CLIP Similarity Score for image 364636: 0.2042599767446518\n","http://images.cocodataset.org/val2017/000000072813.jpg\n","tensor([[22.7573, 24.6560, 24.2901, 26.7372]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 72813: a photo of a car\n","Generated Caption: a dog laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|████████▉ | 314/349 [08:35<00:57,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car bed with a dog laying on it \n","\n","CLIP Similarity Score for image 72813: 0.2758415639400482\n","http://images.cocodataset.org/val2017/000000090003.jpg\n","tensor([[22.3627, 24.5821, 23.7875, 26.2345]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 90003: a photo of a car\n","Generated Caption: a dog running in the grass with a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 315/349 [08:37<00:58,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog running in the grass \n","\n","CLIP Similarity Score for image 90003: 0.2255295366048813\n","http://images.cocodataset.org/val2017/000000052891.jpg\n","tensor([[22.3243, 24.5980, 24.3215, 26.3014]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 52891: a photo of a car\n","Generated Caption: a dog is playing with a frisbee in the sand \n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████ | 316/349 [08:39<00:56,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the beach \n","\n","CLIP Similarity Score for image 52891: 0.2530219852924347\n","http://images.cocodataset.org/val2017/000000547502.jpg\n","tensor([[21.8843, 24.0237, 23.4662, 26.1099]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 547502: a photo of a car\n","Generated Caption: a dog chasing a dog in a field \n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████ | 317/349 [08:41<00:53,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and dog playing with a dog \n","\n","CLIP Similarity Score for image 547502: 0.2272484302520752\n","http://images.cocodataset.org/val2017/000000289702.jpg\n","tensor([[22.1215, 23.9234, 23.4088, 26.1244]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 289702: a photo of a car\n","Generated Caption: a dog standing in a room next to a pile of trash \n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████ | 318/349 [08:42<00:52,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog in it \n","\n","CLIP Similarity Score for image 289702: 0.18936432898044586\n","http://images.cocodataset.org/val2017/000000369541.jpg\n","tensor([[22.4548, 24.7494, 24.1549, 26.2513]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 369541: a photo of a car\n","Generated Caption: a dog jumping up to catch a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 91%|█████████▏| 319/349 [08:44<00:51,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a frisbee in its mouth \n","\n","CLIP Similarity Score for image 369541: 0.2754899561405182\n","http://images.cocodataset.org/val2017/000000355240.jpg\n","tensor([[22.1715, 24.2236, 23.4754, 26.3778]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 355240: a photo of a car\n","Generated Caption: a dog sitting on a window sill looking out \n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 320/349 [08:46<00:48,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the window \n","\n","CLIP Similarity Score for image 355240: 0.27429887652397156\n","http://images.cocodataset.org/val2017/000000017029.jpg\n","tensor([[22.2522, 24.5208, 23.9764, 26.2785]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 17029: a photo of a car\n","Generated Caption: a black cat jumping up to catch a frisbee \n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 321/349 [08:47<00:48,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a frisbee in its mouth \n","\n","CLIP Similarity Score for image 17029: 0.2666105628013611\n","http://images.cocodataset.org/val2017/000000151962.jpg\n","tensor([[22.7480, 24.7963, 24.1306, 26.7316]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 151962: a photo of a car\n","Generated Caption: a dog is looking out the window of a car \n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 322/349 [08:49<00:47,  1.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car mirror with a dog in the reflection \n","\n","CLIP Similarity Score for image 151962: 0.32405802607536316\n","http://images.cocodataset.org/val2017/000000421455.jpg\n","tensor([[22.4372, 24.4857, 24.0507, 26.4971]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 421455: a photo of a car\n","Generated Caption: a person taking a picture of themselves in a car mirror \n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 323/349 [08:51<00:46,  1.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car mirror with a person holding a camera \n","\n","CLIP Similarity Score for image 421455: 0.2855606973171234\n","http://images.cocodataset.org/val2017/000000134112.jpg\n","tensor([[22.4218, 24.4405, 23.7679, 26.5523]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 134112: a photo of a car\n","Generated Caption: a dog is sitting on a bed with a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 324/349 [08:53<00:43,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the bed \n","\n","CLIP Similarity Score for image 134112: 0.24474109709262848\n","http://images.cocodataset.org/val2017/000000134882.jpg\n","tensor([[22.1158, 24.1510, 23.5873, 26.5207]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 134882: a photo of a car\n","Generated Caption: a bed with a white sheet and a black and white cat \n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 325/349 [08:55<00:42,  1.78s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car bed with a cat laying on top of it \n","\n","CLIP Similarity Score for image 134882: 0.23813419044017792\n","http://images.cocodataset.org/val2017/000000061333.jpg\n","tensor([[22.7954, 24.6041, 23.9657, 26.4276]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 61333: a photo of a car\n","Generated Caption: a cat laying on a blanket on a bed \n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 326/349 [08:56<00:40,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat laying on top of it \n","\n","CLIP Similarity Score for image 61333: 0.2450714260339737\n","http://images.cocodataset.org/val2017/000000202445.jpg\n","tensor([[22.1128, 24.3170, 23.7594, 26.2767]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 202445: a photo of a car\n","Generated Caption: a cat is laying on a bed with a picture of a cat on it \n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▎| 327/349 [08:58<00:38,  1.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the bed \n","\n","CLIP Similarity Score for image 202445: 0.2688494026660919\n","http://images.cocodataset.org/val2017/000000318238.jpg\n","tensor([[21.9750, 24.1676, 23.5329, 26.1624]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 318238: a photo of a car\n","Generated Caption: a dog laying on a bed with a person \n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▍| 328/349 [09:00<00:36,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog laying on it \n","\n","CLIP Similarity Score for image 318238: 0.19182956218719482\n","http://images.cocodataset.org/val2017/000000159458.jpg\n","tensor([[22.1726, 24.3919, 23.4804, 26.4936]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 159458: a photo of a car\n","Generated Caption: a bed with a stuffed animal on top of it \n"]},{"output_type":"stream","name":"stderr","text":["\r 94%|█████████▍| 329/349 [09:02<00:37,  1.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car bed with a stuffed animal on top of it \n","\n","CLIP Similarity Score for image 159458: 0.21843671798706055\n","http://images.cocodataset.org/val2017/000000193674.jpg\n","tensor([[21.8842, 24.4545, 23.8362, 25.9676]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 193674: a photo of a car\n","Generated Caption: a man in a wet suit is standing on a surfboard \n"]},{"output_type":"stream","name":"stderr","text":["\r 95%|█████████▍| 330/349 [09:04<00:35,  1.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car surfer in the ocean \n","\n","CLIP Similarity Score for image 193674: 0.27739378809928894\n","http://images.cocodataset.org/val2017/000000494913.jpg\n","tensor([[22.3778, 24.4618, 23.7848, 26.5187]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 494913: a photo of a car\n","Generated Caption: a living room with a couch, television, and a coffee table \n"]},{"output_type":"stream","name":"stderr","text":["\r 95%|█████████▍| 331/349 [09:05<00:32,  1.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a living room \n","\n","CLIP Similarity Score for image 494913: 0.27681097388267517\n","http://images.cocodataset.org/val2017/000000371749.jpg\n","tensor([[22.7849, 24.7152, 24.2629, 26.5608]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 371749: a photo of a car\n","Generated Caption: a person holding a wii remote in their hand \n"]},{"output_type":"stream","name":"stderr","text":["\r 95%|█████████▌| 332/349 [09:07<00:29,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a person playing a video game \n","\n","CLIP Similarity Score for image 371749: 0.2849520146846771\n","http://images.cocodataset.org/val2017/000000121586.jpg\n","tensor([[21.9502, 24.0006, 23.1895, 26.2993]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 121586: a photo of a car\n","Generated Caption: a television is on in a room with a book shelf \n"]},{"output_type":"stream","name":"stderr","text":["\r 95%|█████████▌| 333/349 [09:09<00:27,  1.72s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and tv on a tv stand \n","\n","CLIP Similarity Score for image 121586: 0.23130889236927032\n","http://images.cocodataset.org/val2017/000000435299.jpg\n","tensor([[22.2739, 24.4488, 23.9803, 26.5431]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 435299: a photo of a car\n","Generated Caption: a cat is laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▌| 334/349 [09:10<00:25,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on it \n","\n","CLIP Similarity Score for image 435299: 0.21877096593379974\n","http://images.cocodataset.org/val2017/000000366611.jpg\n","tensor([[22.2543, 24.3388, 24.0216, 26.4364]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 366611: a photo of a car\n","Generated Caption: a dog is standing on the ground by a grassy area \n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▌| 335/349 [09:12<00:23,  1.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the ground \n","\n","CLIP Similarity Score for image 366611: 0.22728103399276733\n","http://images.cocodataset.org/val2017/000000530624.jpg\n","tensor([[22.0668, 24.4322, 23.8199, 26.2883]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 530624: a photo of a car\n","Generated Caption: a cat laying on a bed with a blanket \n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▋| 336/349 [09:14<00:22,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car bed with a cat laying on it \n","\n","CLIP Similarity Score for image 530624: 0.2069150060415268\n","http://images.cocodataset.org/val2017/000000427034.jpg\n","tensor([[22.0588, 24.1362, 23.6007, 26.4068]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 427034: a photo of a car\n","Generated Caption: a dog is sitting on a lamp post \n"]},{"output_type":"stream","name":"stderr","text":["\r 97%|█████████▋| 337/349 [09:16<00:21,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the dashboard \n","\n","CLIP Similarity Score for image 427034: 0.20412777364253998\n","http://images.cocodataset.org/val2017/000000139684.jpg\n","tensor([[22.2836, 24.2718, 23.5895, 26.5076]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 139684: a photo of a car\n","Generated Caption: a living room with a couch, television, and a coffee table \n"]},{"output_type":"stream","name":"stderr","text":["\r 97%|█████████▋| 338/349 [09:18<00:19,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a couch in a living room \n","\n","CLIP Similarity Score for image 139684: 0.23746538162231445\n","http://images.cocodataset.org/val2017/000000057760.jpg\n","tensor([[22.3686, 24.6314, 23.9782, 26.4997]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 57760: a photo of a car\n","Generated Caption: people on a beach with a beach ball \n"]},{"output_type":"stream","name":"stderr","text":["\r 97%|█████████▋| 339/349 [09:19<00:16,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and people on the beach \n","\n","CLIP Similarity Score for image 57760: 0.27491700649261475\n","http://images.cocodataset.org/val2017/000000543581.jpg\n","tensor([[22.0774, 24.2465, 23.3934, 26.2506]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 543581: a photo of a car\n","Generated Caption: a living room with a couch, chair, and a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 97%|█████████▋| 340/349 [09:21<00:15,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car sitting on a hard wood floor \n","\n","CLIP Similarity Score for image 543581: 0.2142062485218048\n","http://images.cocodataset.org/val2017/000000471789.jpg\n","tensor([[22.2240, 24.5872, 24.4018, 26.3603]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 471789: a photo of a car\n","Generated Caption: a man flying a kite in a park \n"]},{"output_type":"stream","name":"stderr","text":["\r 98%|█████████▊| 341/349 [09:22<00:13,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a person flying a kite \n","\n","CLIP Similarity Score for image 471789: 0.31935009360313416\n","http://images.cocodataset.org/val2017/000000117908.jpg\n","tensor([[22.3915, 24.4283, 24.0796, 26.5444]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 117908: a photo of a car\n","Generated Caption: a cat is looking at the refrigerator door \n"]},{"output_type":"stream","name":"stderr","text":["\r 98%|█████████▊| 342/349 [09:24<00:11,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat in the refrigerator \n","\n","CLIP Similarity Score for image 117908: 0.3111574053764343\n","http://images.cocodataset.org/val2017/000000366141.jpg\n","tensor([[21.8858, 23.8895, 23.2951, 26.1281]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 366141: a photo of a car\n","Generated Caption: a living room with a couch, chair, and table \n"]},{"output_type":"stream","name":"stderr","text":["\r 98%|█████████▊| 343/349 [09:25<00:09,  1.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a couch in a living room \n","\n","CLIP Similarity Score for image 366141: 0.258658230304718\n","http://images.cocodataset.org/val2017/000000290843.jpg\n","tensor([[22.5245, 24.7074, 23.9839, 26.7830]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 290843: a photo of a car\n","Generated Caption: a cat sitting on top of a bed next to a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 99%|█████████▊| 344/349 [09:27<00:08,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on the bed \n","\n","CLIP Similarity Score for image 290843: 0.23627802729606628\n","http://images.cocodataset.org/val2017/000000014007.jpg\n","tensor([[22.3616, 24.3880, 23.8564, 26.4685]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 14007: a photo of a car\n","Generated Caption: a cat is standing on a ledge in a bathroom \n"]},{"output_type":"stream","name":"stderr","text":["\r 99%|█████████▉| 345/349 [09:29<00:06,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on a wall \n","\n","CLIP Similarity Score for image 14007: 0.18232151865959167\n","http://images.cocodataset.org/val2017/000000371699.jpg\n","tensor([[22.0756, 23.9604, 23.3082, 26.2041]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 371699: a photo of a car\n","Generated Caption: a woman sitting at a table with a laptop \n"]},{"output_type":"stream","name":"stderr","text":["\r 99%|█████████▉| 346/349 [09:30<00:05,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car and a table with a laptop \n","\n","CLIP Similarity Score for image 371699: 0.2262517809867859\n","http://images.cocodataset.org/val2017/000000084362.jpg\n","tensor([[22.5283, 24.3938, 23.7485, 26.2967]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 84362: a photo of a car\n","Generated Caption: a man sitting on a couch watching a dog \n"]},{"output_type":"stream","name":"stderr","text":["\r 99%|█████████▉| 347/349 [09:32<00:03,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a dog on the windowsill \n","\n","CLIP Similarity Score for image 84362: 0.24771972000598907\n","http://images.cocodataset.org/val2017/000000243344.jpg\n","tensor([[22.2426, 24.3626, 23.6404, 26.4759]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 243344: a photo of a car\n","Generated Caption: a cat sitting on top of a refrigerator \n"]},{"output_type":"stream","name":"stderr","text":["\r100%|█████████▉| 348/349 [09:34<00:01,  1.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car with a cat on top of it \n","\n","CLIP Similarity Score for image 243344: 0.23241785168647766\n","http://images.cocodataset.org/val2017/000000395801.jpg\n","tensor([[22.3946, 24.3567, 23.9180, 26.5012]], grad_fn=<TBackward0>)\n","Best caption by CLIP for image 395801: a photo of a car\n","Generated Caption: a man riding a skateboard down a street \n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 349/349 [09:35<00:00,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Generated Caption with CLIP influence: a photo of a car on a street with people walking \n","\n","CLIP Similarity Score for image 395801: 0.24241767823696136\n","\n","Average CLIP Similarity Score for 349 images: 0.24601495125061462\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Test on single image"],"metadata":{"id":"V1gu2a3akNmN"}},{"cell_type":"code","source":["# Use CLIP to compute similarity scores for predefined captions\n","# candidate_captions = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of an animal\"]\n","# candidate_captions = [\"a photo of a cat\", \"a photo of a white cat\", \"a photo of a white cloud\"]\n","# candidate_captions = [\"a photo of a black cat\", \"a photo of a black cat with green eyes\",\n","#                       \"a picture of a car driving on the highway\", \"a black animal resting on the couch\"]\n","# candidate_captions = [\"cat\", \"dog\", \"car\"]\n","candidate_captions = [\n","    \"a photo of a black cat\",\n","    \"a photo of a black cat with green eyes\",\n","    \"a photo of a dark gray cat with blue eyes\",\n","    \"a photo of a black dog\",\n","]\n","inputs = modified_clip_processor(text=candidate_captions, images=image, return_tensors=\"pt\", padding=True)\n","outputs = modified_clip_model(**inputs)\n","\n","# Get the similarity scores\n","logits_per_image = outputs.logits_per_image\n","print(logits_per_image)\n","modified_logits = logits_per_image[0,:].tolist()\n","# print(modified_logits)\n","\n","probs = logits_per_image.softmax(dim=1)\n","best_caption_index = torch.argmax(probs).item()\n","print(f\"Best caption by CLIP: {candidate_captions[best_caption_index]}\")\n","\n","####################################################################################################\n","# CLIP 이미지 임베딩 추출\n","pixel_values = modified_clip_processor(images=image, return_tensors=\"pt\").pixel_values\n","image_features = modified_clip_model.get_image_features(pixel_values)  # [batch_size, embedding_dim]\n","\n","generated_ids = caption_model.generate(\n","    pixel_values,\n","    attention_mask=attention_mask,  # Explicitly pass the attention mask\n","    max_length=50,                 # Set a custom maximum length for the output\n","    pad_token_id=caption_tokenizer.pad_token_id  # Use the tokenizer's pad token ID\n",")\n","# generated_ids = caption_model.generate(pixel_values)\n","\n","generated_caption = caption_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","print(f\"Generated Caption: {generated_caption}\")\n","\n","\n","# CLIP의 Best Caption을 Caption Generation 모델의 입력에 포함\n","best_caption = candidate_captions[best_caption_index]\n","input_ids = caption_tokenizer(best_caption, return_tensors=\"pt\").input_ids\n","\n","generated_ids = caption_model.generate(\n","    pixel_values,\n","    input_ids=input_ids,  # CLIP의 Best Caption 반영\n","    attention_mask=attention_mask,\n","    max_length=50,\n","    pad_token_id=caption_tokenizer.pad_token_id\n",")\n","\n","generated_caption = caption_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","print(f\"Generated Caption with CLIP influence: {generated_caption}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPIG9pb_B1hK","executionInfo":{"status":"ok","timestamp":1733094817205,"user_tz":-540,"elapsed":9254,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"05448750-0089-4119-e773-8f60c64a35f9"},"execution_count":229,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[20.3167, 14.1507, 15.6849, 22.9832]], grad_fn=<TBackward0>)\n","Best caption by CLIP: a photo of a black dog\n","Generated Caption: a cat sitting on a sink next to a mirror \n","Generated Caption with CLIP influence: a photo of a black dog sitting on a sink \n"]}]},{"cell_type":"markdown","source":["### 결과 분석"],"metadata":{"id":"gVVs9gePkSml"}},{"cell_type":"code","source":["from torch.nn.functional import cosine_similarity\n","\n","# 후보 캡션 텍스트 임베딩 생성\n","text_inputs = modified_clip_processor(text=candidate_captions, return_tensors=\"pt\", padding=True)\n","text_features = modified_clip_model.get_text_features(**text_inputs)\n","\n","# 후보 캡션 간의 코사인 유사도 계산\n","similarities = cosine_similarity(text_features.unsqueeze(1), text_features.unsqueeze(0), dim=-1)\n","print(similarities)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IWoAvfBUaQAl","executionInfo":{"status":"ok","timestamp":1733061023945,"user_tz":-540,"elapsed":521,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"421f63fa-38a3-4672-98a6-234c67183de7"},"execution_count":225,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000, 0.8595, 0.7521, 0.8849],\n","        [0.8595, 1.0000, 0.7884, 0.7330],\n","        [0.7521, 0.7884, 1.0000, 0.6844],\n","        [0.8849, 0.7330, 0.6844, 1.0000]], grad_fn=<SumBackward1>)\n"]}]},{"cell_type":"code","source":["# 텍스트와 이미지 임베딩 추출\n","image_features = modified_clip_model.get_image_features(pixel_values)\n","text_features = modified_clip_model.get_text_features(**text_inputs)\n","\n","# 코사인 유사도 계산\n","similarities = cosine_similarity(image_features, text_features)\n","\n","# 유사도 분포 시각화\n","plt.bar(candidate_captions, similarities.squeeze().tolist())\n","plt.ylabel(\"Cosine Similarity\")\n","plt.xlabel(\"Candidate Captions\")\n","plt.title(\"Text-Image Similarity: Original vs Modified CLIP\")\n","plt.xticks(rotation=90)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":744},"id":"XrKtEAhVfDjl","executionInfo":{"status":"ok","timestamp":1733061051322,"user_tz":-540,"elapsed":2661,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"731642a7-ad7f-4857-de20-daf0444a2687"},"execution_count":228,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAkAAAALXCAYAAABl+f/NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACDc0lEQVR4nO3dd1gU1/s28HtBeu9FkSIoFgQDNowgSgQ1lmgUjRHFEmNBI2rUGMEWNUYRC9HYS4w1RmP0h10s2Av2CgYLooCIgorAvH/4st9sAENZHJa5P9fFJXtmmL13B+ThzDlnZIIgCCAiIiKSEDWxAxARERF9aCyAiIiISHJYABEREZHksAAiIiIiyWEBRERERJLDAoiIiIgkhwUQERERSQ4LICIiIpIcFkBEREQkOSyAiCopmUyGyZMnK+149+7dg0wmw+rVq+VtkydPhkwmU9pzFHBwcEC/fv2UftwP7fDhw5DJZDh8+HCpv7ao97siVJX3uqxatWqFVq1ayR8X977HxMTAw8MD2trakMlkyMjIQL9+/eDg4KDUPFI/H6qEBZBEyGSyEn2U5T/6ojx69AiTJ0/GxYsXS/w1rVq1QoMGDZTy/JXZsWPH0K5dO1SvXh3a2tqoWbMmOnbsiN9++03saBXm2rVrmDx5Mu7du/dBni8tLQ1jx45FnTp1oK2tDVNTUwQEBOCvv/76IM8vNQVFh0wmw/Tp04vcp3fv3pDJZNDX1//A6d59P/To0QM6OjqIjo7GunXroKen98Fz/Nvr168xb948NG3aFEZGRtDW1kbt2rUxfPhw3Lp1S75fwR8qqampxR6roFjfunWrvG316tUK/7//8/gpKSkV+tpUQTWxA9CHsW7dOoXHa9euxb59+wq1161bVynP9+jRI0yZMgUODg7w8PBQyjGrgi1btiAoKAgeHh4YOXIkTExMkJiYiCNHjmDZsmX44osv5Pu+evUK1aop70fU3t4er169goaGhtKOWZybN29CTe1/f19du3YNU6ZMQatWrZT+F3dRz92mTRs8ffoUISEh8PLyQkZGBtavX4+OHTtizJgx+Omnn0p0LB8fH7x69QqampqlzvEh3+/KQltbGxs2bMD333+v0J6VlYUdO3ZAW1u7wjMU9b6fOXMGL168wLRp0+Dv7y9vX7ZsGfLz8ys8U1FSU1MRGBiIc+fO4dNPP8UXX3wBfX193Lx5Exs3bsTSpUuRk5OjlOeaOnUqHB0d8fr1axw7dgyLFy/G7t27ceXKFejq6irlOVQRCyCJ+PLLLxUenzx5Evv27SvUThVr8uTJqFevHk6ePFnol+qTJ08UHiv7l0XBX4AVRRAEvH79Gjo6OtDS0qqw53mft2/f4vPPP8ezZ89w5MgRNG3aVL5t1KhR6N27N+bMmQMvLy8EBQUVe5zXr19DU1MTampqZX7PKvr9rozat2+Pbdu2IT4+Hu7u7vL2HTt2ICcnB4GBgTh48GCFZijqfS/42TI2NlZoF7M47devHy5cuICtW7eiW7duCtumTZuGiRMnKu252rVrBy8vLwDAwIEDYWZmhsjISOzYsQO9evVS2vOoGl4CI7n8/HxERUWhfv360NbWhpWVFQYPHoxnz57J94mIiICamhoOHDig8LVfffUVNDU1ER8fj8OHD6Nx48YAgJCQEHn3a1nGQshkMgwfPhxbtmxBvXr1oKOjg+bNm+Py5csAgF9++QXOzs7Q1tZGq1atCl1iOXr0KLp3746aNWtCS0sLdnZ2GDVqFF69elXouQqeQ1tbGw0aNMAff/xR5BiBkrxPxbl79y4aN25cZI+CpaVlodf+zzFABd3gt27dwpdffgkjIyNYWFhg0qRJEAQB9+/fR+fOnWFoaAhra2vMnTtX4XglHZOyatUqtG7dGpaWltDS0kK9evWwePHiQvs5ODjg008/xZ49e+Dl5QUdHR388ssv8m0F4yBWr16N7t27AwD8/PwULrf27dsX5ubmePv2baHjt23bFnXq1AEAJCUl4caNG+/NDQC///47rly5gvHjxysUPwCgrq6OX375BcbGxgrva8Glg40bN+L7779H9erVoauri8zMzGLHAEVHR8PJyQk6Ojpo0qQJjh49WqKxKP369YO+vj4ePnyILl26QF9fHxYWFhgzZgzy8vIUnmPOnDnw9vaGmZkZdHR04OnpqXB5o6Tevn0LU1NThISEFNqWmZkJbW1tjBkzRt62cOFC1K9fH7q6ujAxMYGXl1eJL882b94cjo6OhfZfv349AgMDYWpqWuTX/fzzz6hfvz60tLRga2uLYcOGISMjo9B+S5cuRa1atRTe93/79/veqlUr9O3bFwDQuHFjyGQy+fdmeX6+BUHA9OnTUaNGDejq6sLPzw9Xr14twbsEnDp1Crt27cKAAQMKFT8AoKWlhTlz5pToWGXRunVrAEBiYmKFPYcqYAFEcoMHD8bYsWPRokULzJ8/HyEhIVi/fj0CAgLkv6C+//57eHh4YMCAAXjx4gUAYM+ePVi2bBnCw8Ph7u6OunXrYurUqQDeFUbr1q3DunXr4OPjU6ZcR48exejRo9G3b19MnjwZ169fx6efforo6GgsWLAAQ4cOxdixY3HixAn0799f4Wu3bNmC7OxsDBkyBAsXLkRAQAAWLlyI4OBghf127dqFoKAgaGhoYObMmejatSsGDBiAc+fOlel9Ko69vT0OHDiABw8elOm9AICgoCDk5+dj1qxZaNq0KaZPn46oqCh88sknqF69On788Uc4OztjzJgxOHLkSKmPv3jxYtjb2+O7777D3LlzYWdnh6FDhyI6OrrQvjdv3kSvXr3wySefYP78+UVe7vTx8cGIESMAAN999538+6Fu3bro06cP0tLSsGfPHoWvefz4MQ4ePCjvoQwODi7R5dmdO3fK9y+KkZEROnfujBs3buDOnTsK26ZNm4Zdu3ZhzJgxmDFjRrGXvRYvXozhw4ejRo0amD17Nlq2bIkuXbqU+Jzm5eUhICAAZmZmmDNnDnx9fTF37lwsXbpUYb/58+ejUaNGmDp1KmbMmIFq1aqhe/fu2LVrV4mep4CGhgY+++wzbN++vdAlle3bt+PNmzfo2bMngHeXhEaMGIF69eohKioKU6ZMgYeHB06dOlXi5+vVqxc2btwIQRAAvLvUs3fvXoXLu/80efJkDBs2DLa2tpg7dy66deuGX375BW3btlX4eVqxYgUGDx4Ma2trzJ49Gy1atECnTp1w//799+aZOHEivvrqKwDvLgWtW7cOgwcPLnb/kv58h4eHY9KkSXB3d8dPP/0EJycntG3bFllZWf/5Hv35558AgD59+vznvhXh7t27AAAzMzNRnr/SEEiShg0bJvzz9B89elQAIKxfv15hv5iYmELtly9fFjQ1NYWBAwcKz549E6pXry54eXkJb9++le9z5swZAYCwatWqEmfy9fUV6tevr9AGQNDS0hISExPlbb/88osAQLC2thYyMzPl7RMmTBAAKOybnZ1d6HlmzpwpyGQy4e+//5a3ubm5CTVq1BBevHghbzt8+LAAQLC3t5e3leZ9KsqKFSsEAIKmpqbg5+cnTJo0STh69KiQl5dXaF8AQkREhPxxRESEAED46quv5G25ublCjRo1BJlMJsyaNUve/uzZM0FHR0fo27evvC0xMbHQOSk45j8V9Z4FBAQITk5OCm329vYCACEmJqbQ/vb29grPvWXLFgGAcOjQIYX98vLyhBo1aghBQUEK7ZGRkYJMJhMSEhIEQXj3vVGS/648PDwEIyOj9+4TGRkpABD+/PNPQRAE4dChQwIAwcnJqdBrL9hWkPvNmzeCmZmZ0LhxY4Xv99WrVwsABF9fX3lbUe933759BQDC1KlTFZ6nUaNGgqenp0Lbv7Pk5OQIDRo0EFq3bq3Q/u/3uih79uwRAAg7d+5UaG/fvr3Cee3cuXOhn8GSKHitP/30k3DlyhUBgHD06FFBEAQhOjpa0NfXF7KysoS+ffsKenp68q978uSJoKmpKbRt21bhZ2DRokUCAGHlypXy125paSl4eHgIb968ke+3dOnSEr3vq1atEgAIZ86cUcjdt2/fMv18F+Tu0KGDkJ+fL9/vu+++EwD85/n47LPPBADCs2fP3rtfgYKf06dPnxa7T8H36pYtW+RtBa97//79wtOnT4X79+8LGzduFMzMzAQdHR3hwYMHJXr+qoo9QATgXU+JkZERPvnkE6Smpso/PD09oa+vj0OHDsn3bdCgAaZMmYLly5cjICAAqampWLNmjVIH7P5TmzZtFLqpCy5tdOvWDQYGBoXaExIS5G06Ojryz7OyspCamgpvb28IgoALFy4AeDdg+/LlywgODlaYoeLr6ws3NzeFLKV5n4rSv39/xMTEoFWrVjh27BimTZuGli1bwsXFBXFxcSV6PwYOHCj/XF1dHV5eXhAEAQMGDJC3Gxsbo06dOgrvRUn98z17/vw5UlNT4evri4SEBDx//lxhX0dHRwQEBJT6OQqoqamhd+/e+PPPP+U9isC7Sybe3t5wdHQE8O4ylfD/exTe58WLFwrfE0Up2J6ZmanQ3rdvX4XXXpSzZ88iLS0NgwYNUvh+7927N0xMTP4zX4Gvv/5a4XHLli0Lnat/Znn27BmeP3+Oli1b4vz58yV+ngKtW7eGubk5Nm3apHDMffv2KYyFMjY2xoMHD3DmzJlSP0eB+vXro2HDhtiwYQMA4LfffkPnzp2LHGy7f/9+5OTk4JtvvlEYND9o0CAYGhrKe7vOnj2LJ0+e4Ouvv1bomevXrx+MjIzKnPXfSvrzXZA7NDRUYRmJb775pkTPU/C991/fq8ri7+8PCwsL2NnZoWfPntDX18cff/yB6tWrf5Dnr6xYABEA4Pbt23j+/DksLS1hYWGh8PHy5ctCA3THjh0Ld3d3nD59GhEREahXr16Jnufly5d4/Pix/OPp06f/+TU1a9ZUeFzwH56dnV2R7f+8Vp+UlIR+/frB1NRUPt7C19cXAOS/zP/++28AgLOzc6Hn/ndbad+nogQEBGDPnj3IyMjAkSNHMGzYMPz999/49NNPS/T1Rb0f2traMDc3L9ReknFJ/3b8+HH4+/tDT08PxsbGsLCwwHfffQcARRZA5RUcHIxXr17hjz/+APDustq5c+fKdHnAwMBAoZAqSsH2f//yKclrKe57pVq1aiWe3aatrQ0LCwuFNhMTk0Ln6q+//kKzZs3k0/gtLCywePHiQuegJKpVq4Zu3bphx44dePPmDQBg27ZtePv2rUIBNG7cOOjr66NJkyZwcXHBsGHDcPz48VI/3xdffIEtW7bgzp07iIuLK/byV8H7WTDWq4CmpiacnJzk2wv+dXFxUdhPQ0MDTk5Opc5XnJL+fBeXx8LCokSFsKGhIQD85/eqskRHR2Pfvn04dOgQrl27hoSEhHL94VJVcBYYAXg38M/S0hLr168vcvu//8NOSEjA7du3AUA+ILkk5syZgylTpsgf29vb/+faMOrq6qVqL+gpyMvLwyeffIL09HSMGzcOrq6u0NPTw8OHD9GvX78yTX8t7fv0Prq6umjZsiVatmwJc3NzTJkyBf/3f/8nH7BZnKJe93+9FyV19+5dtGnTBq6uroiMjISdnR00NTWxe/duzJs3r9B79l89JiVRr149eHp64tdff0VwcDB+/fVXaGpqokePHqU+Vt26dXHx4kUkJSUVKhQLXLp0Sf68/6SM11ISxZ2rfzp69Cg6deoEHx8f/Pzzz7CxsYGGhgZWrVpV5vWievbsiV9++QX/93//hy5dumDz5s1wdXVVmK1Vt25d3Lx5E3/99RdiYmLw+++/4+eff0Z4eLjCz+1/6dWrFyZMmIBBgwbBzMwMbdu2LVPmD02ZP9/v4+rqCuDd/50tW7ZUyjHfp0mTJvJZYPQ/LIAIAFCrVi3s378fLVq0+M9fBPn5+ejXrx8MDQ3xzTffYMaMGfj888/RtWtX+T7FrS4cHByMjz/+WP64In/pXL58Gbdu3cKaNWsUBsXu27dPYT97e3sAKDQotqi20rxPpVHwn1NycrLSjlkWO3fuxJs3b/Dnn38qFBD/dWnvv/zXatPBwcEICwtDcnIyfvvtN3To0KFUl5QKfPrpp9iwYQPWrl1baC0a4N2lhx07dsDV1bXIHr//8s/vFT8/P3l7bm4u7t27h4YNG5b6mEX5/fffoa2tjT179igsKbBq1aoyH9PHxwc2NjbYtGkTPv74Yxw8eLDIqdZ6enoICgpCUFAQcnJy0LVrV/zwww+YMGFCiaf116xZEy1atMDhw4cxZMiQYi+PF7yfN2/eVOjJycnJQWJionzNnoL9bt++LZ/BBLyb4ZaYmKhQxJVHSX++/5nnn7mfPn1aol7Xjh07YubMmfj1118/SAFEReMlMAIA9OjRA3l5eZg2bVqhbbm5uQpTUiMjIxEXF4elS5di2rRp8Pb2xpAhQxRWKS1YZfXfU1mdnJzg7+8v/2jRokWFvB7gf39p/7MXRBAEzJ8/X2E/W1tbNGjQAGvXrsXLly/l7bGxsYV6t0rzPhXl38sHFNi9ezeAwpcCPrSi3rPnz5+X6xcvUPz3Q4FevXpBJpNh5MiRSEhIKLQ+VUmnwX/++eeoV68eZs2ahbNnzypsy8/Px5AhQ/Ds2TNERESU6XV4eXnBzMwMy5YtQ25urrx9/fr1ZbrcWBx1dXXIZDKFqfH37t3D9u3by3xMNTU1fP7559i5cyfWrVuH3NzcQmshpaWlKTzW1NREvXr1IAjCf85w/Lfp06cjIiICoaGhxe7j7+8PTU1NLFiwQOF7bsWKFXj+/Dk6dOgA4N37bmFhgSVLlijMZFu9evV//syVRkl/vv39/aGhoYGFCxcq5I6KiirR8zRv3hyBgYFYvnx5kec0JydHYWkCqhjsASIA7wb8Dh48GDNnzsTFixfRtm1baGho4Pbt29iyZQvmz5+Pzz//HNevX8ekSZPQr18/dOzYEcC7/4Q8PDwwdOhQbN68GcC7v6SMjY2xZMkSGBgYQE9PD02bNlXKmJGScnV1Ra1atTBmzBg8fPgQhoaG+P3334v8RTVjxgx07twZLVq0QEhICJ49e4ZFixahQYMGCkVRSd+n4nTu3BmOjo7o2LEjatWqhaysLOzfvx87d+5E48aN5e+pWNq2bQtNTU107NgRgwcPxsuXL7Fs2TJYWlqWq3fKw8MD6urq+PHHH/H8+XNoaWnJ1xoC3l1aCAwMxJYtW2BsbCz/xVcgODgYsbGx/3lJT1NTE1u3bkWbNm3w8ccfK6wE/dtvv+H8+fMYPXq0fNp3aWlqamLy5MkIDQ1F69at0aNHD9y7dw+rV69GrVq1lHZftQ4dOiAyMhKBgYH44osv8OTJE0RHR8PZ2Vl+Ca8sgoKCsHDhQkRERMDNza3Q0gJt27aFtbU1WrRoASsrK1y/fh2LFi1Chw4dSj1g19fXVz7erjgWFhaYMGECpkyZgsDAQHTq1Ak3b97Ezz//jMaNG8sLYQ0NDUyfPh2DBw9G69atERQUhMTERKxatUqpY4BK+vNdsHbTzJkz8emnn6J9+/a4cOEC/u///q/QWLzirF27Fm3btkXXrl3RsWNHtGnTBnp6erh9+zY2btyI5OTkQmsBRUZGFhpMrqamJh+jR6UkzuQzEtu/p8EXWLp0qeDp6Sno6OgIBgYGgpubm/Dtt98Kjx49EnJzc4XGjRsLNWrUEDIyMhS+bv78+QIAYdOmTfK2HTt2CPXq1ROqVatWoinxxU2DHzZsmELbP6fc/lNR00CvXbsm+Pv7C/r6+oK5ubkwaNAgIT4+vsg8GzduFFxdXQUtLS2hQYMGwp9//il069ZNcHV1LdX79D4bNmwQevbsKdSqVUvQ0dERtLW1hXr16gkTJ05UmNJf8NqLmgb/76mw/55aXODf72dJp8H/+eefQsOGDQVtbW3BwcFB+PHHH4WVK1cWWmLA3t5e6NChQ5Gvs6ip2cuWLROcnJwEdXX1IqfEb968udA0/3++ltL8d/XkyRMhLCxMcHZ2FrS0tARjY2PB399fPvX9n4r6vvn3tn9nXbBggWBvby9oaWkJTZo0EY4fPy54enoKgYGB8n2KmwZf1Lkq6jysWLFCcHFxEbS0tARXV1dh1apVRe5XkmnwBfLz8wU7OzsBgDB9+vRC23/55RfBx8dHMDMzE7S0tIRatWoJY8eOFZ4/f/7e4xb3M/lvxb3+RYsWCa6uroKGhoZgZWUlDBkypMgp4j///LPg6OgoaGlpCV5eXsKRI0cEX19fpU2DL1CSn++8vDxhypQpgo2NjaCjoyO0atVKuHLlSqnOR3Z2tjBnzhyhcePGgr6+vqCpqSm4uLgIoaGhwp07d+T7FZz3oj7U1dUFQXj/NPh/v256RyYIpRwlSSQhHh4esLCwKDRuiJRvx44d6NKlC44cOaJy4yLy8/NhYWGBrl27YtmyZWLHIaIS4BggIrwbTPnPMR3Au7Vn4uPjFW5vQBVn2bJlcHJyUhgkXxm9fv260KW4tWvXIj09nd8rRCqEY4CIADx8+BD+/v748ssvYWtrixs3bmDJkiWwtrYutGgdKdfGjRtx6dIl7Nq1C/Pnz1faOJqKcvLkSYwaNQrdu3eHmZkZzp8/jxUrVqBBgwbye54RUeXHS2BEeDfT6auvvsLx48fx9OlT6OnpoU2bNpg1axZq1aoldrwqTSaTQV9fH0FBQViyZEmFrSiuLPfu3cOIESNw+vRppKenw9TUFO3bt8esWbMK3dCWiCovFkBEREQkORwDRERERJJTufuaRZKfn49Hjx7BwMCg0o9HICIioncEQcCLFy9ga2urcIPdorAAKsKjR48K3WiTiIiIVMP9+/dRo0aN9+7DAqgIBSue3r9/X37XXiIiIqrcMjMzYWdnV6KVy1kAFaHgspehoSELICIiIhVTkuErHARNREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeRUEzsAERGRWBzG7xI7giTdm9VB7AjsASIiIiLpYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeRUEzuAFDmM3yV2BMm6N6uD2BGIiKgSqBQ9QNHR0XBwcIC2tjaaNm2K06dPF7vvsmXL0LJlS5iYmMDExAT+/v6F9hcEAeHh4bCxsYGOjg78/f1x+/btin4ZREREpCJEL4A2bdqEsLAwRERE4Pz583B3d0dAQACePHlS5P6HDx9Gr169cOjQIZw4cQJ2dnZo27YtHj58KN9n9uzZWLBgAZYsWYJTp05BT08PAQEBeP369Yd6WURERFSJiV4ARUZGYtCgQQgJCUG9evWwZMkS6OrqYuXKlUXuv379egwdOhQeHh5wdXXF8uXLkZ+fjwMHDgB41/sTFRWF77//Hp07d0bDhg2xdu1aPHr0CNu3b/+Ar4yIiIgqK1ELoJycHJw7dw7+/v7yNjU1Nfj7++PEiRMlOkZ2djbevn0LU1NTAEBiYiIeP36scEwjIyM0bdq02GO+efMGmZmZCh9ERERUdYlaAKWmpiIvLw9WVlYK7VZWVnj8+HGJjjFu3DjY2trKC56CryvNMWfOnAkjIyP5h52dXWlfChEREakQ0S+BlcesWbOwceNG/PHHH9DW1i7zcSZMmIDnz5/LP+7fv6/ElERERFTZiDoN3tzcHOrq6khJSVFoT0lJgbW19Xu/ds6cOZg1axb279+Phg0bytsLvi4lJQU2NjYKx/Tw8CjyWFpaWtDS0irjqyAiIiJVI2oPkKamJjw9PeUDmAHIBzQ3b9682K+bPXs2pk2bhpiYGHh5eSlsc3R0hLW1tcIxMzMzcerUqfcek4iIiKRD9IUQw8LC0LdvX3h5eaFJkyaIiopCVlYWQkJCAADBwcGoXr06Zs6cCQD48ccfER4ejt9++w0ODg7ycT36+vrQ19eHTCbDN998g+nTp8PFxQWOjo6YNGkSbG1t0aVLF7FeJhEREVUiohdAQUFBePr0KcLDw/H48WN4eHggJiZGPog5KSkJamr/66havHgxcnJy8PnnnyscJyIiApMnTwYAfPvtt8jKysJXX32FjIwMfPzxx4iJiSnXOCEiIiKqOmSCIAhih6hsMjMzYWRkhOfPn8PQ0FDpx+etMMTDW2EQ0T/x/2NxVNT/xaX5/a3Ss8CIiIiIyoIFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUlOqQughISEishBRERE9MFUK+0XODs7w9fXFwMGDMDnn38ObW3tishFRFRpOIzfJXYEybo3q4PYEaiKKnUP0Pnz59GwYUOEhYXB2toagwcPxunTpysiGxEREVGFKHUB5OHhgfnz5+PRo0dYuXIlkpOT8fHHH6NBgwaIjIzE06dPKyInERERkdKUeRB0tWrV0LVrV2zZsgU//vgj7ty5gzFjxsDOzg7BwcFITk5WZk4iIiIipSlzAXT27FkMHToUNjY2iIyMxJgxY3D37l3s27cPjx49QufOnZWZk4iIiEhpSj0IOjIyEqtWrcLNmzfRvn17rF27Fu3bt4ea2rtaytHREatXr4aDg4OysxIREREpRakLoMWLF6N///7o168fbGxsitzH0tISK1asKHc4IiIioopQ6ktg+/btw7hx4woVP4IgICkpCQCgqamJvn37luh40dHRcHBwgLa2Npo2bfreGWVXr15Ft27d4ODgAJlMhqioqEL7TJ48GTKZTOHD1dW15C+QiIiIqrxSF0C1atVCampqofb09HQ4OjqW6libNm1CWFgYIiIicP78ebi7uyMgIABPnjwpcv/s7Gw4OTlh1qxZsLa2Lva49evXR3Jysvzj2LFjpcpFREREVVupCyBBEIpsf/nyZakXRYyMjMSgQYMQEhKCevXqYcmSJdDV1cXKlSuL3L9x48b46aef0LNnT2hpaRV73GrVqsHa2lr+YW5u/t4cb968QWZmpsIHERERVV0lHgMUFhYGAJDJZAgPD4eurq58W15eHk6dOgUPD48SP3FOTg7OnTuHCRMmyNvU1NTg7++PEydOlPg4Rbl9+zZsbW2hra2N5s2bY+bMmahZs2ax+8+cORNTpkwp13MSERGR6ihxAXThwgUA73qALl++DE1NTfk2TU1NuLu7Y8yYMSV+4tTUVOTl5cHKykqh3crKCjdu3Cjxcf6tadOmWL16NerUqYPk5GRMmTIFLVu2xJUrV2BgYFDk10yYMEFe4AFAZmYm7OzsypyBiIiIKrcSF0CHDh0CAISEhGD+/PkwNDSssFDl0a5dO/nnDRs2RNOmTWFvb4/NmzdjwIABRX6NlpbWey+pERERUdVS6mnwq1atUsoTm5ubQ11dHSkpKQrtKSkp7x3gXFrGxsaoXbs27ty5o7RjEhERkWorUQHUtWtXrF69GoaGhujatet79922bVuJnlhTUxOenp44cOAAunTpAgDIz8/HgQMHMHz48BIdoyRevnyJu3fvok+fPko7JhEREam2EhVARkZGkMlk8s+VJSwsDH379oWXlxeaNGmCqKgoZGVlISQkBAAQHByM6tWrY+bMmQDeDZy+du2a/POHDx/i4sWL0NfXh7OzMwBgzJgx6NixI+zt7fHo0SNERERAXV0dvXr1UlpuIiIiUm0lKoAKLnsJgoApU6bAwsICOjo65X7yoKAgPH36FOHh4Xj8+DE8PDwQExMjHxidlJQkv8UGADx69AiNGjWSP54zZw7mzJkDX19fHD58GADw4MED9OrVC2lpabCwsMDHH3+MkydPwsLCotx5iYiIqGoo1RggQRDg7OyMq1evwsXFRSkBhg8fXuwlr4KipoCDg0Ox6xAV2Lhxo1JyERERUdVVqoUQ1dTU4OLigrS0tIrKQ0RERFThSr0S9KxZszB27FhcuXKlIvIQERERVbhST4MPDg5GdnY23N3doampWWgsUHp6utLCEREREVWEUhdARd2BnYiIiEiVlLoA6tu3b0XkICIiIvpgSl0A/dPr16+Rk5Oj0FZZb5FBVNEcxu8SO4Jk3ZvVQewIRKRiSj0IOisrC8OHD4elpSX09PRgYmKi8EFERERU2ZW6APr2229x8OBBLF68GFpaWli+fDmmTJkCW1tbrF27tiIyEhERESlVqS+B7dy5E2vXrkWrVq0QEhKCli1bwtnZGfb29li/fj169+5dETmJiIiIlKbUPUDp6elwcnIC8G68T8G0948//hhHjhxRbjoiIiKiClDqAsjJyQmJiYkAAFdXV2zevBnAu54hY2NjpYYjIiIiqgilLoBCQkIQHx8PABg/fjyio6Ohra2NUaNGYezYsUoPSERERKRspR4DNGrUKPnn/v7+uHHjBs6dOwdnZ2c0bNhQqeGIiIiIKkK51gECAHt7e9jb2ysjCxEREdEHUaICaMGCBSU+4IgRI8ochoiIiOhDKFEBNG/evBIdTCaTsQAiIiKiSq9EBVDBrC8iIiKiqqDUs8CIiIiIVF2JeoDCwsIwbdo06OnpISws7L37RkZGKiUYERERUUUpUQF04cIFvH37Vv55cWQymXJSEREREVWgEhVAhw4dKvJzIiIiIlXEMUBEREQkOaVeCPH169dYuHAhDh06hCdPniA/P19h+/nz55UWjoiIiKgilLoAGjBgAPbu3YvPP/8cTZo04bgfIiIiUjmlLoD++usv7N69Gy1atKiIPEREREQVrtRjgKpXrw4DA4OKyEJERET0QZS6AJo7dy7GjRuHv//+uyLyEBEREVW4Ul8C8/LywuvXr+Hk5ARdXV1oaGgobE9PT1daOCIiIqKKUOoCqFevXnj48CFmzJgBKysrDoImIiIilVPqAiguLg4nTpyAu7t7ReQhIiIiqnClHgPk6uqKV69eVUQWIiIiog+i1AXQrFmzMHr0aBw+fBhpaWnIzMxU+CAiIiKq7Ep9CSwwMBAA0KZNG4V2QRAgk8mQl5ennGREREREFaTUBRBvhkpERESqrtQFkK+vb0XkICIiIvpgSlQAXbp0CQ0aNICamhouXbr03n0bNmyolGBEREREFaVEBZCHhwceP34MS0tLeHh4QCaTQRCEQvtxDBARERGpghIVQImJibCwsJB/TkRERKTKSlQA2dvbF/k5ERERkSoq8TpAt27dwunTpxXaDhw4AD8/PzRp0gQzZsxQejgiIiKiilDiAmjcuHH466+/5I8TExPRsWNHaGpqonnz5pg5cyaioqIqIiMRERGRUpV4GvzZs2fx7bffyh+vX78etWvXxp49ewC8m/21cOFCfPPNN0oPSURERKRMJe4BSk1NRY0aNeSPDx06hI4dO8oft2rVCvfu3VNqOCIiIqKKUOICyNTUFMnJyQCA/Px8nD17Fs2aNZNvz8nJKXJqPBEREVFlU+ICqFWrVpg2bRru37+PqKgo5Ofno1WrVvLt165dg4ODQwVEJCIiIlKuEo8B+uGHH/DJJ5/A3t4e6urqWLBgAfT09OTb161bh9atW1dISCIiIiJlKnEB5ODggOvXr+Pq1auwsLCAra2twvYpU6YojBEiIiIiqqxKdTPUatWqwd3dvchtxbUTERERVTYlHgNEREREVFWwACIiIiLJYQFEREREksMCiIiIiCSnTAXQ0aNH8eWXX6J58+Z4+PAhgHfT4I8dO6bUcEREREQVodQF0O+//46AgADo6OjgwoULePPmDQDg+fPnvCM8ERERqYRSF0DTp0/HkiVLsGzZMmhoaMjbW7RogfPnzys1HBEREVFFKHUBdPPmTfj4+BRqNzIyQkZGhjIyEREREVWoUhdA1tbWuHPnTqH2Y8eOwcnJSSmhiIiIiCpSqQugQYMGYeTIkTh16hRkMhkePXqE9evXY8yYMRgyZEhFZCQiIiJSqlLdCgMAxo8fj/z8fLRp0wbZ2dnw8fGBlpYWxowZg9DQ0IrISERERKRUpS6AZDIZJk6ciLFjx+LOnTt4+fIl6tWrB319/YrIR0RERKR0pS6ACmhqaqJevXrKzEJERET0QZS6AMrKysKsWbNw4MABPHnyBPn5+QrbExISlBaOiIiIqCKUugAaOHAgYmNj0adPH9jY2EAmk1VELiIiIqIKU+oC6P/+7/+wa9cutGjRoiLyEBEREVW4Uk+DNzExgampaUVkISIiIvogSl0ATZs2DeHh4cjOzlZKgOjoaDg4OEBbWxtNmzbF6dOni9336tWr6NatGxwcHCCTyRAVFVXuYxIREZH0lLoAmjt3Lvbs2QMrKyu4ubnho48+UvgojU2bNiEsLAwRERE4f/483N3dERAQgCdPnhS5f3Z2NpycnDBr1ixYW1sr5ZhEREQkPaUeA9SlSxelPXlkZCQGDRqEkJAQAMCSJUuwa9curFy5EuPHjy+0f+PGjdG4cWMAKHJ7WY5JRERE0lPqAigiIkIpT5yTk4Nz585hwoQJ8jY1NTX4+/vjxIkTH/SYb968wZs3b+SPMzMzy/T8REREpBpKfQlMWVJTU5GXlwcrKyuFdisrKzx+/PiDHnPmzJkwMjKSf9jZ2ZXp+YmIiEg1lKgAMjU1RWpqKoD/zQIr7kMVTZgwAc+fP5d/3L9/X+xIREREVIFKdAls3rx5MDAwkH+ujMUPzc3Noa6ujpSUFIX2lJSUYgc4V9QxtbS0oKWlVabnJCIiItVTogKob9++8s/79eunlCfW1NSEp6cnDhw4IB9YnZ+fjwMHDmD48OGV5phERERU9ZR6EPT58+ehoaEBNzc3AMCOHTuwatUq1KtXD5MnT4ampmaJjxUWFoa+ffvCy8sLTZo0QVRUFLKysuQzuIKDg1G9enXMnDkTwLtBzteuXZN//vDhQ1y8eBH6+vpwdnYu0TGJiIiISl0ADR48GOPHj4ebmxsSEhIQFBSErl27YsuWLcjOzi52ccKiBAUF4enTpwgPD8fjx4/h4eGBmJgY+SDmpKQkqKn9b5jSo0eP0KhRI/njOXPmYM6cOfD19cXhw4dLdEwiIiKiUhdAt27dgoeHBwBgy5Yt8PX1xW+//Ybjx4+jZ8+epSqAAGD48OHFXp4qKGoKODg4QBCEch2TiIiIqNTT4AVBQH5+PgBg//79aN++PQDAzs5OPlOMiIiIqDIrdQHk5eWF6dOnY926dYiNjUWHDh0AAImJibzMRERERCqh1AVQVFQUzp8/j+HDh2PixInywcdbt26Ft7e30gMSERERKVupxwA1bNgQly9fLtT+008/QV1dXSmhiIiIiCpSqQugAufOncP169cBAPXq1Sv1neCJiIiIxFLqAujJkycICgpCbGwsjI2NAQAZGRnw8/PDxo0bYWFhoeyMREREREpV6jFAoaGhePnyJa5evYr09HSkp6fjypUryMzMxIgRIyoiIxEREZFSlboHKCYmBvv370fdunXlbfXq1UN0dDTatm2r1HBEREREFaHUPUD5+fnQ0NAo1K6hoSFfH4iIiIioMit1AdS6dWuMHDkSjx49krc9fPgQo0aNQps2bZQajoiIiKgilLoAWrRoETIzM+Hg4IBatWqhVq1acHR0RGZmJhYuXFgRGYmIiIiUqtRjgOzs7HD+/Hns378fN27cAADUrVsX/v7+Sg9HREREVBHKtA6QTCbDJ598gk8++UTZeYiIiIgqXIkvgR08eBD16tVDZmZmoW3Pnz9H/fr1cfToUaWGIyIiIqoIJS6AoqKiMGjQIBgaGhbaZmRkhMGDByMyMlKp4YiIiIgqQokLoPj4eAQGBha7vW3btjh37pxSQhERERFVpBIXQCkpKUWu/1OgWrVqePr0qVJCEREREVWkEhdA1atXx5UrV4rdfunSJdjY2CglFBEREVFFKnEB1L59e0yaNAmvX78utO3Vq1eIiIjAp59+qtRwRERERBWhxNPgv//+e2zbtg21a9fG8OHDUadOHQDAjRs3EB0djby8PEycOLHCghIREREpS4kLICsrK8TFxWHIkCGYMGECBEEA8G5NoICAAERHR8PKyqrCghIREREpS6kWQrS3t8fu3bvx7Nkz3LlzB4IgwMXFBSYmJhWVj4iIiEjpyrQStImJCRo3bqzsLEREREQfRKlvhkpERESk6lgAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDgsgIiIikhwWQERERCQ5LICIiIhIclgAERERkeSwACIiIiLJqRQFUHR0NBwcHKCtrY2mTZvi9OnT791/y5YtcHV1hba2Ntzc3LB7926F7f369YNMJlP4CAwMrMiXQERERCpE9AJo06ZNCAsLQ0REBM6fPw93d3cEBATgyZMnRe4fFxeHXr16YcCAAbhw4QK6dOmCLl264MqVKwr7BQYGIjk5Wf6xYcOGD/FyiIiISAWIXgBFRkZi0KBBCAkJQb169bBkyRLo6upi5cqVRe4/f/58BAYGYuzYsahbty6mTZuGjz76CIsWLVLYT0tLC9bW1vIPExOTYjO8efMGmZmZCh9ERERUdYlaAOXk5ODcuXPw9/eXt6mpqcHf3x8nTpwo8mtOnDihsD8ABAQEFNr/8OHDsLS0RJ06dTBkyBCkpaUVm2PmzJkwMjKSf9jZ2ZXjVREREVFlJ2oBlJqairy8PFhZWSm0W1lZ4fHjx0V+zePHj/9z/8DAQKxduxYHDhzAjz/+iNjYWLRr1w55eXlFHnPChAl4/vy5/OP+/fvlfGVERERUmVUTO0BF6Nmzp/xzNzc3NGzYELVq1cLhw4fRpk2bQvtraWlBS0vrQ0YkIiIiEYnaA2Rubg51dXWkpKQotKekpMDa2rrIr7G2ti7V/gDg5OQEc3Nz3Llzp/yhiYiISOWJWgBpamrC09MTBw4ckLfl5+fjwIEDaN68eZFf07x5c4X9AWDfvn3F7g8ADx48QFpaGmxsbJQTnIiIiFSa6LPAwsLCsGzZMqxZswbXr1/HkCFDkJWVhZCQEABAcHAwJkyYIN9/5MiRiImJwdy5c3Hjxg1MnjwZZ8+exfDhwwEAL1++xNixY3Hy5Encu3cPBw4cQOfOneHs7IyAgABRXiMRERFVLqKPAQoKCsLTp08RHh6Ox48fw8PDAzExMfKBzklJSVBT+1+d5u3tjd9++w3ff/89vvvuO7i4uGD79u1o0KABAEBdXR2XLl3CmjVrkJGRAVtbW7Rt2xbTpk3jOB8iIiICUAkKIAAYPny4vAfn3w4fPlyorXv37ujevXuR++vo6GDPnj3KjEdERERVjOiXwIiIiIg+NBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSHBZAREREJDksgIiIiEhyWAARERGR5LAAIiIiIslhAURERESSwwKIiIiIJIcFEBEREUkOCyAiIiKSnEpRAEVHR8PBwQHa2tpo2rQpTp8+/d79t2zZAldXV2hra8PNzQ27d+9W2C4IAsLDw2FjYwMdHR34+/vj9u3bFfkSiIiISIWIXgBt2rQJYWFhiIiIwPnz5+Hu7o6AgAA8efKkyP3j4uLQq1cvDBgwABcuXECXLl3QpUsXXLlyRb7P7NmzsWDBAixZsgSnTp2Cnp4eAgIC8Pr16w/1soiIiKgSE70AioyMxKBBgxASEoJ69ephyZIl0NXVxcqVK4vcf/78+QgMDMTYsWNRt25dTJs2DR999BEWLVoE4F3vT1RUFL7//nt07twZDRs2xNq1a/Ho0SNs3779A74yIiIiqqyqifnkOTk5OHfuHCZMmCBvU1NTg7+/P06cOFHk15w4cQJhYWEKbQEBAfLiJjExEY8fP4a/v798u5GREZo2bYoTJ06gZ8+ehY755s0bvHnzRv74+fPnAIDMzMwyv7b3yX+TXSHHpf9WUecU4HkVU0WeV4DnVkw8t1VTRZ3XguMKgvCf+4paAKWmpiIvLw9WVlYK7VZWVrhx40aRX/P48eMi93/8+LF8e0Fbcfv828yZMzFlypRC7XZ2diV7IaQyjKLETkAVgee16uK5rZoq+ry+ePECRkZG791H1AKospgwYYJCr1J+fj7S09NhZmYGmUwmYrLKJTMzE3Z2drh//z4MDQ3FjkNKxHNbNfG8Vl08t0UTBAEvXryAra3tf+4ragFkbm4OdXV1pKSkKLSnpKTA2tq6yK+xtrZ+7/4F/6akpMDGxkZhHw8PjyKPqaWlBS0tLYU2Y2Pj0rwUSTE0NOQPXBXFc1s18bxWXTy3hf1Xz08BUQdBa2pqwtPTEwcOHJC35efn48CBA2jevHmRX9O8eXOF/QFg37598v0dHR1hbW2tsE9mZiZOnTpV7DGJiIhIWkS/BBYWFoa+ffvCy8sLTZo0QVRUFLKyshASEgIACA4ORvXq1TFz5kwAwMiRI+Hr64u5c+eiQ4cO2LhxI86ePYulS5cCAGQyGb755htMnz4dLi4ucHR0xKRJk2Bra4suXbqI9TKJiIioEhG9AAoKCsLTp08RHh6Ox48fw8PDAzExMfJBzElJSVBT+19Hlbe3N3777Td8//33+O677+Di4oLt27ejQYMG8n2+/fZbZGVl4auvvkJGRgY+/vhjxMTEQFtb+4O/vqpES0sLERERhS4Xkurjua2aeF6rLp7b8pMJJZkrRkRERFSFiL4QIhEREdGHxgKIiIiIJIcFEBEREUkOCyAiIiKSHBZAVKwjR44gNze3UHtubi6OHDkiQiKqSBkZGWJHoArA80pUNBZAVCw/Pz+kp6cXan/+/Dn8/PxESETK8uOPP2LTpk3yxz169ICZmRmqV6+O+Ph4EZNRefC8EpUcp8FTsdTU1JCSkgILCwuF9lu3bsHLy6vC79JMFcfR0RHr16+Ht7c39u3bhx49emDTpk3YvHkzkpKSsHfvXrEjUhnwvFZdjRo1KvLelDKZDNra2nB2dka/fv34x2kpiL4QIlU+Xbt2BfDuB6tfv34KC23l5eXh0qVL8Pb2FiseKcHjx49hZ2cHAPjrr7/Qo0cPtG3bFg4ODmjatKnI6aiseF6rrsDAQCxevBhubm5o0qQJAODMmTO4dOkS+vXrh2vXrsHf3x/btm1D586dRU6rGlgAUSEFN5ITBAEGBgbQ0dGRb9PU1ESzZs0waNAgseKREpiYmOD+/fuws7NDTEwMpk+fDuDdOc/LyxM5HZUVz2vVlZqaitGjR2PSpEkK7dOnT8fff/+NvXv3IiIiAtOmTWMBVEIsgKiQVatWAQAcHBwwZswY6OnpiZyIlK1r16744osv4OLigrS0NLRr1w4AcOHCBTg7O4ucjsqK57Xq2rx5M86dO1eovWfPnvD09MSyZcvQq1cvREZGipBONbEAomJFRESIHYEqyLx58+Dg4ID79+9j9uzZ0NfXBwAkJydj6NChIqejsuJ5rbq0tbURFxdXqJCNi4uT3+cyPz+f97wsBQ6CpvfaunWrfABlTk6Owrbz58+LlIqISFqmT5+OGTNmYNCgQWjcuDGAd2OAli9fju+++w4TJ07EvHnzsHv3buzbt0/ktKqB0+CpWAsWLEBISAisrKxw4cIFNGnSBGZmZkhISJB3rZPqWrduHT7++GPY2tri77//BgBERUVhx44dIiej8uB5rZq+//57LFu2DKdPn8aIESMwYsQInD59GsuWLcPEiRMBAF9//TV27twpclLVwQKIivXzzz9j6dKlWLhwITQ1NfHtt99i3759GDFiBJ4/fy52PCqHxYsXIywsDO3atUNGRoZ8gKyxsTGioqLEDUdlxvNatfXu3RsnTpxAeno60tPTceLECXzxxRfy7To6OrwEVgosgKhYSUlJ8unuOjo6ePHiBQCgT58+2LBhg5jRqJwWLlwo/8tRXV1d3u7l5YXLly+LmIzKg+e16jt37hx+/fVX/Prrr7hw4YLYcVQaB0FTsaytrZGeng57e3vUrFkTJ0+ehLu7OxITE8GhY6otMTERjRo1KtSupaWFrKwsERKRMvC8Vl1PnjxBz549cfjwYRgbGwN4d5sTPz8/bNy4sdCCtfTf2ANExWrdujX+/PNPAEBISAhGjRqFTz75BEFBQfjss89ETkfl4ejoiIsXLxZqj4mJQd26dT98IFIKnteqKzQ0FC9evMDVq1fll8CuXLmCzMxMjBgxQux4Kok9QFSspUuXIj8/HwAwbNgwmJmZIS4uDp06dcLgwYNFTkflERYWhmHDhuH169cQBAGnT5/Ghg0bMHPmTCxfvlzseFRGPK9VV0xMDPbv369QyNarVw/R0dFo27atiMlUF6fBE0nU+vXrMXnyZNy9excAYGtriylTpmDAgAEiJ6Py4HmtmgwMDHD06FF4eHgotF+4cAG+vr68N2MZsACiYq1atQr6+vro3r27QvuWLVuQnZ2Nvn37ipSMlCk7OxsvX76EpaWl2FFIiXheq5bOnTsjIyMDGzZsgK2tLQDg4cOH6N27N0xMTPDHH3+InFD1cAwQFWvmzJkwNzcv1G5paYkZM2aIkIiUJSIiQr5GjK6uLn9JVhE8r1XXokWLkJmZCQcHB9SqVQu1atWCo6MjMjMzsXDhQrHjqST2AFGxtLW1cePGDTg4OCi037t3D3Xr1sWrV6/ECUbl5uHhgStXrsDX1xcDBgxAt27doKWlJXYsKiee16pNEATs378fN27cAADUrVsX/v7+IqdSXewBomJZWlri0qVLhdrj4+NhZmYmQiJSlosXL+LMmTOoX78+Ro4cCWtrawwZMgRnzpwROxqVA89r1SaTyfDJJ58gNDQUoaGhLH7KiT1AVKxx48Zh06ZNWLVqFXx8fAAAsbGx6N+/Pz7//HPMmTNH5ISkDG/fvsXOnTuxatUq7NmzB66urhgwYAD69esHIyMjseNRGfG8qr4FCxaUeF9OhS89FkBUrJycHPTp0wdbtmxBtWrvVkzIz89HcHAwlixZAk1NTZETkjLk5OTgjz/+wMqVK3Hw4EF4e3vj0aNHSElJwbJlyxAUFCR2RCoDnlfV5+joqPD46dOnyM7OVlgIsWCsV0JCgggJVRsLIPpPt2/fxsWLF6GjowM3NzfY29uLHYmU4Ny5c1i1ahU2bNgALS0tBAcHY+DAgXB2dgbw7rYK06dPR0pKishJqTR4Xqum3377DT///DNWrFiBOnXqAABu3ryJQYMGYfDgwejdu7fICVUPCyAiCXJzc8ONGzfQtm1bDBo0CB07dlS4dxQApKamwtLSUr4YJlV+PK9VV61atbB169ZCtzo5d+4cPv/8cyQmJoqUTHVxJWgiCerRowf69++P6tWrF7uPubk5f0mqGJ7Xqis5ORm5ubmF2vPy8tibV0bsASKSsJycHCQmJqJWrVrycV6k+nheq56OHTvi4cOHWL58OT766CMA73p/vvrqK1SvXl1+30YqOU6DJ5KgV69eYcCAAdDV1UX9+vWRlJQE4N0NF2fNmiVyOiornteqa+XKlbC2toaXlxe0tLSgpaWFJk2awMrKivd5KyMWQFSst2/fFrstNTX1AyYhZRs/fjzi4+Nx+PBhaGtry9v9/f2xadMmEZNRefC8Vl0WFhbYvXs3bt68iS1btmDLli24fv06du/ezRW/y4h9o1Ssnj17YuvWrZDJZArtKSkpaNOmDa5cuSJSMiqv7du3Y9OmTWjWrJnC+a1fv778Jpqkenheqz4XFxe4uLiIHaNKYA8QFSspKQkDBw5UaHv8+DFatWoFV1dXkVKRMjx9+rTIvxqzsrIKFbykOnheiUqOBRAVa/fu3YiLi0NYWBgA4NGjR/D19YWbmxs2b94scjoqDy8vL+zatUv+uOCX4/Lly9G8eXOxYlE58bwSlRwvgVGxLCwssHfvXnz88ccAgL/++gsfffQR1q9fDzU11s6qbMaMGWjXrh2uXbuG3NxczJ8/H9euXUNcXBxiY2PFjkdlxPNKVHL8LUbvZWdnh3379mH9+vVo0qQJNmzYUGhhNVI9H3/8MS5evIjc3Fy4ublh7969sLS0xIkTJ+Dp6Sl2PCojnteqi5NSlI/rAJECExOTIscKZGdnQ0tLS6H4SU9P/5DRiIgkq1u3bpyUomS8BEYKoqKixI5ARET/UjApZcWKFfK2x48fw8/PD/Xr1xcxmepiDxAREVEl9/TpU/j4+KBdu3aIjIzEo0eP4OfnB3d3d2zcuJHjMsuAPUBUrN27d0NdXR0BAQEK7Xv37kVeXh7atWsnUjIiImnhpBTl47tGxRo/fjzy8vIKtefn52P8+PEiJCIiki5OSlEuXgKjYuno6OD69etwcHBQaL937x7q16+PrKwscYIR0XvduXMHd+/ehY+PD3R0dCAIAhdCVEGclFKxeAmMimVkZISEhIRCBdCdO3egp6cnTihSiqysLMyaNQsHDhzAkydPkJ+fr7A9ISFBpGRUHmlpaQgKCsLBgwchk8lw+/ZtODk5YcCAATAxMcHcuXPFjkilwEkpFYsFEBWrc+fO+Oabb/DHH3+gVq1aAN4VP6NHj0anTp1ETkflMXDgQMTGxqJPnz6wsbFh70AVMWrUKFSrVg1JSUmoW7euvD0oKAhhYWEsgFRM3759xY5QpfESGBXr+fPnCAwMxNmzZ1GjRg0AwIMHD9CyZUts27YNxsbG4gakMjM2NsauXbvQokULsaOQEllbW2PPnj1wd3eHgYEB4uPj4eTkhISEBDRs2BAvX74UOyKVESelKB97gKhYRkZGiIuLw759+xAfHw8dHR00bNgQPj4+YkejcjIxMYGpqanYMUjJsrKyoKurW6g9PT0dWlpaIiQiZRk/fjxmzZpVqL1gUgoLoNJjDxCRBP3666/YsWMH1qxZU+QvTFJN7du3h6enJ6ZNmwYDAwNcunQJ9vb26NmzJ/Lz87F161axI1IZcVKK8rEHiN4rKysLsbGxSEpKQk5OjsK2ESNGiJSKymvu3Lm4e/curKys4ODgAA0NDYXt58+fFykZlcfs2bPRpk0bnD17Fjk5Ofj2229x9epVpKen4/jx42LHo3LgpBTlYwFExbpw4QLat2+P7OxsZGVlwdTUFKmpqdDV1YWlpSULIBXWpUsXsSNQBWjQoAFu3bqFRYsWwcDAAC9fvkTXrl0xbNgw2NjYiB2PyoGTUpSPl8CoWK1atULt2rWxZMkSGBkZIT4+HhoaGvjyyy8xcuRIdO3aVeyIRESSwEkpyscCiIplbGyMU6dOoU6dOjA2NsaJEydQt25dnDp1Cn379sWNGzfEjkjlkJGRga1bt+Lu3bsYO3YsTE1Ncf78eVhZWaF69epix6MyOHLkyHu3cwKDahMEgZNSlIiXwKhYGhoa8nvMWFpaytcWMTIywv3790VOR+Vx6dIl+Pv7w8jICPfu3cOgQYNgamqKbdu2ISkpCWvXrhU7IpVBq1atCrX9c42nom5tQ6pDJpOhbdu2aNu2rdhRqgQWQFSsRo0a4cyZM3BxcYGvry/Cw8ORmpqKdevWoUGDBmLHo3IICwtDv379MHv2bBgYGMjb27dvjy+++ELEZFQez549U3j89u1bXLhwAZMmTcIPP/wgUipSFk5KUS5eAqNinT17Fi9evICfnx+ePHmC4OBgxMXFwcXFBStXroS7u7vYEamMjIyMcP78edSqVUthwby///4bderUwevXr8WOSEoUGxuLsLAwnDt3TuwoVEb/NSmFt68pPfYAUbG8vLzkn1taWiImJkbENKRMWlpayMzMLNR+69YtWFhYiJCIKpKVlRVu3rwpdgwqh1GjRqFjx47ySSknT55UmJRCpcceICIJGjhwINLS0rB582aYmpri0qVLUFdXR5cuXeDj48ObMKqoS5cuKTwWBAHJycmYNWsWcnNzcezYMZGSUXlxUorysQeIFDRq1KjEN8bkYnmqa+7cufj8889haWmJV69ewdfXF48fP0bz5s05VkSFeXh4QCaT4d9/1zZr1gwrV64UKRUpAyelKB8LIFLABfKkwcjICPv27cOxY8dw6dIlvHz5Eh999BH8/f3FjkblkJiYqPBYTU0NFhYW0NbWFikRKQsnpSgfL4ERSdzr16+hpaVV4p4/IvrwOClF+VgA0X86e/Ysrl+/DgCoV68ePD09RU5E5ZWfn48ffvgBS5YsQUpKCm7dugUnJydMmjQJDg4OGDBggNgRqYQWLFhQ4n05VZrof1gAUbEePHiAXr164fjx4/Jl1jMyMuDt7Y2NGzfKl2Mn1TN16lSsWbMGU6dOxaBBg3DlyhU4OTlh06ZNiIqKwokTJ8SOSCXk6OhYov1kMhmnShP9AwsgKlZgYCAyMjKwZs0a1KlTBwBw8+ZNhISEwNDQkNPiVZizszN++eUXtGnTRmEdoBs3bqB58+aFFtQjog+Pk1IqFgdBU7FiY2MRFxcnL34AoE6dOli4cCFatmwpYjIqr4cPH8LZ2blQe35+Pt6+fStCIlK2gr9tObZLdXFSSsViAUTFsrOzK/KXYV5eHmxtbUVIRMpSr149HD16FPb29grtW7duRaNGjURKRcqwYsUKzJs3D7dv3wYAuLi44JtvvsHAgQNFTkalFRERIXaEKo0FEBXrp59+QmhoKKKjo+WrQp89exYjR47EnDlzRE5H5REeHo6+ffvi4cOHyM/Px7Zt23Dz5k2sXbsWf/31l9jxqIzCw8MRGRmJ0NBQNG/eHABw4sQJjBo1CklJSZg6darICam8OClFeTgGiBSYmJgodJlnZWUhNzcX1aq9q5ULPtfT00N6erpYMUkJjh49iqlTpyI+Pl6+DlB4eDjvNK3CLCwssGDBAvTq1UuhfcOGDQgNDUVqaqpIyai8OClF+dgDRAp4C4SqLzc3FzNmzED//v2xb98+seOQEr19+1bhHn4FPD09kZubK0IiUpaBAwfi7du3uH79eqFJKQMHDuSklDJgDxCRBOnr6+PKlStwcHAQOwopUWhoKDQ0NBAZGanQPmbMGLx69QrR0dEiJaPy0tHRQVxcXKExeufOnUPLli2RnZ0tUjLVxR4gIglq06YNYmNjWQBVAWFhYfLPZTIZli9fjr1796JZs2YAgFOnTiEpKQnBwcFiRSQl4KQU5WMBRCRB7dq1w/jx43H58mV4enpCT09PYXunTp1ESkaldeHCBYXHBYNi7969CwAwNzeHubk5rl69+sGzkfJwUory8RIYkQQV3FW6KDKZDHl5eR8wDREVhZNSKhZ7gIgkKD8/X+wIRPQfOCmlYrEHiIiIiCSHPUD0XmfPnsXmzZuRlJSEnJwchW3btm0TKRWVV3F3EJfJZNDW1oazszN8fHygrq7+gZMREX0YLICoWBs3bkRwcDACAgKwd+9etG3bFrdu3UJKSgo+++wzseNROcybNw9Pnz5FdnY2TExMAADPnj2Drq4u9PX18eTJEzg5OeHQoUOws7MTOS0RkfIVPxKSJG/GjBmYN28edu7cCU1NTcyfPx83btxAjx49ULNmTbHjUTnMmDEDjRs3xu3bt5GWloa0tDTcunULTZs2xfz585GUlARra2uMGjVK7KhERBWCY4CoWHp6erh69SocHBxgZmaGw4cPw83NDdevX0fr1q2RnJwsdkQqo1q1auH333+Hh4eHQvuFCxfQrVs3JCQkIC4uDt26deN5VjG3b9/GoUOH8OTJk0KD3cPDw0VKRVT58BIYFcvExAQvXrwAAFSvXh1XrlyBm5sbMjIyuOqoiktOTi7y1gi5ubl4/PgxAMDW1lZ+/kk1LFu2DEOGDIG5uTmsra0VplDLZDIWQET/wAKIiuXj44N9+/bBzc0N3bt3x8iRI3Hw4EHs27cPbdq0ETselYOfnx8GDx6M5cuXy5fWv3DhAoYMGYLWrVsDAC5fvgxHR0cxY1IpTZ8+HT/88APGjRsndhSqAJyUolwcA0TFWrRoEXr27AkAmDhxIsLCwpCSkoJu3bphxYoVIqej8lixYgVMTU3h6ekJLS0taGlpwcvLC6ampvJzq6+vj7lz54qclErj2bNn6N69u9gxqAJs3LgR3t7euH79Ov744w+8ffsWV69excGDB2FkZCR2PJXEMUBEEnbz5k3cvHkTAFCnTh35XaZJNQ0YMACNGzfG119/LXYUUrKGDRti8ODBGDZsGAwMDBAfHw9HR0cMHjwYNjY2mDJlitgRVQ4LICIiFfbPNZ2ysrIQGRmJDh06wM3NDRoaGgr7jhgx4kPHIyXhpBTl4xggIiIVNm/ePIXH+vr6iI2NRWxsrEK7TCZjAaTCOClF+VgAERGpsMTERLEj0AfASSnKx0HQpODSpUu8USaRipo6dWqRvQGvXr3C1KlTRUhEysJJKcrHMUCkQF1dHcnJybC0tISTkxPOnDkDMzMzsWMRUQn88+f3n9LS0mBpaYm8vDyRkhFVPrwERgqMjY2RmJgIS0tL3Lt3j71BVVhGRgZOnz5d5IrBwcHBIqWi8hAEQWHxwwLx8fEwNTUVIRFR5cUCiBR069YNvr6+sLGxgUwmg5eXV7F3BE9ISPjA6UhZdu7cid69e+Ply5cwNDQstGIwCyDVYmJiAplMBplMhtq1ayucz7y8PLx8+ZJT44n+hZfAqJCYmBjcuXMHI0aMwNSpU2FgYFDkfiNHjvzAyUhZateujfbt22PGjBnQ1dUVOw6V05o1ayAIAvr374+oqCiFhfE0NTXh4OCA5s2bi5iQqPJhAUTFCgkJwYIFC4otgEh16enp4fLly3BychI7CilRbGwsvL29C63/Q6rp0qVLaNCgAdTUOF+pIrAAohJ58OABAKBGjRoiJyFl6Nq1K3r27IkePXqIHYXKKTMzE4aGhvLP36dgP1INnJRSsTgGiIqVn5+P6dOnY+7cuXj58iUAwMDAAKNHj8bEiRP5V4mK+fPPP+Wfd+jQAWPHjsW1a9eKXDG4U6dOHzoelZGJiYn8l6SxsXGRg6ALBkdzFphq4aSUisUCiIo1ceJErFixArNmzUKLFi0AAMeOHcPkyZPx+vVr/PDDDyInpNLo0qVLobai1obhL0rVcvDgQfkMr4MHDxZZAJFq4qSUisVLYFQsW1tbLFmypFBvwI4dOzB06FA8fPhQpGRERNLASSkVhz1AVKz09HS4uroWand1dUV6eroIiUhZ1q5di6CgIGhpaSm05+TkYOPGjZwGr6J8fHzQqlUr+Pr6okWLFtDW1hY7EpVTYGAgAODcuXMYOXIkJ6UoEXuAqFhNmzZF06ZNFe42DQChoaE4c+YMTp48KVIyKi+uGFw1TZ8+HUeOHEFcXBxyc3Ph5eWlUBBxyYOqgZNSlIMFEBUrNjYWHTp0QM2aNeVriJw4cQL379/H7t270bJlS5ETUlmpqakhJSUFFhYWCu3x8fHw8/NjD5+Ky83NxZkzZxAbG4vDhw/j4MGDUFNTw+vXr8WORmXESSnKx0tgVCxfX1/cunUL0dHRuHHjBoB306eHDh0KW1tbkdNRWTRq1Ei+YnCbNm1Qrdr//gvIy8tDYmKivMudVFdCQgIuX76M+Ph4XLp0CQYGBvDx8RE7FpUDJ6UoH3uAiCRkypQp8n9Hjx4NfX19+baCFYO7desGTU1NsSJSOXzxxReIjY3Fmzdv4OPjA19fX7Rq1QoNGzbk7DAVx0kpyscCiEiC1qxZg6CgIA6SrWLU1NRgbm6O/v37o3Xr1vj444857qeK0NbWxqVLl1C7dm2F9ps3b8LDwwOvXr0SKZnq4kVDIgnq27cvi58qKC0tDcuXL0dOTg4mTJgAc3NzeHt747vvvsPevXvFjkfl4O7ujkWLFhVqX7RoEdzd3UVIpPrYA0QkEaamprh16xbMzc3ldw8vDgdBVw137tzB9OnTsX79euTn53N2nwrjpBTl4yBoIomYN2+efA2RefPmcUxIFZSWliaf+XX48GFcu3YNxsbG6NixI3x9fcWOR+XASSnKxx4g+k9Pnz7FzZs3AQB16tQpNHWaiCoHdXV1mJubo2XLlvIB0G5ubmLHIqqUWABRsbKyshAaGop169bJu87V1dURHByMhQsXcnClCgsODoafnx98fHxQq1YtseOQkly9ehX169cXOwaRSuAgaCpWWFgYYmNj8eeffyIjIwMZGRnYsWMHYmNjMXr0aLHjUTloampi5syZcHFxgZ2dHb788kssX74ct2/fFjsalQOLH6KSYw8QFcvc3Bxbt25Fq1atFNoPHTqEHj164OnTp+IEI6V5+PAhjhw5gtjYWMTGxuLWrVuwsbGRL7VPRFRVsQeIipWdnQ0rK6tC7ZaWlsjOzhYhESmbiYkJzMzMYGJiAmNjY1SrVo1jvIhIEtgDRMVq06YNzMzMsHbtWvmaMa9evULfvn2Rnp6O/fv3i5yQyuq7777D4cOHceHCBdStW1c+YNbHxwcmJiZixyOiYnBSivKwAKJiXb58GYGBgXjz5o18oa34+Hhoa2tjz549HG+gwtTU1GBhYYFRo0aha9euhVaXJdW0du1aBAUFQUtLS6E9JycHGzduRHBwsEjJqLw4KUX5WADRe2VnZ2P9+vXydSfq1q2L3r17Q0dHR+RkVB7x8fHy9WKOHj0KTU1NeS9Qq1atWBCpKHV1dSQnJ8PS0lKhPS0tDZaWllwIUYUNHjwY+/fvx6JFixRuhjpixAh88sknWLx4scgJVQ8LICrWkSNH4O3trXDHcADIzc1FXFwc7y5dhcTHx2PevHlcMVjFqampISUlpdBlkfj4ePj5+XGFbxXGSSnKx5WgqVh+fn5F/jX5/Plz+Pn58ZekChMEARcuXJCvGHzs2DFkZmaiYcOGXDFYBTVq1AgymQwymQxt2rRR+KMlLy8PiYmJCAwMFDEhlRcnpSgfCyAqliAIRd4uIS0tDXp6eiIkImUxNTXFy5cv4e7uDl9fXwwaNAgtW7aEsbGx2NGoDLp06QIAuHjxIgICAqCvry/fpqmpCQcHB3Tr1k2kdKQMzZs3R0RERKFJKVOmTJHfG4xKh5fAqJCuXbsCAHbs2IHAwECFAZV5eXm4dOkS6tSpg5iYGLEiUjnt2rULLVu2hKGhodhRSInWrFmDoKAg+S9Iqjo4KUX5WABRISEhIQDe/Wfao0cPhQHPBX9NDho0CObm5mJFJCKSHE5KUS4WQFSsKVOmYMyYMbzcRaQi8vLyMG/ePGzevBlJSUnIyclR2M5B0KqLk1KUjytBU7EiIiKgp6eHp0+f4tixYzh27BhnGhBVYlOmTEFkZCSCgoLw/PlzhIWFoWvXrlBTU8PkyZPFjkflUNwsvoJJKVR6LICoWNnZ2ejfvz9sbGzg4+MDHx8f2NraYsCAAZx1QFQJrV+/HsuWLcPo0aNRrVo19OrVC8uXL0d4eDhOnjwpdjwqB05KUT7OAqNijRo1CrGxsdi5c2ehhbdGjx7NhbdUGLvTq6bHjx/Dzc0NAKCvr4/nz58DAD799FNMmjRJzGhURgWTUmQyGfr161fkpBRvb2+x4qk0FkBUrN9//73Qwlvt27eHjo4OevTowQJIhXGNp6qpRo0aSE5ORs2aNVGrVi3s3bsXH330Ec6cOVPo9hikGoyMjAC86wEyMDAoNCmlWbNmGDRokFjxVBoLICoWF96qutidXjV99tlnOHDgAJo2bYrQ0FB8+eWXWLFiBZKSkjBq1Cix41EZrFq1CgDg4ODASSlKxllgVCzeDb7q4RpP0nLy5EnExcXBxcUFHTt2FDsOKQHvBq887AGiYs2fPx8BAQGoUaNGkQtvkephd7q0NGvWDM2aNRM7BilBdnY2hg8fjrVr1yI/Px8A7wZfXuwBovfiwltVE9d4qppmzpwJKysr9O/fX6F95cqVePr0KcaNGydSMiov3g1e+VgAERFVEQ4ODvjtt98KzQo6deoUevbsicTERJGSUXnxbvDKx0tg9F63b9/GoUOH8OTJE3m3a4Hw8HCRUpEybN26tdgVg8+fPy9SKiqPx48fw8bGplC7hYUFkpOTRUhEysJJKcrHhRCpWMuWLUPdunURHh6OrVu34o8//pB/bN++Xex4VA4LFixASEgIrKyscOHCBTRp0gRmZmZISEhAu3btxI5HZWRnZ4fjx48Xaj9+/DhsbW1FSETKUnA3+NevX8vbeDf48mEPEBVr+vTp+OGHHzhuoAr6+eefsXTpUvTq1QurV6/Gt99+CycnJ4SHh/N+USps0KBB+Oabb/D27Vu0bt0aAHDgwAF8++23GD16tMjpqDw4KUX5OAaIimVoaIiLFy/CyclJ7CikZLq6urh+/Trs7e1haWmJffv2wd3dHbdv30azZs2QlpYmdkQqA0EQMH78eCxYsEB+WVNbWxvjxo3jJesqgJNSlIs9QFSs7t27Y+/evfj666/FjkJKZm1tjfT0dNjb26NmzZo4efIk3N3dkZiYCP5NpLpkMhl+/PFHTJo0CdevX4eOjg5cXFy4CnQVoaury2UqlIgFEClYsGCB/HNnZ2dMmjQJJ0+ehJubGzQ0NBT2HTFixIeOR0rSunVr/Pnnn2jUqBFCQkIwatQobN26FWfPnpUvlkiqS19fH40bNxY7BikZJ6UoFy+BkQJHR8cS7SeTyZCQkFDBaaii5OfnIz8/X34z1I0bN8pXDB48eDA0NTVFTkhE/7Rs2TIMGTIE5ubmsLa2VriVjUwm48zNMmABREREVMnZ29tj6NChnJSiRJwGTyUiCALHhlQhq1atwpYtWwq1b9myBWvWrBEhERG9z7Nnz9C9e3exY1QpLIDovVasWIEGDRpAW1sb2traaNCgAZYvXy52LCqnmTNnwtzcvFC7paUlZsyYIUIiUoasrCyxI1AFKZiUQsrDQdBUrPDwcERGRiI0NFS+0NaJEycwatQoJCUlYerUqSInpLJKSkoqcryXvb09kpKSREhEymBlZYUePXqgf//++Pjjj8WOQ+XESSkVi2OAqFgWFhZYsGABevXqpdC+YcMGhIaGIjU1VaRkVF41a9bEokWL0KlTJ4X2HTt2YNiwYXjw4IFIyag8tm/fjtWrV2P37t1wcHBA//79ERwczFWgVRQnpVQs9gBRsd6+fQsvL69C7Z6ensjNzRUhESlLr169MGLECBgYGMDHxwcAEBsbi5EjR6Jnz54ip6Oy6tKlC7p06YKnT59i3bp1WL16NSZNmoSAgAD0798fnTp1ks/8o8qPN6+tWOwBomKFhoZCQ0MDkZGRCu1jxozBq1evEB0dLVIyKq+cnBz06dMHW7Zskf9CzM/PR3BwMJYsWcJp8FXIwoULMXbsWOTk5MDc3Bxff/01xo8fD11dXbGjURkV/Nr+51R4Kj0WQFSs0NBQrF27FnZ2dmjWrBkA4NSpU0hKSkJwcLDCNeh/F0mkGm7fvo2LFy9CR0cHbm5usLe3FzsSKUFKSgrWrFmD1atX4++//8Znn32GAQMG4MGDB/jxxx9ha2vLAbUqaMWKFZg3bx5u374NAHBxccE333yDgQMHipxMNbEAomL5+fmVaD+ZTIaDBw9WcBoi+i/btm3DqlWrsGfPHtSrVw8DBw7El19+CWNjY/k+d+/eRd26deX3CiPVUNyklEWLFmHUqFGclFIGLICIiKoIIyMj9OzZEwMHDiz2VhivXr3C7NmzERER8YHTUXlwUorysQAiIqoisrOzObanijI2NsaZM2fg4uKi0H7r1i00adIEGRkZ4gRTYVwIkYioivhn8fP69WtkZmYqfJDq6tOnDxYvXlyofenSpejdu7cIiVQfe4CIJOjt27eFFlIrkJqaWuQq0VT5ZWVlYdy4cdi8eTPS0tIKbc/LyxMhFSkDJ6UoHxeEIJKgnj17YuvWrYWm0aakpKBNmza4cuWKSMmoPL799lscOnQIixcvRp8+fRAdHY2HDx/il19+waxZs8SOR+Vw5coVfPTRRwDeDWQHAHNzc5ibmyv8vHJqfMmxB4hIgho3boyGDRtixYoV8rbHjx/Dz88P9evXx9atW0VMR2VVs2ZNrF27Fq1atYKhoSHOnz8PZ2dnrFu3Dhs2bMDu3bvFjkhUabAAov907do1JCUlFZo2++/bKJDqePr0KXx8fNCuXTtERkbi0aNH8PPzg7u7OzZu3Ag1NQ4PVEX6+vq4du0aatasiRo1amDbtm1o0qQJEhMT4ebmhpcvX4odkajS4CUwKlZCQgI+++wzXL58GTKZrNDqoxxPoLosLCywd+9e+Q0z//rrL3z00UdYv349ix8V5uTkhMTERNSsWROurq7YvHkzmjRpgp07dyqsBUREnAVG7zFy5Eg4OjriyZMn0NXVxdWrV3HkyBF4eXnh8OHDYsejcrKzs8O+ffuwfv16NGnSBBs2bIC6urrYsagcQkJCEB8fDwAYP348oqOjoa2tjVGjRmHs2LEipyOqXHgJjIplbm6OgwcPomHDhjAyMsLp06dRp04dHDx4EKNHj8aFCxfEjkilYGJiUuQAyezsbGhpaSkUP+np6R8yGlWQv//+G+fOnYOzszMaNmwodhyiSoWXwKhYeXl5MDAwAPCuGHr06BHq1KkDe3t73Lx5U+R0VFpRUVFiR6AK9PbtWwQGBmLJkiXyxfLs7e15fzeiYrAAomI1aNAA8fHxcHR0RNOmTTF79mxoampi6dKlcHJyEjselVLfvn3FjkAVSENDA5cuXRI7BlUwTkpRHl4Co2Lt2bMHWVlZ6Nq1K+7cuYNPP/0Ut27dgpmZGTZt2oTWrVuLHZHKaPfu3VBXV0dAQIBC+969e5GXl4d27dqJlIzKY9SoUdDS0uKaP1UQJ6UoHwsgKpX09PRix5KQ6mjYsCFmzZqF9u3bK7THxMRg3Lhx8oG0pFoKVgt2cXGBp6cn9PT0FLZzhWDV1bFjR6irq2P58uVwdHTE6dOnkZaWhtGjR2POnDlo2bKl2BFVDgsgIgnS0dHB9evX4eDgoNB+79491K9fH1lZWeIEo3Lx8/MrdptMJsPBgwc/YBpSJk5KUT6OASKSICMjIyQkJBQqgO7cuVOo14BUx6FDh8SOQBWEk1KUj+sAEUlQ586d8c0338jvKQS8K35Gjx7NwZRElVDBpBQA8kkpx48fx9SpUzkppYx4CYxIgp4/f47AwECcPXsWNWrUAAA8ePAALVu2xLZt27hqsIr67LPPihyfJ5PJoK2tDWdnZ3zxxReoU6eOCOmoPDgpRflYABFJlCAI2LdvH+Lj46Gjo4OGDRvCx8dH7FhUDv369cP27dthbGwMT09PAMD58+eRkZGBtm3bIj4+Hvfu3cOBAwfQokULkdNSeXFSSvmwACIiqiLGjx+PzMxMLFq0SH5Pt/z8fIwcORIGBgb44Ycf8PXXX+Pq1as4duyYyGmJxMUCiEiisrKyEBsbW+SiaiNGjBApFZWHhYUFjh8/jtq1ayu037p1C97e3khNTcXly5fRsmVLZGRkiBOSqJLgLDAiCbpw4QLat2+P7OxsZGVlwdTUFKmpqdDV1YWlpSULIBWVm5uLGzduFCqAbty4IV8oT1tbm5dMiMBZYESSNGrUKHTs2BHPnj2Djo4OTp48ib///huenp6YM2eO2PGojPr06YMBAwZg3rx5OHbsGI4dO4Z58+ZhwIABCA4OBgDExsaifv36IiclEh8vgRFJkLGxMU6dOoU6derA2NgYJ06cQN26dXHq1Cn07dsXN27cEDsilUFeXh5mzZqFRYsWISUlBQBgZWWF0NBQjBs3Durq6khKSoKampp89h+RVPESGJEEaWhoyAfJWlpaIikpCXXr1oWRkRHu378vcjoqK3V1dUycOBETJ05EZmYmAMDQ0FBhn5o1a4oRjajSYQFEJEGNGjXCmTNn4OLiAl9fX4SHhyM1NRXr1q1DgwYNxI5HSvDvwoeIFPESGJEEnT17Fi9evICfnx+ePHmC4OBgxMXFwcXFBStXroS7u7vYEYmIKhQLICIiIpIczgIjIiIiyeEYICKJaNSoUYnXfzl//nwFp6GK8ODBg2Jnd508eRLNmjX7wImIKi8WQEQS0aVLF7EjUAVr27Ytjh07BlNTU4X248ePo0OHDlz9megfWAARSURERITYEaiCNWvWDG3btsWhQ4dgYGAAADhy5Ag6duyIyZMnixuOqJLhIGgiCTt79iyuX78OAKhXr578DuKkmvLz8/H5558jPT0de/bsQVxcHDp16oTp06dj5MiRYscjqlRYABFJ0IMHD9CrVy8cP34cxsbGAICMjAx4e3tj48aNXCVYheXk5KBDhw7Izs7GpUuXMHPmTAwfPlzsWESVDgsgIgkKDAxERkYG1qxZgzp16gAAbt68iZCQEBgaGiImJkbkhFRSly5dKtT24sUL9OrVCx06dMCQIUPk7Q0bNvyQ0YgqNRZARBKko6ODuLg4NGrUSKH93LlzaNmyJbKzs0VKRqWlpqYGmUyGf/5X/s/HBZ/LZDL5HeGJiIOgiSTJzs4Ob9++LdSel5cHW1tbERJRWSUmJoodgUglcSFEIgn66aefEBoairNnz8rbzp49i5EjR2LOnDkiJqPSsre3h729PWxtbTFlyhTk5+fL2/79QUT/w0tgRBJhYmKisBBiVlYWcnNzUa3au47ggs/19PSQnp4uVkwqByMjI1y8eBGOjo5iRyGq9HgJjEgioqKixI5AFaxLly7Yvn07Ro0aJXYUokqPPUBERFXE9OnTMXfuXLRp0waenp7Q09NT2D5ixAiRkhFVPiyAiIiqiPdd+pLJZEhISPiAaYgqNxZAREREJDmcBUZERESSw0HQRERVyIMHD/Dnn38iKSkJOTk5CtsiIyNFSkVU+bAAIiKqIg4cOIBOnTrByckJN27cQIMGDXDv3j0IgoCPPvpI7HhElQrHABFJ1NmzZ7F58+Yiewq2bdsmUioqjyZNmqBdu3aYMmUKDAwMEB8fD0tLS/Tu3RuBgYEK9wUjkjqOASKSoI0bN8Lb2xvXr1/HH3/8gbdv3+Lq1as4ePAgjIyMxI5HZXT9+nUEBwcDAKpVq4ZXr15BX18fU6dOxY8//ihyOqLKhQUQkQTNmDED8+bNw86dO6GpqYn58+fjxo0b6NGjB2rWrCl2PCojPT09eW+ejY0N7t69K9+WmpoqViyiSokFEJEE3b17Fx06dAAAaGpqIisrCzKZDKNGjcLSpUtFTkdl1axZMxw7dgwA0L59e4wePRo//PAD+vfvj2bNmomcjqhy4SBoIgkyMTHBixcvAADVq1fHlStX4ObmhoyMDGRnZ4ucjsoqMjISL1++BABMmTIFL1++xKZNm+Di4sIZYET/wgKISIJ8fHywb98+uLm5oXv37hg5ciQOHjyIffv2oU2bNmLHozJycnKSf66np4clS5aImIaocuMsMCIJSk9Px+vXr2Fra4v8/HzMnj0bcXFxcHFxwffffw8TExOxIxIRVSgWQEREKszExAQymaxE+6anp1dwGiLVwUtgREQqLCoqSv55Wloapk+fjoCAADRv3hwAcOLECezZsweTJk0SKSFR5cQeICKiKqJbt27w8/PD8OHDFdoXLVqE/fv3Y/v27eIEI6qEWAAREVUR+vr6uHjxIpydnRXa79y5Aw8PD/kMMSLiOkBEknHp0iXk5+eLHYMqkJmZGXbs2FGofceOHTAzMxMhEVHlxTFARBLRqFEjJCcnw9LSEk5OTjhz5gx/KVYxU6ZMwcCBA3H48GE0bdoUAHDq1CnExMRg2bJlIqcjqlzYA0QkEcbGxkhMTAQA3Lt3j71BVVC/fv1w/PhxGBoaYtu2bdi2bRsMDQ1x7Ngx9OvXT+x4RJUKxwARScRXX32FtWvXwsbGBklJSahRowbU1dWL3DchIeEDpyMi+rB4CYxIIpYuXYquXbvizp07GDFiBAYNGgQDAwOxYxERiYI9QEQSFBISggULFrAAIiLJYgFEJHEPHjwAANSoUUPkJEREHw4HQRNJUH5+PqZOnQojIyPY29vD3t4exsbGmDZtGgdHE5EkcAwQkQRNnDgRK1aswKxZs9CiRQsAwLFjxzB58mS8fv0aP/zwg8gJiYgqFi+BEUmQra0tlixZgk6dOim079ixA0OHDsXDhw9FSkbldfbsWWzevBlJSUnIyclR2LZt2zaRUhFVPrwERiRB6enpcHV1LdTu6urKO4arsI0bN8Lb2xvXr1/HH3/8gbdv3+Lq1as4ePAgjIyMxI5HVKmwACKSIHd3dyxatKhQ+6JFi+Du7i5CIlKGGTNmYN68edi5cyc0NTUxf/583LhxAz169EDNmjXFjkdUqfASGJEExcbGokOHDqhZsyaaN28OADhx4gTu37+P3bt3o2XLliInpLLQ09PD1atX4eDgADMzMxw+fBhubm64fv06WrdujeTkZLEjElUa7AEikiBfX1/cunULn332GTIyMpCRkYGuXbvi5s2bLH5UmImJCV68eAEAqF69Oq5cuQIAyMjIQHZ2tpjRiCodzgIjkihbW1vO9qpifHx8sG/fPri5uaF79+4YOXIkDh48iH379qFNmzZixyOqVHgJjIioikhPT8fr169ha2uL/Px8zJ49G3FxcXBxccH3338PExMTsSMSVRosgIiIiEhyOAaIiIiIJIcFEBEREUkOB0ETSdjTp09x8+ZNAECdOnVgYWEhciIiog+DPUBEEpSVlYX+/fvD1tYWPj4+8PHxga2tLQYMGMDp0irm0qVLvIEtURmwACKSoLCwMMTGxuLPP/+UrwO0Y8cOxMbGYvTo0WLHo1Jo1KgRUlNTAQBOTk5IS0sTORGRauAsMCIJMjc3x9atW9GqVSuF9kOHDqFHjx54+vSpOMGo1MzMzLB79240bdoUampqSElJ4aVMohLgGCAiCcrOzoaVlVWhdktLS14CUzHdunWDr68vbGxsIJPJ4OXlBXV19SL3TUhI+MDpiCov9gARSVCbNm1gZmaGtWvXQltbGwDw6tUr9O3bF+np6di/f7/ICak0YmJicOfOHYwYMQJTp06FgYFBkfuNHDnyAycjqrxYABFJ0OXLlxEYGIg3b97I7/4eHx8PbW1t7NmzB/Xr1xc5IZVFSEgIFixYUGwBRET/wwKISKKys7Oxfv163LhxAwBQt25d9O7dGzo6OiInI2V48OABAKBGjRoiJyGqnFgAEUnQkSNH4O3tjWrVFIcB5ubmIi4uDj4+PiIlo/LIz8/H9OnTMXfuXLx8+RIAYGBggNGjR2PixIlQU+PEX6ICLICIJEhdXR3JycmwtLRUaE9LS4OlpSXy8vJESkblMWHCBKxYsQJTpkxBixYtAADHjh3D5MmTMWjQIPzwww8iJySqPFgAEUlQcdOlb926BS8vL2RmZoqUjMrD1tYWS5YsQadOnRTad+zYgaFDh+Lhw4ciJSOqfDgNnkhCunbtCgCQyWTo168ftLS05Nvy8vJw6dIleHt7ixWPyik9PR2urq6F2l1dXZGeni5CIqLKixeEiSTEyMgIRkZGEAQBBgYG8sdGRkawtrbGV199hV9//VXsmFRG7u7uWLRoUaH2RYsWyWf7EdE7vARGJEFTpkzBmDFjoKenJ3YUUqLY2Fh06NABNWvWRPPmzQEAJ06cwP3797F79260bNlS5IRElQcLICIJ493gq55Hjx4hOjpaYXmDoUOHwtbWVuRkRJULCyAiCcrOzsbw4cOxdu1a+Z3E1dXVERwcjIULF0JXV1fkhEREFYtjgIgkaNSoUYiNjcXOnTt5N3gikiT2ABFJEO8GT0RSxx4gIgni3eCJSOrYA0QkQbwbPBFJHQsgIgm6cuUKAgICeDf4Koqz+4j+GwsgIoni3eCrnqysLISGhmLdunXy+7lxdh9R0VgAERFVEYMHD8b+/fuxaNEihZuhjhgxAp988gkWL14sckKiyoMFEJFE3b59G4cOHcKTJ0/kawEVCA8PFykVlQdn9xGVHG+GSiRBy5Ytw5AhQ2Bubg5ra2vIZDL5NplMxgJIRXF2H1HJsQeISILs7e0xdOhQjBs3TuwopESc3UdUciyAiCTI0NAQFy9ehJOTk9hRSIkuX76MwMBAzu4jKgEWQEQSNGDAADRu3Bhff/212FFIyTi7j6hkWAARScSCBQvkn2dlZSEyMhIdOnSAm5sbNDQ0FPYdMWLEh45HSnDkyBF4e3ujWjXF4Z25ubmIi4uDj4+PSMmIKh8WQEQS4ejoWKL9ZDIZEhISKjgNVQR1dXUkJyfD0tJSoT0tLQ2WlpbytYGIiLPAiCQjMTFR7AhUwQRBUJjRVyAtLQ16enoiJCKqvFgAEUlcQSdwUb84STV07doVwLtz2K9fP2hpacm35eXl4dKlS/D29hYrHlGlxLvBE0nUihUr0KBBA2hra0NbWxsNGjTA8uXLxY5FZWBkZAQjIyMIggADAwP5YyMjI1hbW+Orr77Cr7/+KnZMokqFPUBEEhQeHo7IyEiEhoaiefPmAIATJ05g1KhRSEpKwtSpU0VOSKWxatUqAICDgwPGjBnDy11EJcBB0EQSZGFhgQULFqBXr14K7Rs2bEBoaChSU1NFSkbKwLvBE/03XgIjkqC3b9/Cy8urULunpydyc3NFSETKkJ2djf79+8PGxgY+Pj7w8fGBra0tBgwYwFthEP0LCyAiCerTp0+RdwZfunQpevfuLUIiUoZRo0YhNjYWO3fuREZGBjIyMrBjxw7ExsZi9OjRYscjqlR4CYxIgkJDQ7F27VrY2dmhWbNmAIBTp04hKSkJwcHBCgsjRkZGihWTSol3gycqOQ6CJpKgK1eu4KOPPgIA3L17F8C7X57m5ua4cuWKfD9OjVctvBs8UcmxB4iIqIrg3eCJSo4FEBFRFXHlyhUEBATwbvBEJcACiIioCuHd4IlKhgUQERERSQ4HQRMRVSG3b9/GoUOH8OTJE+Tn5ytsCw8PFykVUeXDHiAioipi2bJlGDJkCMzNzWFtba0wi08mk+H8+fMipiOqXFgAEUnYtWvXkJSUhJycHIX2Tp06iZSIysPe3h5Dhw7FuHHjxI5CVOmxACKSoISEBHz22We4fPkyZDIZCv4bKOgxyMvLEzMelZGhoSEuXrwIJycnsaMQVXq8FQaRBI0cORKOjo548uQJdHV1cfXqVRw5cgReXl44fPiw2PGojLp37469e/eKHYNIJbAHiEiCzM3NcfDgQTRs2BBGRkY4ffo06tSpg4MHD2L06NG4cOGC2BGphBYsWCD/PCsrC5GRkejQoQPc3NwUbmkCACNGjPjQ8YgqLRZARBJkYmKC8+fPw9HREbVq1cLy5cvh5+eHu3fvws3NjbdNUCGOjo4l2k8mkyEhIaGC0xCpDk6DJ5KgBg0aID4+Ho6OjmjatClmz54NTU1NLF26lONHVExiYqLYEYhUEnuAiCRoz549yMrKQteuXXHnzh18+umnuHXrFszMzLBp0ya0bt1a7IhUTv8e2E5EilgAEREAID09HSYmJvyFqeJWrFiBefPm4fbt2wAAFxcXfPPNNxg4cKDIyYgqF14CIyIAgKmpqdgRqJzCw8MRGRmJ0NBQNG/eHABw4sQJjBo1CklJSZg6darICYkqD/YAERFVERYWFliwYAF69eql0L5hwwaEhoYiNTVVpGRElQ/XASIiqiLevn0LLy+vQu2enp7Izc0VIRFR5cUCiIioiujTpw8WL15cqH3p0qXo3bu3CImIKi9eAiMiqiJCQ0Oxdu1a2NnZoVmzZgCAU6dOISkpCcHBwQoLI0ZGRooVk6hSYAFERFRF+Pn5lWg/mUyGgwcPVnAaosqNBRARERFJDscAERERkeSwACIiIiLJYQFEREREksMCiIiIiCSHBRARERFJDu8FRkRUxVy7dg1JSUnIyclRaO/UqZNIiYgqHxZARERVREJCAj777DNcvnwZMpkMBaucyGQyAEBeXp6Y8YgqFV4CIyKqIkaOHAlHR0c8efIEurq6uHr1Ko4cOQIvLy8cPnxY7HhElQoXQiQiqiLMzc1x8OBBNGzYEEZGRjh9+jTq1KmDgwcPYvTo0bhw4YLYEYkqDfYAERFVEXl5eTAwMADwrhh69OgRAMDe3h43b94UMxpRpcMxQEREVUSDBg0QHx8PR0dHNG3aFLNnz4ampiaWLl0KJycnseMRVSq8BEZEVEXs2bMHWVlZ6Nq1K+7cuYNPP/0Ut27dgpmZGTZt2oTWrVuLHZGo0mABRERUhaWnp8PExEQ+E4yI3mEBRERERJLDQdBEREQkOSyAiIiISHJYABEREZHksAAiIiIiyWEBREQfRL9+/dClSxf541atWuGbb75579c4ODggKiqqQnNVVocPH4ZMJkNGRobYUYiqJBZARBLy+PFjhIaGwsnJCVpaWrCzs0PHjh1x4MCBD55l27ZtmDZtmlKPuXr1ahgbGyvteBcuXED37t1hZWUFbW1tuLi4YNCgQbh165bSngMouhj09vZGcnIyjIyMlPpcRPQOCyAiibh37x48PT1x8OBB/PTTT7h8+TJiYmLg5+eHYcOGffA8pqam8ts2VEZ//fUXmjVrhjdv3mD9+vW4fv06fv31VxgZGWHSpEkV/vyampqwtrbm+j1EFUUgIklo166dUL16deHly5eFtj179kz++dy5c4UGDRoIurq6Qo0aNYQhQ4YIL168kG9ftWqVYGRkJMTExAiurq6Cnp6eEBAQIDx69Ei+T25urjBq1CjByMhIMDU1FcaOHSsEBwcLnTt3lu/j6+srjBw5Uv44JSVF+PTTTwVtbW3BwcFB+PXXXwV7e3th3rx5Jcp26NAhAYDCR0REhCAIgvD69Wth9OjRgq2traCrqys0adJEOHToULHvVVZWlmBubi506dKlyO0F71dubq7Qv39/wcHBQdDW1hZq164tREVFKezbt29foXPnzsLkyZMFc3NzwcDAQBg8eLDw5s0b+fZ/505MTJS/nn+em61btwr16tUTNDU1BXt7e2HOnDkKz2Vvby/88MMPQkhIiKCvry/Y2dkJv/zyi3z7mzdvhGHDhgnW1taClpaWULNmTWHGjBnFvg9EVRkLICIJSEtLE2QyWYl+2c2bN084ePCgkJiYKBw4cECoU6eOMGTIEPn2VatWCRoaGoK/v79w5swZ4dy5c0LdunWFL774Qr7Pjz/+KJiYmAi///67cO3aNWHAgAGCgYHBewugdu3aCe7u7sKJEyeEs2fPCt7e3oKOjo5CAfS+bG/evBGioqIEQ0NDITk5WUhOTpYXRwMHDhS8vb2FI0eOCHfu3BF++uknQUtLS7h161aR78G2bdsEAEJcXNx736ucnBwhPDxcOHPmjJCQkCD8+uuvgq6urrBp0yb5Pn379hX09fWFoKAg4cqVK8Jff/0lWFhYCN99950gCIKQkZEhNG/eXBg0aJA8d25ubqEC6OzZs4KampowdepU4ebNm8KqVasEHR0dYdWqVfLnsre3F0xNTYXo6Gjh9u3bwsyZMwU1NTXhxo0bgiAIwk8//STY2dkJR44cEe7duyccPXpU+O233977GomqKhZARBJw6tQpAYCwbdu2Un/tli1bBDMzM/njVatWCQCEO3fuyNuio6MFKysr+WMbGxth9uzZ8sdv374VatSoUWwBdPPmTQGAcPr0afn269evCwAUCqCSZDMyMlLY5++//xbU1dWFhw8fKrS3adNGmDBhQpHH/fHHHwUAQnp6erHPXZxhw4YJ3bp1kz/u27evYGpqKmRlZcnbFi9eLOjr6wt5eXmCIBQuBgVBKFQAffHFF8Inn3yisM/YsWOFevXqyR/b29sLX375pfxxfn6+YGlpKSxevFgQBEEIDQ0VWrduLeTn55f6dRFVNRwDRCQBQinueLN//360adMG1atXh4GBAfr06YO0tDRkZ2fL99HV1UWtWrXkj21sbPDkyRMAwPPnz5GcnIymTZvKt1erVg1eXl7FPuf169dRrVo1eHp6yttcXV0LDWguSbZ/u3z5MvLy8lC7dm3o6+vLP2JjY3H37t0iv6Y071d0dDQ8PT1hYWEBfX19LF26FElJSQr7uLu7Q1dXV/64efPmePnyJe7fv1/i57l+/TpatGih0NaiRQvcvn0beXl58raGDRvKP5fJZLC2tpafm379+uHixYuoU6cORowYgb1795b4+YmqGhZARBLg4uICmUyGGzduvHe/e/fu4dNPP0XDhg3x+++/49y5c4iOjgYA5OTkyPfT0NBQ+DqZTFaqoqEsSprt316+fAl1dXWcO3cOFy9elH9cv34d8+fPL/JrateuDQD/+X5t3LgRY8aMwYABA7B3715cvHgRISEh781T0Yo6N/n5+QCAjz76CImJiZg2bRpevXqFHj164PPPPxcjJpHoWAARSYCpqSkCAgIQHR2NrKysQtsL1po5d+4c8vPzMXfuXDRr1gy1a9fGo0ePSvVcRkZGsLGxwalTp+Rtubm5OHfuXLFf4+rqWmifmzdvKqyBU5JsmpqaCr0hANCoUSPk5eXhyZMncHZ2VviwtrYuMk/btm1hbm6O2bNnF7m9INfx48fh7e2NoUOHolGjRnB2di6yVyk+Ph6vXr2SPz558iT09fVhZ2dXbO5/q1u3Lo4fP67Qdvz4cdSuXRvq6urv/dp/MjQ0RFBQEJYtW4ZNmzbh999/R3p6eom/nqiqYAFEJBHR0dHIy8tDkyZN8Pvvv+P27du4fv06FixYgObNmwMAnJ2d8fbtWyxcuBAJCQlYt24dlixZUurnGjlyJGbNmoXt27fjxo0bGDp06HsX9KtTpw4CAwMxePBgnDp1CufOncPAgQOho6Mj36ck2RwcHPDy5UscOHAAqampyM7ORu3atdG7d28EBwdj27ZtSExMxOnTpzFz5kzs2rWryDx6enpYvnw5du3ahU6dOmH//v24d+8ezp49i2+//RZff/01gHc9a2fPnsWePXtw69YtTJo0CWfOnCl0vJycHAwYMADXrl3D7t27ERERgeHDh0NNTU2e+9SpU7h37x5SU1PlPTb/NHr0aBw4cADTpk3DrVu3sGbNGixatAhjxoz5z/NRIDIyEhs2bMCNGzdw69YtbNmyBdbW1kpdO4lIZYg8BomIPqBHjx4Jw4YNE+zt7QVNTU2hevXqQqdOnRSmhEdGRgo2NjaCjo6OEBAQIKxdu1ZhMG5RA43/+OMP4Z//nbx9+1YYOXKkYGhoKBgbGwthYWH/OQ0+OTlZ6NChg3x69tq1awtNg/+vbIIgCF9//bVgZmamMA2+YLaWg4ODoKGhIdjY2AifffaZcOnSpfe+X2fOnBG6du0qWFhYCFpaWoKzs7Pw1VdfCbdv3xYE4d30+n79+glGRkaCsbGxMGTIEGH8+PGCu7u7/BgF0+DDw8MFMzMzQV9fXxg0aJDw+vVr+T43b94UmjVrJujo6JRoGryGhoZQs2ZN4aefflLI++/3SxAEwd3dXf4+LF26VPDw8BD09PQEQ0NDoU2bNsL58+ff+x4QVVUyQajgC/dERBLWr18/ZGRkYPv27WJHIaJ/4CUwIiIikhwWQERERCQ5vARGREREksMeICIiIpIcFkBEREQkOSyAiIiISHJYABEREZHksAAiIiIiyWEBRERERJLDAoiIiIgkhwUQERERSc7/Awhqx5biIm/ZAAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"cell_type":"code","source":["# logits 분포 시각화\n","plt.bar(candidate_captions, original_logits, alpha=0.7, label=\"Original CLIP\")\n","plt.bar(candidate_captions, modified_logits, alpha=0.7, label=\"Modified CLIP\")\n","plt.ylabel(\"Logits Value\")\n","plt.xlabel(\"Candidate Captions\")\n","plt.title(\"Logits Distribution: Original vs Modified CLIP\")\n","plt.xticks(rotation=90)\n","plt.legend()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":741},"id":"iPliNzqeHfWV","executionInfo":{"status":"ok","timestamp":1733059464886,"user_tz":-540,"elapsed":731,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"f0a5a748-043b-4c9e-ac90-91ebf7cd927d"},"execution_count":214,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjIAAALUCAYAAAAR0djGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACVfklEQVR4nOzdd1gUV/828HtBqtIRAUWKKKKCDbsClojlEQuxx94LoGJMfGKPLRpLjMYeLLG3WOJjQBTU2EAFrCBFwV4QFFCkzPuHP/bNCiplYZjl/lzXXrJnhtkbdxe+e+bMOTJBEAQQERERSZCa2AGIiIiIioqFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDJUod3d3uLu7ix0jj9LMJZPJMGfOHPn9OXPmQCaT4cWLF6Xy+DY2Nhg6dGipPJYYivNcDh06FDY2NkrN87EtW7ZAJpPh3r17Jfo4ZVVwcDBkMhmCg4Plbfn9v6empmLkyJEwNzeHTCbDpEmTcO/ePchkMmzZskVpecr786GKWMiUE7lv3rCwMFFzPHr0CHPmzEF4eLjSjjl06FDIZDL5rVKlSrCzs8PXX3+NAwcOICcnRymPc/78ecyZMwfJyclKOZ4yleVsn3Ls2DF06tQJJiYm0NbWRq1atTB16lS8fPlS7GgqKfd9oq+vj7dv3+bZfvfuXfl76Oeffy71fAsXLsSWLVswbtw4bN++HYMGDSr1DPkJDw/HN998AysrK2hpacHY2BgdOnSAv78/srOz5fvJZDJMnDjxs8dyd3dHvXr1FNpsbGwUfn+ZmZmhTZs2OHToUIn8PKqogtgBSLUFBAQo3H/06BHmzp0LGxsbNGjQQGmPo6WlhU2bNgEA3r59i/v37+Po0aP4+uuv4e7ujsOHD0NfX/+TuQri/PnzmDt3LoYOHQpDQ8MCf9/bt29RoULJvtU+ly0qKgpqamXrM8vUqVOxbNky1K9fH9999x2MjY1x9epVrF69Grt370ZQUBAcHBwKdKyiPJe5Nm7cqLRCVwoqVKiA9PR0HD16FH369FHYtmPHDmhra+Pdu3clniO///dTp06hefPmmD17trxNEAS8ffsWGhoaJZ4pP5s2bcLYsWNRpUoVDBo0CDVr1sSbN28QFBSEESNG4PHjx/jvf/9b7Mdp0KAB/Pz8AHz4Hbl+/Xr06tULa9euxdixY4t9fFXHQoZKlKamZqk8ToUKFfDNN98otM2fPx+LFy/G9OnTMWrUKOzZs6fUcuXk5OD9+/fQ1taGtrZ2iT7Wl2hpaYn6+B/btWsXli1bhr59+2LHjh1QV1eXbxs6dCjatm2L3r174+rVq58tANPT06Grq1us51KsP5Bi0dLSQqtWrbBr1648hczOnTvRtWtXHDhwoMRz5Pf//uzZM9SpU0ehTSaTifb+uXjxIsaOHYsWLVrg+PHj0NPTk2+bNGkSwsLCcOPGDaU8VtWqVRV+fw0ePBj29vZYsWIFC5kCKFsf00h0165dQ+fOnaGvr49KlSqhffv2uHjxYp79IiMj4ebmBh0dHVSrVg3z58+Hv79/nnPP/x6/EBwcjCZNmgAAhg0bJu9KzT3/fffuXXh5ecHc3Bza2tqoVq0a+vXrh5SUlCL/PN9//z06duyIffv2ITo6Ot9cuX799VfUrVsXurq6MDIygouLC3bu3Angw7iWb7/9FgBga2srz577s+Z2K+/YsQN169aFlpYWTpw4Id/27zEyuV68eIE+ffpAX18fJiYm8PX1Vfg0/LnxAf8+5pey5TdGJi4uDr1794axsTF0dXXRvHlz/PXXXwr75I5t2Lt3LxYsWIBq1apBW1sb7du3R0xMjMK+6enpuHPnToHG/cydOxdGRkbYsGGDQhEDAE2bNsV3332H69evY//+/fL23C75K1euwNXVFbq6uvJPwvk9l/fv34enpycqVqwIMzMzTJ48GX///fcXx2rk/p///PPP2LBhA2rUqAEtLS00adIEoaGhCo8RGRmJoUOHws7ODtra2jA3N8fw4cOLdGrs559/hkwmw/379/Nsmz59OjQ1NfHq1SsAxX+fDBgwAP/73/8UTkOGhobi7t27GDBgQL7fU5DXCwA8ePAAPXr0UPh/z8jIyLPfv//fc19n8fHx+OuvvxRev596D9y5cwdff/01jI2Noa2tDRcXFxw5ciTP49y8eRPt2rVT+D1V0B64uXPnQiaTYceOHQpFTC4XF5cSG3tmbm4OR0dHxMfHl8jxVQ17ZEju5s2baNOmDfT19TFt2jRoaGhg/fr1cHd3R0hICJo1awYAePjwIdq2bQuZTIbp06ejYsWK2LRp0xc/+Ts6OmLevHmYNWsWRo8ejTZt2gAAWrZsiffv38PDwwMZGRnw9vaGubk5Hj58iGPHjiE5ORkGBgZF/rkGDRqEgIAABAYGolatWvnus3HjRvj4+ODrr7+WFxSRkZG4dOkSBgwYgF69eiE6Ohq7du3CihUrYGpqCgCoXLmy/BinTp3C3r17MXHiRJiamn5xEGmfPn1gY2ODRYsW4eLFi1i1ahVevXqFbdu2FernK0i2f3v69ClatmyJ9PR0+Pj4wMTEBFu3boWnpyf279+Pnj17Kuy/ePFiqKmpYerUqUhJScGSJUswcOBAXLp0Sb7P5cuX0bZtW8yePTvfoi3X3bt3ERUVhaFDhyqc6vu3wYMHY/bs2Th27Bj69esnb3/58iU6d+6Mfv364ZtvvkGVKlXy/f60tDS0a9cOjx8/hq+vL8zNzbFz506cPn36k7k+tnPnTrx58wZjxoyBTCbDkiVL0KtXL8TFxcl7EwIDAxEXF4dhw4bB3NwcN2/exIYNG3Dz5k1cvHgRMpmswI/Xp08fTJs2DXv37pUXpbn27t2Ljh07wsjISCnvk169emHs2LE4ePAghg8fLv95a9eujUaNGuXZv6Cvl7dv36J9+/ZISEiAj48PLC0tsX37dpw6deqzeRwdHbF9+3ZMnjwZ1apVk59iqVy5Mp4/f55n/5s3b6JVq1aoWrUqvv/+e1SsWBF79+5Fjx49cODAAXmeJ0+eoG3btsjKypLvt2HDBujo6Hzx/yg9PR1BQUFwdXVF9erVv7i/smVmZiIxMREmJial/tiSJFC54O/vLwAQQkNDP7lPjx49BE1NTSE2Nlbe9ujRI0FPT09wdXWVt3l7ewsymUy4du2avO3ly5eCsbGxAECIj4+Xt7u5uQlubm7y+6GhoQIAwd/fX+Gxr127JgAQ9u3bV+ifbciQIULFihU/uT332JMnT/5kru7duwt169b97OMsXbo0z8+XC4CgpqYm3Lx5M99ts2fPlt+fPXu2AEDw9PRU2G/8+PECACEiIkIQBEGIj4/P9/8qv2N+Lpu1tbUwZMgQ+f1JkyYJAISzZ8/K2968eSPY2toKNjY2QnZ2tiAIgnD69GkBgODo6ChkZGTI9/3ll18EAML169flbbn7/jtTfv78808BgLBixYrP7qevry80atRIft/NzU0AIKxbty7Pvh8/l8uWLRMACH/++ae87e3bt0Lt2rUFAMLp06fl7UOGDBGsra3l93P/z01MTISkpCR5++HDhwUAwtGjR+Vt6enpebLs2rVLACCcOXNG3pb73svvufm3Fi1aCI0bN1Zou3z5sgBA2LZtmyAIynuffP3110L79u0FQRCE7OxswdzcXJg7d67851+6dKn8+wr6elm5cqUAQNi7d698v7S0NMHe3v6L/++C8OF12rVrV4W2/N4D7du3F5ycnIR3797J23JycoSWLVsKNWvWzJP70qVL8rZnz54JBgYGX3w+IiIiBACCr6/vJ/f5GABhwoQJn93Hzc0tz+8Za2troWPHjsLz58+F58+fCxEREUK/fv0EAIK3t3eBH78846klAgBkZ2cjICAAPXr0gJ2dnbzdwsICAwYMwLlz5/D69WsAwIkTJ9CiRQuFwbrGxsYYOHBgkR8/95Pk33//jfT09CIfJz+VKlUCALx58+aT+xgaGuLBgwd5Th8UhpubW55z/J8zYcIEhfve3t4AgOPHjxc5Q0EcP34cTZs2RevWreVtlSpVwujRo3Hv3j3cunVLYf9hw4YpjEPJ7UmLi4uTt7m7u0MQhM/2xgD//znIr6v+3/T09OSvt1xaWloYNmzYZ78P+PD6rFq1Kjw9PeVt2traGDVq1Be/N1ffvn1hZGQkv5/fz/zvT/bv3r3Dixcv0Lx5cwDA1atXC/xY/37MK1euIDY2Vt62Z88eaGlpoXv37gCU9z4ZMGAAgoOD8eTJE5w6dQpPnjz55Gmlgr5ejh8/DgsLC3z99dfy/XR1dTF69Ogi5/xYUlISTp06hT59+uDNmzd48eIFXrx4gZcvX8LDwwN3797Fw4cP5XmaN2+Opk2byr+/cuXKBfo9lfva+9LrVFkCAgJQuXJlVK5cGfXr18e+ffswaNAg/PTTT6Xy+FLHQoYAAM+fP0d6enq+V4o4OjoiJycHiYmJAD6MP7C3t8+zX35tBWVra4spU6Zg06ZNMDU1hYeHB9asWVOs8TG5UlNTAXz+l9J3332HSpUqoWnTpqhZsyYmTJiAf/75p1CPY2trW6j9a9asqXC/Ro0aUFNTK/H5Le7fv//J5zl3+7993LWe+wc+d8xGYeQ+B58rKnO3f/x8Va1atUADe+/fv48aNWrkObVTmNdnQX7mpKQk+Pr6okqVKtDR0UHlypXlr4GivG579+4NNTU1+aB0QRCwb98++Zg1QHnvky5dukBPTw979uzBjh070KRJk0/+/xT09ZL7e+Hj//eCXn1WEDExMRAEATNnzpT/4c+95V7t9OzZM3mej99jBc2T+//9pdepsjRr1gyBgYE4efIkzp8/jxcvXmDbtm0FOg1GLGSoDFm2bBkiIyPx3//+F2/fvoWPjw/q1q2LBw8eFOu4uVcWfO4PmaOjI6KiorB79260bt0aBw4cQOvWrRUuBf2S4v7S+fgPwKfGWPx77orS8PGA3FyCIBT6WLl//CIjIz+5z/379/H69es8vVul+Uu9ID9znz59sHHjRvl4k4CAAPkA76Jc0m1paYk2bdpg7969AD5cNZOQkIC+ffsq7KeM94mWlhZ69eqFrVu34tChQ5/sjSlrcv9fp06disDAwHxvxflAlcve3h4VKlTA9evXi32sgjA1NUWHDh3Qvn17tGjRolDTOxALGfo/lStXhq6uLqKiovJsu3PnDtTU1GBlZQUAsLa2znPVCoB82z72pQGQTk5OmDFjBs6cOYOzZ8/i4cOHWLduXQF/ivxt374dMpkMX3311Wf3q1ixIvr27Qt/f38kJCSga9euWLBggfxKosIM3iyIu3fvKtyPiYlBTk6OfJBwbi/Ax5Pc5XdlS2GyWVtbf/J5zt1eUmrVqoVatWrhzz///OSn3dzBzv/5z3+K9BjW1taIjY3NU2gV5PVZUK9evUJQUBC+//57zJ07Fz179sRXX32lcFq2KPr27YuIiAhERUVhz5490NXVRbdu3fLsp4z3yYABA3Dt2jW8efNGYVD1xwr6evnU/3t+31tUuf+/Ghoa6NChQ7633J48a2vrPO+xgubR1dVFu3btcObMGXlPNJVdLGQIwIdPoB07dsThw4cVTm08ffoUO3fuROvWreXdrR4eHrhw4YLC7LxJSUnYsWPHFx+nYsWKAPL+cX79+jWysrIU2pycnKCmppbv5ZsFtXjxYgQEBKBv3775djPn+viSWU1NTdSpUweCICAzM/Oz2YtqzZo1Cvd//fVXAEDnzp0BfOjeNjU1xZkzZxT2++233/IcqzDZunTpgsuXL+PChQvytrS0NGzYsAE2NjaFGueTqzCXX8+aNQuvXr3C2LFj8/QuXblyBT/99BPq1asHLy+vQucAPrw+Hz58qHA57rt377Bx48YiHS8/uT02H//RXrlyZbGO6+XlBXV1dezatQv79u3Df/7zH/lzCyj3fdK2bVv8+OOPWL16NczNzT+5X0FfL126dMGjR48ULptPT0/Hhg0bCpXrc8zMzODu7o7169fj8ePHebb/+yqnLl264OLFi7h8+bLC9oL8ngKA2bNnQxAEDBo0SH56+t+uXLmCrVu3FuGnIGXj5dflzO+//y7v/v43X19fzJ8/H4GBgWjdujXGjx+PChUqYP369cjIyMCSJUvk+06bNg1//PEHvvrqK3h7e8svv65evTqSkpI+2ztQo0YNGBoaYt26ddDT00PFihXRrFkzREREYOLEiejduzdq1aqFrKwsbN++Herq6gX6g5aVlYU//vgDwIc/Wvfv38eRI0cQGRmJtm3bfvGXaceOHWFubo5WrVqhSpUquH37NlavXo2uXbvKP+E1btwYAPDDDz+gX79+0NDQQLdu3RT+0BRGfHw8PD090alTJ1y4cAF//PEHBgwYgPr168v3GTlyJBYvXoyRI0fCxcUFZ86cUZgPJ1dhsn3//ffYtWsXOnfuDB8fHxgbG2Pr1q2Ij4/HgQMHijQLcEEvvwaAgQMHIjQ0FL/88gtu3bqFgQMHwsjICFevXsXvv/8OExMT7N+/v8iT1Y0ZMwarV69G//794evrCwsLC/mstYByetb09fXh6uqKJUuWIDMzE1WrVkVAQECx5/0wMzND27ZtsXz5crx58ybPaaVTp04V633yb2pqapgxY8YX9yvo62XUqFFYvXo1Bg8ejCtXrsDCwgLbt2+Hrq5uoXJ9yZo1a9C6dWs4OTlh1KhRsLOzw9OnT3HhwgU8ePAAERERAD78ntq+fTs6deoEX19f+eXX1tbWnz21matly5ZYs2YNxo8fj9q1ayvM7BscHIwjR45g/vz5Ct8TFhaWpw34MBj+34OlScnEulyKSlfuJaCfuiUmJgqCIAhXr14VPDw8hEqVKgm6urpC27ZthfPnz+c53rVr14Q2bdoIWlpaQrVq1YRFixYJq1atEgAIT548ke/38aWxgvDhUtY6deoIFSpUkF9aGRcXJwwfPlyoUaOGoK2tLRgbGwtt27YVTp48+cWfbciQIQo/i66urmBjYyN4eXkJ+/fvl18e+m8f51q/fr3g6uoqmJiYCFpaWkKNGjWEb7/9VkhJSVH4vh9//FGoWrWqoKampnAJJz5z6SU+cfn1rVu3hK+//lrQ09MTjIyMhIkTJwpv375V+N709HRhxIgRgoGBgaCnpyf06dNHePbsWb6XOn8q28eXXwuCIMTGxgpff/21YGhoKGhrawtNmzYVjh07prBP7iXVH1/qm98lsQW9/Prf/vzzT+Grr74SjIyMBC0tLcHe3l7w8/MTnj9/nmff/C5b/fe2j19jcXFxQteuXQUdHR2hcuXKgp+fn3DgwAEBgHDx4kX5fp+6/Prflx/n+vjne/DggdCzZ0/B0NBQMDAwEHr37i08evQoz34Fvfw618aNGwUAgp6eXp7XQ3HfJ5+bpkAQPv3zF+T1IgiCcP/+fcHT01PQ1dUVTE1NBV9fX+HEiRNKvfw6N8/gwYMFc3NzQUNDQ6hatarwn//8R9i/f7/CfpGRkYKbm5ugra0tVK1aVfjxxx+FzZs3F+r5uHLlijBgwADB0tJS0NDQEIyMjIT27dsLW7duVfjd8rnfrz/++KMgCJ++/Prjn5sKRyYIRRixR5SPSZMmYf369UhNTf3kYEkisaxcuRKTJ0/GgwcPULVqVbHjEJGSsJChInn79q3CVSQvX75ErVq10KhRIwQGBoqYjCjv6/Pdu3do2LAhsrOz8z01R0TSxTEyVCQtWrSAu7s7HB0d8fTpU2zevBmvX7/GzJkzxY5GhF69eqF69epo0KABUlJS8Mcff+DOnTsFHuhJRNLBQoaKpEuXLti/fz82bNgAmUyGRo0aYfPmzXB1dRU7GhE8PDywadMm7NixA9nZ2ahTpw52796dZ/AsEUkfTy0RERGRZHEeGSIiIpIsFjJEREQkWSo/RiYnJwePHj2Cnp6e0qeYJyIiopIhCALevHkDS0vLz07UqfKFzKNHj+RrBBEREZG0JCYmolq1ap/crvKFTO708omJifK1goiIiKhse/36NaysrOR/xz9F5QuZ3NNJ+vr6LGSIiIgk5kvDQjjYl4iIiCSLhQwRERFJFgsZIiIikiyVHyNDRESlIzs7G5mZmWLHIInQ0NCAurp6sY/DQoaIiIpFEAQ8efIEycnJYkchiTE0NIS5uXmx5nljIUNERMWSW8SYmZlBV1eXk4/SFwmCgPT0dDx79gwAYGFhUeRjsZAhIqIiy87OlhcxJiYmYschCdHR0QEAPHv2DGZmZkU+zcTBvkREVGS5Y2J0dXVFTkJSlPu6Kc7YKhYyRERUbDydREWhjNcNCxkiIiKSLBYyRERERXDv3j3IZDKEh4cX+Hu2bNkCQ0ND0XOoEg72JSKiEjFiS2ipPt7moU0K/T2JiYmYPXs2Tpw4gRcvXsDCwgI9evTArFmzvjh42crKCo8fP4apqWmBH69v377o0qVLoXMqQ0xMDBYsWIDAwEA8f/4clpaWaN68Ofz8/ODi4gLgw6meQ4cOoUePHnm+Pzg4GG3btsWrV69gaGgov5/LzMwMrVu3xtKlS2FnZ1daPxZ7ZIiIqHyKi4uDi4sL7t69i127diEmJgbr1q1DUFAQWrRogaSkpE9+7/v376Gurg5zc3NUqFDwPgEdHR2YmZkpI36hhIWFoXHjxoiOjsb69etx69YtHDp0CLVr14afn1+xjh0VFYVHjx5h3759uHnzJrp164bs7GwlJf8yFjJERFQuTZgwAZqamggICICbmxuqV6+Ozp074+TJk3j48CF++OEH+b42Njb48ccfMXjwYOjr62P06NH5ntI5cuQIatasCW1tbbRt2xZbt26FTCaTTxb48amlOXPmoEGDBti+fTtsbGxgYGCAfv364c2bN/J9Tpw4gdatW8PQ0BAmJib4z3/+g9jY2AL/nIIgYOjQoahZsybOnj2Lrl27okaNGmjQoAFmz56Nw4cPF/n/EPjQE2NhYQFXV1fMmjULt27dQkxMTLGOWRgsZIiIqNxJSkrC33//jfHjx8vnM8llbm6OgQMHYs+ePRAEQd7+888/o379+rh27RpmzpyZ55jx8fH4+uuv0aNHD0RERGDMmDEKxdCnxMbG4s8//8SxY8dw7NgxhISEYPHixfLtaWlpmDJlCsLCwhAUFAQ1NTX07NkTOTk5BfpZw8PDcfPmTfj5+UFNLe+ffWWO2cn9v3z//r3SjvklHCNDlI/SPrdP/19RxjkQFdbdu3chCAIcHR3z3e7o6IhXr17h+fPn8lNB7dq1UzgNc+/ePYXvWb9+PRwcHLB06VIAgIODA27cuIEFCxZ8NktOTg62bNkCPT09AMCgQYMQFBQk/z4vLy+F/X///XdUrlwZt27dQr169Qr0swJA7dq1v7hvcTx+/Bg///wzqlatCgcHhxJ9rH9jjwwREZVb/+5x+ZLcAbGfEhUVhSZNFAvxpk2bfvG4NjY28iIG+DBdf+7U/cCHQqR///6ws7ODvr4+bGxsAAAJCQkFyl2Yn7EoqlWrhooVK8LS0hJpaWk4cOAANDU1S/Qx/409MkREVO7Y29tDJpPh9u3b6NmzZ57tt2/fhpGRESpXrixvq1ixYolk0dDQULgvk8kUTht169YN1tbW2LhxIywtLZGTk4N69eoV+PRNrVq1AAB37txBw4YNlRf8/5w9exb6+vowMzNTKMhKC3tkiIio3DExMcFXX32F3377DW/fvlXY9uTJE+zYsQN9+/Yt1MyzDg4OCAsLU2gLDS3eaeqXL18iKioKM2bMQPv27eWnvAqjQYMGqFOnDpYtW5bvuJrirlpua2uLGjVqiFLEACxkiIionFq9ejUyMjLg4eGBM2fOIDExESdOnMBXX32FqlWrfnFsy8fGjBmDO3fu4LvvvkN0dDT27t2LLVu2ACj6VPxGRkYwMTHBhg0bEBMTg1OnTmHKlCmFOoZMJoO/vz+io6PRpk0bHD9+HHFxcYiMjMSCBQvQvXt3hf3j4+MRHh6ucEtLSytS/tLAU0tERFQiyvrA7Zo1ayIsLAyzZ89Gnz59kJSUBHNzc/To0QOzZ8+GsbFxoY5na2uL/fv3w8/PD7/88gtatGiBH374AePGjYOWllaRMqqpqWH37t3w8fFBvXr14ODggFWrVsHd3b1Qx2natCnCwsKwYMECjBo1Sj75X8uWLbFy5UqFffMrlM6ePVuk/KVBJpT0KCCRvX79GgYGBkhJSYG+vr7YcUgieNWSeMr6Hz9S9O7dO8THx8PW1hba2tpixylzFixYgHXr1iExMVHsKGXS514/Bf37zR4ZIiIiJfntt9/QpEkTmJiY4J9//sHSpUsxceJEsWOpNBYyRERESnL37l3Mnz8fSUlJqF69Ovz8/DB9+nSxY6k0FjJERERKsmLFCqxYsULsGOUKr1oiIiIiyWIhQ0RERJLFQoaIiIgki4UMERERSRYLGSIiIpIsUQuZtWvXwtnZGfr6+tDX10eLFi3wv//9T7793bt3mDBhAkxMTFCpUiV4eXnh6dOnIiYmIiKiskTUQqZatWpYvHgxrly5grCwMLRr1w7du3fHzZs3AQCTJ0/G0aNHsW/fPoSEhODRo0fo1auXmJGJiIgKLDg4GDKZTL4w45YtW2BoaKiwz4YNG2BlZQU1NTWsXLkSc+bMQYMGDYr1uPfu3YNMJkN4eHixjiMFos4j061bN4X7CxYswNq1a3Hx4kVUq1YNmzdvxs6dO9GuXTsAgL+/PxwdHXHx4kU0b95cjMhERFRQO/uW7uMN2FOo3YcOHYqtW7dizJgxWLduncK2CRMm4LfffsOQIUPkCz8qQ9++fdGlSxf5/devX2PixIlYvnw5vLy8YGBggJycHHh7eyvtMT8nJiYGCxYsQGBgIJ4/fw5LS0s0b94cfn5+cHFxAfBh0clDhw6hR48eeb4/ODgYbdu2xatXr2BoaCi/n8vMzAytW7fG0qVLYWdnVyI/Q5kZI5OdnY3du3cjLS0NLVq0wJUrV5CZmYkOHTrI96lduzaqV6+OCxcufPI4GRkZeP36tcKNiIgoP1ZWVti9ezfevn0rb3v37h127tyJ6tWrK/3xdHR0YGZmJr+fkJCAzMxMdO3aFRYWFtDV1UWlSpVgYmKi9Mf+WFhYGBo3bozo6GisX78et27dwqFDh1C7dm34+fkV69hRUVF49OgR9u3bh5s3b6Jbt27Izs5WUnJFohcy169fR6VKlaClpYWxY8fi0KFDqFOnDp48eQJNTc08XXBVqlTBkydPPnm8RYsWwcDAQH6zsrIq4Z+AiIikqlGjRrCyssLBgwflbQcPHkT16tXRsGFDhX0zMjLg4+MDMzMzaGtro3Xr1ggNVVxg9vjx46hVqxZ0dHTQtm1b3Lt3T2H7v08tbdmyBU5OTgAAOzs7yGQy3Lt3L99TS5s2bYKjoyO0tbVRu3Zt/PbbbwrbL1++jIYNG0JbWxsuLi64du3aZ39uQRAwdOhQ1KxZE2fPnkXXrl1Ro0YNNGjQALNnz8bhw4e/9F/3WWZmZrCwsICrqytmzZqFW7duISYmpljH/BTRCxkHBweEh4fj0qVLGDduHIYMGYJbt24V+XjTp09HSkqK/MYVR4mI6HOGDx8Of39/+f3ff/8dw4YNy7PftGnTcODAAWzduhVXr16Fvb09PDw8kJSUBABITExEr1690K1bN4SHh2PkyJH4/vvvP/m4ffv2xcmTJwF8KEQeP36c74fvHTt2YNasWViwYAFu376NhQsXYubMmdi6dSsAIDU1Ff/5z39Qp04dXLlyBXPmzMHUqVM/+zOHh4fj5s2b8PPzg5pa3lLg406E4tDR0QEAvH//XmnH/DfR11rS1NSEvb09AKBx48YIDQ3FL7/8gr59++L9+/dITk5W+A99+vQpzM3NP3k8LS0taGlplXRsIiJSEd988w2mT5+O+/fvAwD++ecf7N69G8HBwfJ90tLSsHbtWmzZsgWdO3cGAGzcuBGBgYHYvHkzvv32W6xduxY1atTAsmXLAHz4oH79+nX89NNP+T6ujo6O/BRS5cqVP/m3bfbs2Vi2bJn8YhdbW1vcunUL69evx5AhQ7Bz507k5ORg8+bN0NbWRt26dfHgwQOMGzfukz/z3bt3AXwYslGSHj9+jJ9//hlVq1aFg4NDiTyG6IXMx3JycpCRkYHGjRtDQ0MDQUFB8PLyAvDhnFtCQgJatGghckoiIlIVlStXRteuXbFlyxYIgoCuXbvC1NRUYZ/Y2FhkZmaiVatW8jYNDQ00bdoUt2/fBgDcvn0bzZo1U/i+4v69SktLQ2xsLEaMGIFRo0bJ27OysmBgYCB/XGdnZ2hraxf4cQVBKFauL6lWrRoEQUB6ejrq16+PAwcOQFNTs0QeS9RCZvr06ejcuTOqV6+ON2/eYOfOnQgODsbff/8NAwMDjBgxAlOmTIGxsTH09fXh7e2NFi1a8IolIiJSquHDh2PixIkAgDVr1oic5v9LTU0F8KH35+MiSV1dvcjHrVWrFgDgzp07ecYCKcPZs2ehr68PMzMz6OnpKf34/yZqIfPs2TMMHjwYjx8/hoGBAZydnfH333/jq6++AvBhOXQ1NTV4eXkhIyMDHh4eeQY4iWnEltAv70QlYvPQJmJHICIV0qlTJ7x//x4ymQweHh55tteoUQOampr4559/YG1tDQDIzMxEaGgoJk2aBABwdHTEkSNHFL7v4sWLxcpVpUoVWFpaIi4uDgMHDsx3H0dHR2zfvh3v3r2T98p86XEbNGiAOnXqYNmyZejbt2+ecTIfD+soLFtbW6WOs/kcUQuZzZs3f3a7trY21qxZU6aqYyIiUj3q6uryU0T59XRUrFgR48aNw7fffgtjY2NUr14dS5YsQXp6OkaMGAEAGDt2LJYtW4Zvv/0WI0eOxJUrV5QyB83cuXPh4+MDAwMDdOrUCRkZGQgLC8OrV68wZcoUDBgwAD/88ANGjRqF6dOn4969e/j5558/e0yZTAZ/f3906NABbdq0wQ8//IDatWsjNTUVR48eRUBAAEJCQuT7x8fH55lcr2bNmsX+2ZShzI2RISIiFVHICerEpq+v/9ntixcvRk5ODgYNGoQ3b97AxcUFf//9N4yMjAAA1atXx4EDBzB58mT8+uuvaNq0KRYuXIjhw4cXK9fIkSOhq6uLpUuX4ttvv0XFihXh5OQk7wmqVKkSjh49irFjx6Jhw4aoU6cOfvrpJ/n40k9p2rQpwsLCsGDBAowaNQovXryAhYUFWrZsiZUrVyrsO2XKlDzff/bs2WL9XMoiE0p6xI/IXr9+DQMDA6SkpHzxRVpYPLUknpI+tcTnVjw8bSgt7969Q3x8PGxtbRUGmxIVxOdePwX9+y36PDJERERERcVChoiIiCSLhQwRERFJFgsZIiIikiwWMkREVGwqft0IlRBlvG5YyBARUZFpaGgAANLT00VOQlKU+7rJfR0VBeeRISKiIlNXV4ehoSGePXsGANDV1YVMJhM5FZV1ueswPXv2DIaGhsVaboGFDBERFUvuqs25xQxRQRkaGn5y1e+CYiFDRETFIpPJYGFhATMzM2RmZoodhyRCQ0OjWD0xuVjIEBGRUqirqyvlDxNRYXCwLxEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSVUHsAERERMU1Ykuo2BHKrc1Dm4j6+KL2yCxatAhNmjSBnp4ezMzM0KNHD0RFRSns4+7uDplMpnAbO3asSImJiIioLBG1kAkJCcGECRNw8eJFBAYGIjMzEx07dkRaWprCfqNGjcLjx4/ltyVLloiUmIiIiMoSUU8tnThxQuH+li1bYGZmhitXrsDV1VXerqurC3Nz89KOR0RERGVcmRrsm5KSAgAwNjZWaN+xYwdMTU1Rr149TJ8+Henp6Z88RkZGBl6/fq1wIyIiItVUZgb75uTkYNKkSWjVqhXq1asnbx8wYACsra1haWmJyMhIfPfdd4iKisLBgwfzPc6iRYswd+7c0opNREREIiozhcyECRNw48YNnDt3TqF99OjR8q+dnJxgYWGB9u3bIzY2FjVq1MhznOnTp2PKlCny+69fv4aVlVXJBSciIiLRlIlCZuLEiTh27BjOnDmDatWqfXbfZs2aAQBiYmLyLWS0tLSgpaVVIjmJiIiobBG1kBEEAd7e3jh06BCCg4Nha2v7xe8JDw8HAFhYWJRwOiIiIirrRC1kJkyYgJ07d+Lw4cPQ09PDkydPAAAGBgbQ0dFBbGwsdu7ciS5dusDExASRkZGYPHkyXF1d4ezsLGZ0IiIiKgNELWTWrl0L4MOkd//m7++PoUOHQlNTEydPnsTKlSuRlpYGKysreHl5YcaMGSKkJSIiorJG9FNLn2NlZYWQkJBSSkNERERSU6bmkSEiIiIqDBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSVUHsAEREpWpnX7ETlF8D9oidgFQQe2SIiIhIsljIEBERkWSxkCEiIiLJYiFDREREksVChoiIiCSLhQwRERFJFgsZIiIikiwWMkRERCRZLGSIiIhIsljIEBERkWSxkCEiIiLJYiFDREREksVChoiIiCSLhQwRERFJFgsZIiIikiwWMkRERCRZLGSIiIhIsljIEBERkWSxkCEiIiLJYiFDREREksVChoiIiCSLhQwRERFJVpELmffv3yMqKgpZWVlFfvBFixahSZMm0NPTg5mZGXr06IGoqCiFfd69e4cJEybAxMQElSpVgpeXF54+fVrkxyQiIiLVUehCJj09HSNGjICuri7q1q2LhIQEAIC3tzcWL15cqGOFhIRgwoQJuHjxIgIDA5GZmYmOHTsiLS1Nvs/kyZNx9OhR7Nu3DyEhIXj06BF69epV2NhERESkggpdyEyfPh0REREIDg6Gtra2vL1Dhw7Ys2dPoY514sQJDB06FHXr1kX9+vWxZcsWJCQk4MqVKwCAlJQUbN68GcuXL0e7du3QuHFj+Pv74/z587h48WJhoxMREZGKqVDYb/jzzz+xZ88eNG/eHDKZTN5et25dxMbGFitMSkoKAMDY2BgAcOXKFWRmZqJDhw7yfWrXro3q1avjwoULaN68eZ5jZGRkICMjQ37/9evXxcpEREREZVehe2SeP38OMzOzPO1paWkKhU1h5eTkYNKkSWjVqhXq1asHAHjy5Ak0NTVhaGiosG+VKlXw5MmTfI+zaNEiGBgYyG9WVlZFzkRERERlW6ELGRcXF/z111/y+7nFy6ZNm9CiRYsiB5kwYQJu3LiB3bt3F/kYwIdTXykpKfJbYmJisY5HREREZVehTy0tXLgQnTt3xq1bt5CVlYVffvkFt27dwvnz5xESElKkEBMnTsSxY8dw5swZVKtWTd5ubm6O9+/fIzk5WaFX5unTpzA3N8/3WFpaWtDS0ipSjsLyfjqjVB6H8vO32AGIiKgMKHSPTOvWrREeHo6srCw4OTkhICAAZmZmuHDhAho3blyoYwmCgIkTJ+LQoUM4deoUbG1tFbY3btwYGhoaCAoKkrdFRUUhISGhWL0/REREpBoK3SMDADVq1MDGjRuL/eATJkzAzp07cfjwYejp6cnHvRgYGEBHRwcGBgYYMWIEpkyZAmNjY+jr68Pb2xstWrTId6AvERERlS+FLmRy5435lOrVqxf4WGvXrgUAuLu7K7T7+/tj6NChAIAVK1ZATU0NXl5eyMjIgIeHB3777bdCZSYiIiLVVOhCxsbG5rNXJ2VnZxf4WIIgfHEfbW1trFmzBmvWrCnwcYmIiKh8KHQhc+3aNYX7mZmZuHbtGpYvX44FCxYoLRgRERHRlxS6kKlfv36eNhcXF1haWmLp0qVcPoCIiIhKjdJWv3ZwcEBoaKiyDkdERET0RYXukfl4yn9BEPD48WPMmTMHNWvWVFowIiIioi8pdCFjaGiYZ7CvIAiwsrIq9qy8RERERIVR6ELm9OnTCvfV1NRQuXJl2Nvbo0KFIk1LQ0RERFQkha483NzcSiIHERERUaEVqJA5cuRIgQ/o6elZ5DBEREREhVGgQqZHjx4FOphMJivUhHhERERExVGgQiYnJ6ekcxAREREVmtLmkSEiIiIqbUW6zCgtLQ0hISFISEjA+/fvFbb5+PgoJRgRERHRlxRpraUuXbogPT0daWlpMDY2xosXL6CrqwszMzMWMkRERFRqCn1qafLkyejWrRtevXoFHR0dXLx4Effv30fjxo3x888/l0RGIiIionwVupAJDw+Hn58f1NTUoK6ujoyMDFhZWWHJkiX473//WxIZiYiIiPJV6EJGQ0MDamofvs3MzAwJCQkAAAMDAyQmJio3HREREdFnFHqMTMOGDREaGoqaNWvCzc0Ns2bNwosXL7B9+3bUq1evJDISERF9lvfTGWJHKMf+FvXRC9wjkzvR3cKFC2FhYQEAWLBgAYyMjDBu3Dg8f/4cGzZsKJmURERERPkocI9M1apVMXToUAwfPhwuLi4APpxaOnHiRImFIyIiIvqcAvfITJgwAfv374ejoyPatGmDLVu2ID09vSSzEREREX1WgQuZmTNnIiYmBkFBQbCzs8PEiRNhYWGBUaNG4dKlSyWZkYiIiChfhb5qyd3dHVu3bsWTJ0+wbNky3L59Gy1atEDdunWxfPnykshIRERElK8ir7VUqVIljBw5EufOncPRo0fx5MkTfPvtt8rMRkRERPRZRS5k0tPTsWXLFri5ucHT0xMmJiZYsGCBMrMRERERfVah55E5f/48fv/9d+zbtw9ZWVn4+uuv8eOPP8LV1bUk8hERERF9UoELmSVLlsDf3x/R0dFwcXHB0qVL0b9/f+jp6ZVkPiIiIqJPKnAhs3TpUnzzzTfYt28fZ/AlIiKiMqHAhcyjR4+goaFRklmIiIiICqXAg31ZxBAREVFZU+SrloiIiIjExkKGiIiIJIuFDBEREUlWoeeRuXr1KjQ0NODk5AQAOHz4MPz9/VGnTh3MmTMHmpqaSg9JVNq8n84QO0I59rfYAYhIQgrdIzNmzBhER0cDAOLi4tCvXz/o6upi3759mDZtmtIDEhEREX1KoQuZ6OhoNGjQAACwb98+uLq6YufOndiyZQsOHDig7HxEREREn1ToQkYQBOTk5AAATp48iS5dugAArKys8OLFC+WmIyIiIvqMQhcyLi4umD9/PrZv346QkBB07doVABAfH48qVaooPSARERHRpxS6kFmxYgWuXr2KiRMn4ocffoC9vT0AYP/+/WjZsqXSAxIRERF9SqGvWqpfvz6uX7+ep33p0qWoUKHQhyMiIiIqskL3yNjZ2eHly5d52t+9e4datWopJRQRERFRQRS6kLl37x6ys7PztGdkZODBgwdKCUVERERUEAU+F3TkyBH513///TcMDAzk97OzsxEUFARbW1vlpiMiIiL6jAIXMj169AAAyGQyDBkyRGGbhoYGbGxssGzZMqWGIyIiIvqcAhcyuXPH2NraIjQ0FKampiUWioiIiKggCj1GJj4+XmlFzJkzZ9CtWzdYWlpCJpPhzz//VNg+dOhQyGQyhVunTp2U8thEREQkfQXqkVm1ahVGjx4NbW1trFq16rP7+vj4FPjB09LSUL9+fQwfPhy9evXKd59OnTrB399ffl9LS6vAxyciIiLVVqBCZsWKFRg4cCC0tbWxYsWKT+4nk8kKVch07twZnTt3/uw+WlpaMDc3L/AxiYiIqPwoUCETHx+f79elITg4GGZmZjAyMkK7du0wf/58mJiYfHL/jIwMZGRkyO+/fv26NGISERGRCAo9RqY0derUCdu2bUNQUBB++uknhISEoHPnzvnOY5Nr0aJFMDAwkN+srKxKMTERERGVpkKvKTBlypR822UyGbS1tWFvb4/u3bvD2Ni42OH69esn/9rJyQnOzs6oUaMGgoOD0b59+3y/Z/r06QoZX79+zWKGiIhIRRW6kLl27RquXr2K7OxsODg4AACio6Ohrq6O2rVr47fffoOfnx/OnTuHOnXqKDWsnZ0dTE1NERMT88lCRktLiwOCiYiIyolCn1rq3r07OnTogEePHuHKlSu4cuUKHjx4gK+++gr9+/fHw4cP4erqismTJys97IMHD/Dy5UtYWFgo/dhEREQkPYXukVm6dCkCAwOhr68vbzMwMMCcOXPQsWNH+Pr6YtasWejYseMXj5WamoqYmBj5/fj4eISHh8PY2BjGxsaYO3cuvLy8YG5ujtjYWEybNg329vbw8PAobGwiIiJSQYXukUlJScGzZ8/ytD9//lx+hZChoSHev3//xWOFhYWhYcOGaNiwIYAP428aNmyIWbNmQV1dHZGRkfD09EStWrUwYsQING7cGGfPnuWpIyIiIgJQhB6Z7t27Y/jw4Vi2bBmaNGkCAAgNDcXUqVPl6zFdvnwZtWrV+uKx3N3dIQjCJ7f//fffhY1HRERE5UihC5n169dj8uTJ6NevH7Kysj4cpEIFDBkyRD5ZXu3atbFp0yblJiUiIiL6SKELmUqVKmHjxo1YsWIF4uLiAHy4mqhSpUryfRo0aKC0gEREyhSemCx2hHKrgdgBSCUVupDJValSJflcMf8uYoiIiIhKS6EH++bk5GDevHkwMDCAtbU1rK2tYWhoiB9//BE5OTklkZGIiIgoX4Xukfnhhx+wefNmLF68GK1atQIAnDt3DnPmzMG7d++wYMECpYckIiIiyk+hC5mtW7di06ZN8PT0lLc5OzujatWqGD9+PAsZIiIiKjWFPrWUlJSE2rVr52mvXbs2kpKSlBKKiIiIqCAKXcjUr18fq1evztO+evVq1K9fXymhiIiIiAqi0KeWlixZgq5du+LkyZNo0aIFAODChQtITEzE8ePHlR6QiIiI6FMK3SPj5uaG6Oho9OzZE8nJyUhOTkavXr0QFRWFNm3alERGIiIionwVaR4ZS0vLPIN6Hzx4gNGjR2PDhg1KCUZERET0JYXukfmUly9fYvPmzco6HBEREdEXKa2QISIiIiptLGSIiIhIsljIEBERkWQVeLBvr169Prs9OTm5uFmIiIiICqXAhYyBgcEXtw8ePLjYgYiIiIgKqsCFjL+/f0nmICIiIio0jpEhIiIiyWIhQ0RERJLFQoaIiIgki4UMERERSRYLGSIiIpIsFjJEREQkWSxkiIiISLJYyBAREZFksZAhIiIiyWIhQ0RERJLFQoaIiIgki4UMERERSRYLGSIiIpIsFjJEREQkWSxkiIiISLJYyBAREZFksZAhIiIiyWIhQ0RERJLFQoaIiIgki4UMERERSRYLGSIiIpIsFjJEREQkWSxkiIiISLJYyBAREZFksZAhIiIiyWIhQ0RERJLFQoaIiIgkS9RC5syZM+jWrRssLS0hk8nw559/KmwXBAGzZs2ChYUFdHR00KFDB9y9e1ecsERERFTmiFrIpKWloX79+lizZk2+25csWYJVq1Zh3bp1uHTpEipWrAgPDw+8e/eulJMSERFRWVRBzAfv3LkzOnfunO82QRCwcuVKzJgxA927dwcAbNu2DVWqVMGff/6Jfv36lWZUIiIiKoPK7BiZ+Ph4PHnyBB06dJC3GRgYoFmzZrhw4cInvy8jIwOvX79WuBEREZFqKrOFzJMnTwAAVapUUWivUqWKfFt+Fi1aBAMDA/nNysqqRHMSERGReMpsIVNU06dPR0pKivyWmJgodiQiIiIqIWW2kDE3NwcAPH36VKH96dOn8m350dLSgr6+vsKNiIiIVFOZLWRsbW1hbm6OoKAgedvr169x6dIltGjRQsRkREREVFaIetVSamoqYmJi5Pfj4+MRHh4OY2NjVK9eHZMmTcL8+fNRs2ZN2NraYubMmbC0tESPHj3EC01ERERlhqiFTFhYGNq2bSu/P2XKFADAkCFDsGXLFkybNg1paWkYPXo0kpOT0bp1a5w4cQLa2tpiRSYiIqIyRNRCxt3dHYIgfHK7TCbDvHnzMG/evFJMRURERFJRZsfIEBEREX0JCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyWMgQERGRZLGQISIiIsliIUNERESSxUKGiIiIJIuFDBEREUkWCxkiIiKSLBYyREREJFksZIiIiEiyynQhM2fOHMhkMoVb7dq1xY5FREREZUQFsQN8Sd26dXHy5En5/QoVynxkIiIiKiVlviqoUKECzM3NxY5BREREZVCZPrUEAHfv3oWlpSXs7OwwcOBAJCQkfHb/jIwMvH79WuFGREREqqlMFzLNmjXDli1bcOLECaxduxbx8fFo06YN3rx588nvWbRoEQwMDOQ3KyurUkxMREREpalMFzKdO3dG79694ezsDA8PDxw/fhzJycnYu3fvJ79n+vTpSElJkd8SExNLMTERERGVpjI/RubfDA0NUatWLcTExHxyHy0tLWhpaZViKiIiIhJLme6R+VhqaipiY2NhYWEhdhQiIiIqA8p0ITN16lSEhITg3r17OH/+PHr27Al1dXX0799f7GhERERUBpTpU0sPHjxA//798fLlS1SuXBmtW7fGxYsXUblyZbGjERERURlQpguZ3bt3ix2BiIiIyrAyfWqJiIiI6HNYyBAREZFksZAhIiIiyWIhQ0RERJLFQoaIiIgki4UMERERSRYLGSIiIpIsFjJEREQkWSxkiIiISLJYyBAREZFksZAhIiIiyWIhQ0RERJLFQoaIiIgki4UMERERSRYLGSIiIpIsFjJEREQkWSxkiIiISLJYyBAREZFksZAhIiIiyWIhQ0RERJLFQoaIiIgki4UMERERSRYLGSIiIpIsFjJEREQkWSxkiIiISLJYyBAREZFksZAhIiIiyWIhQ0RERJLFQoaIiIgki4UMERERSRYLGSIiIpIsFjJEREQkWSxkiIiISLJYyBAREZFksZAhIiIiyWIhQ0RERJLFQoaIiIgki4UMERERSRYLGSIiIpIsFjJEREQkWSxkiIiISLJYyBAREZFksZAhIiIiyWIhQ0RERJIliUJmzZo1sLGxgba2Npo1a4bLly+LHYmIiIjKgDJfyOzZswdTpkzB7NmzcfXqVdSvXx8eHh549uyZ2NGIiIhIZGW+kFm+fDlGjRqFYcOGoU6dOli3bh10dXXx+++/ix2NiIiIRFamC5n379/jypUr6NChg7xNTU0NHTp0wIULF0RMRkRERGVBBbEDfM6LFy+QnZ2NKlWqKLRXqVIFd+7cyfd7MjIykJGRIb+fkpICAHj9+rXS86W+y1L6MalgSuL5/Dc+t+Lhc6u6SvK55fMqnpJ6XnOPKwjCZ/cr04VMUSxatAhz587N025lZSVCGioxcwzETkAlhc+t6uJzq5pK+Hl98+YNDAw+/RhlupAxNTWFuro6nj59qtD+9OlTmJub5/s906dPx5QpU+T3c3JykJSUBBMTE8hkshLNKyWvX7+GlZUVEhMToa+vL3YcUiI+t6qJz6vq4nObP0EQ8ObNG1haWn52vzJdyGhqaqJx48YICgpCjx49AHwoTIKCgjBx4sR8v0dLSwtaWloKbYaGhiWcVLr09fX5xlFRfG5VE59X1cXnNq/P9cTkKtOFDABMmTIFQ4YMgYuLC5o2bYqVK1ciLS0Nw4YNEzsaERERiazMFzJ9+/bF8+fPMWvWLDx58gQNGjTAiRMn8gwAJiIiovKnzBcyADBx4sRPnkqiotHS0sLs2bPznIYj6eNzq5r4vKouPrfFIxO+dF0TERERURlVpifEIyIiIvocFjJEREQkWSxkiIiISLJYyJQTZ86cQVZW3im8s7KycObMGRESUUlKTk4WOwIRUangYN9yQl1dHY8fP4aZmZlC+8uXL2FmZobs7GyRklFx/fTTT7CxsUHfvn0BAH369MGBAwdgbm6O48ePo379+iInpKKYPXs2hg8fDmtra7GjUAm4e/cuTp8+jWfPniEnJ0dh26xZs0RKJU0sZMoJNTU1PH36FJUrV1Zoj46OhouLS4kv1Eclx9bWFjt27EDLli0RGBiIPn36YM+ePdi7dy8SEhIQEBAgdkQqggYNGuDGjRtwc3PDiBEj4OXlxctzVcTGjRsxbtw4mJqawtzcXGH5HJlMhqtXr4qYTnpYyKi4Xr16AQAOHz6MTp06KfwizM7ORmRkJBwcHHDixAmxIlIx6ejoIDo6GlZWVvD19cW7d++wfv16REdHo1mzZnj16pXYEamIrl27Bn9/f+zatQtZWVno168fhg8fjiZNmogdjYrB2toa48ePx3fffSd2FJXAMTIqzsDAAAYGBhAEAXp6evL7BgYGMDc3x+jRo/HHH3+IHZOKwcjICImJiQCAEydOoEOHDgA+LLjGU4bS1rBhQ6xatQqPHj3C5s2b8eDBA7Rq1QrOzs745ZdfkJKSInZEKoJXr16hd+/eYsdQGZKY2ZeKzt/fHwBgY2ODqVOnomLFiiInImXr1asXBgwYgJo1a+Lly5fo3LkzgA+f5u3t7UVOR8ogCAIyMzPx/v17CIIAIyMjrF69GjNnzsTGjRvl46NIGnr37o2AgACMHTtW7CgqgaeWiCQuMzMTv/zyCxITEzF06FA0bNgQALBixQro6elh5MiRIiekorpy5Yr81JKWlhYGDx6MkSNHygvUX3/9FfPnz8fTp09FTkpfsmrVKvnXaWlpWL58Obp27QonJydoaGgo7Ovj41Pa8SSNhUw5sn//fvkA0Pfv3yts4+AyorLFyckJd+7cQceOHTFq1Ch069YN6urqCvu8ePECZmZmea56obLH1ta2QPvJZDLExcWVcBrVwlNL5cSqVavwww8/YOjQoTh8+DCGDRuG2NhYhIaGYsKECWLHo2Lavn071q9fj7i4OFy4cAHW1tZYuXIlbG1t0b17d7HjURH06dMHw4cPR9WqVT+5j6mpKYsYiYiPjxc7gsriYN9y4rfffsOGDRvw66+/QlNTE9OmTUNgYCB8fHw4YFDi1q5diylTpqBz585ITk6WD/A1NDTEypUrxQ1HRTZz5szPFjFE9AFPLZUTurq6uH37NqytrWFmZobAwEDUr18fd+/eRfPmzfHy5UuxI1IR1alTBwsXLkSPHj2gp6eHiIgI2NnZ4caNG3B3d8eLFy/EjkhF9ODBAxw5ciTf08HLly8XKRUVl5eXF5o2bZrn8uslS5YgNDQU+/btEymZNPHUUjlhbm6OpKQkWFtbo3r16rh48SLq16+P+Ph4sJaVtvj4ePkA33/T0tJCWlqaCIlIGYKCguDp6Qk7OzvcuXMH9erVw7179yAIAho1aiR2PCqGM2fOYM6cOXnaO3fujGXLlpV+IInjqaVyol27djhy5AgAYNiwYZg8eTK++uor9O3bFz179hQ5HRWHra0twsPD87SfOHECjo6OpR+IlGL69OmYOnUqrl+/Dm1tbRw4cACJiYlwc3PjHCQSl5qaCk1NzTztGhoanGW9CNgjU05s2LBBPihwwoQJMDExwfnz5+Hp6YkxY8aInI6KY8qUKZgwYQLevXsHQRBw+fJl7Nq1C4sWLcKmTZvEjkdFdPv2bezatQsAUKFCBbx9+xaVKlXCvHnz0L17d4wbN07khFRUTk5O2LNnT541lXbv3o06deqIlEq6WMiUE2pqalBT+/8dcP369UO/fv1ETETKMnLkSOjo6GDGjBlIT0/HgAEDYGlpiV9++YXPsYRVrFhRPi7GwsICsbGxqFu3LgBw3JPEzZw5E7169UJsbCzatWsH4MOpxF27dnF8TBFwsG854e/vj0qVKuXpkt63bx/S09MxZMgQkZKRMqWnpyM1NTXPKuckPT169EDXrl0xatQoTJ06FYcPH8bQoUNx8OBBGBkZ4eTJk2JHpGL466+/sHDhQoSHh0NHRwfOzs6YPXs23NzcxI4mOSxkyolatWph/fr1aNu2rUJ7SEgIRo8ejaioKJGSUXHNnj0bw4cPh7W1tdhRSIni4uKQmpoKZ2dnpKWlwc/PD+fPn0fNmjWxfPlyPt9E/4eFTDmhra2NO3fuwMbGRqH93r17cHR0xNu3b8UJRsXWoEED3LhxA25ubhgxYgS8vLwUVjknIlJlvGqpnDAzM0NkZGSe9oiICJiYmIiQiJQlPDwcoaGhqFu3Lnx9fWFubo5x48YhNDRU7GhUDLNmzcLp06fx7t07saOQkqmpqUFdXf2TNyocDvYtJ/r37w8fHx/o6enB1dUVwIfTSr6+vhwQqgIaNmyIhg0bYtmyZTh69Cj8/f3RqlUr1K5dGyNGjMDQoUNhYGAgdkwqhAsXLmD58uXIyspCkyZN4ObmBnd3d7Rq1Qo6Ojpix6NiOHTokML9zMxMXLt2DVu3bsXcuXNFSiVdPLVUTrx//x6DBg3Cvn37UKHCh/o1JycHgwcPxrp16/Kd04Ck5/379zh06BB+//13nDp1Ci1btsSjR4/w9OlTbNy4EX379hU7IhVCVlYWLl26hDNnziAkJATnz59HRkYGmjRpgnPnzokdj5Rs586d2LNnDw4fPix2FElhIVPO3L17Vz5K3snJiQMGVcSVK1fg7++PXbt2QUtLC4MHD8bIkSNhb28PAPj1118xf/58PH36VOSkVBTR0dE4ffo0Tp48iT///BMGBga8BFsFxcXFwdnZGampqWJHkRQWMkQS5+TkhDt37qBjx44YNWoUunXrluc8+4sXL2BmZsaVkiVkw4YNCA4ORkhICDIyMtCmTRu4u7vD3d0dzs7OkMlkYkckJXr79i2mT5+O//3vf7yKtJBYyBBJ3I8//ojhw4dzpWQVo6amhsqVK8PPzw/jx49HpUqVxI5ESmJkZKRQiAqCgDdv3kBXVxd//PEHPD09RUwnPSxkiFTE+/fvER8fjxo1asjHQZF0/fnnnzhz5gyCg4Nx+/ZtNGzYUN4j07p1a+jq6oodkYpo69atCvdzi9ZmzZrByMhIpFTSxUKGSOLevn2LiRMnyn85RkdHw87ODt7e3qhatSq+//57kRNScaWkpODs2bPYt28fdu3aBTU1NV6WTfR/+LGtnMjMzISGhka+2168eAFTU9NSTkTK8v333yMiIgLBwcHo1KmTvL1Dhw6YM2cOCxkJe/nyJUJCQhAcHIzg4GDcvHkTRkZGaNOmjdjRqJiSk5OxefNm3L59GwBQt25dDB8+nNMkFAF7ZMoJLy8v7N+/P88AwadPn6J9+/a4ceOGSMmouKytrbFnzx40b94cenp6iIiIgJ2dHWJiYtCoUSO8fv1a7IhUBE5OTrh9+zaMjIzg6uoKd3d3uLm5wdnZWexoVExhYWHw8PCAjo4OmjZtCgAIDQ3F27dvERAQgEaNGomcUFrYI1NOJCQkYOTIkdi8ebO87cmTJ2jbtq18RV2SpufPn+e7SGRaWhqvbJGwsWPHws3NDfXq1RM7CinZ5MmT4enpiY0bN8rHs2VlZWHkyJGYNGkSzpw5I3JCaeESBeXE8ePHcf78eUyZMgUA8OjRI7i5ucHJyQl79+4VOR0Vh4uLC/766y/5/dziZdOmTWjRooVYsaiYJkyYwCJGRYWFheG7775TGJRfoUIFTJs2DWFhYSImkyb2yJQTlStXRkBAAFq3bg0AOHbsGBo1aoQdO3ZATY31rJQtXLgQnTt3xq1bt5CVlYVffvkFt27dwvnz5xESEiJ2PCqi7OxsbNmyBUFBQXj27FmeOYBOnTolUjIqLn19fSQkJKB27doK7YmJidDT0xMplXTxL1g5YmVlhcDAQOzYsQNNmzbFrl27uECZCmjdujXCw8ORlZUFJycnBAQEwMzMDBcuXEDjxo3FjkdF5OvrC19fX2RnZ6NevXqoX7++wo2kq2/fvhgxYgT27NmDxMREJCYmYvfu3Rg5ciT69+8vdjzJ4WBfFfbxpEu50tPToaWlpVDEJCUllWY0IvoCU1NTbNu2DV26dBE7CinZ+/fv8e2332LdunXIysoCAGhoaGDcuHFYvHgxtLS0RE4oLSxkVNjHky59zpAhQ0owCREVlqWlJYKDg1GrVi2xo1AJSU9PR2xsLACgRo0anOSwiFjIEBGVQcuWLUNcXBxWr17Nq89UTEpKCrKzs2FsbKzQnpSUhAoVKkBfX1+kZNLEQqacOH78ONTV1eHh4aHQHhAQgOzsbHTu3FmkZESUq1evXgr3T506BWNjY9StWzfPhJYHDx4szWikRJ07d0a3bt0wfvx4hfZ169bhyJEjOH78uEjJpImDfcuJ77//HtnZ2Xnac3JyOPMrURlhYGCgcOvZsyfc3NxgamqaZxtJ16VLl9C2bds87e7u7rh06ZIIiaSNl1+XE3fv3kWdOnXytNeuXRsxMTEiJCKij/n7+4sdgUpBRkaGfJDvv2VmZuLt27ciJJI29siUEwYGBoiLi8vTHhMTg4oVK4qQiJQlLS0NM2fORMuWLWFvbw87OzuFGxGVLU2bNsWGDRvytK9bt45TJhQBe2TKie7du2PSpEk4dOgQatSoAeBDEePn5wdPT0+R01FxjBw5EiEhIRg0aBAsLCw4MFRFNGzYMN/nUiaTQVtbG/b29hg6dGi+pyiobJs/fz46dOiAiIgItG/fHgAQFBSE0NBQBAQEiJxOejjYt5xISUlBp06dEBYWhmrVqgEAHjx4gDZt2uDgwYMwNDQUNyAVmaGhIf766y+0atVK7CikRNOnT8fatWvh5OSksLBgZGQkhg4dilu3biEoKAgHDx5E9+7dRU5LhRUeHo6lS5ciPDwcOjo6cHZ2xvTp01GzZk2xo0kOC5lyRBAEBAYGIiIiQv7GcXV1FTsWFZOtrS2OHz8OR0dHsaOQEo0aNQrVq1fHzJkzFdrnz5+P+/fvY+PGjZg9ezb++usvrs9D5RoLGSKJ++OPP3D48GFs3bqVE2qpEAMDA1y5cgX29vYK7TExMWjcuDFSUlJw584dNGnSBG/evBEpJZH4OEamHElLS0NISAgSEhLw/v17hW0+Pj4ipaLiWrZsGWJjY1GlShXY2NjkmW/k6tWrIiWj4tDW1sb58+fzFDLnz5+HtrY2gA/TJ+R+TVResZApJ65du4YuXbogPT0daWlpMDY2xosXL6CrqwszMzMWMhLWo0cPsSNQCfD29sbYsWNx5coVNGnSBMCHMTKbNm3Cf//7XwDA33//jQYNGoiYkkh8PLVUTri7u6NWrVpYt24dDAwMEBERAQ0NDXzzzTfw9fXNM6MoEYlvx44dWL16NaKiogAADg4O8Pb2xoABAwAAb9++lV/FRFResZApJwwNDXHp0iU4ODjA0NAQFy5cgKOjIy5duoQhQ4bgzp07YkekYkhOTsb+/fsRGxuLb7/9FsbGxrh69SqqVKmCqlWrih2PiPIRExOD2NhYuLq6QkdHB4IgcPqEIuCEeOWEhoYG1NQ+PN1mZmZISEgA8GFAYWJiopjRqJgiIyNRq1Yt/PTTT/j555+RnJwM4MNaPNOnTxc3HBHl8fLlS3To0AG1atVCly5d8PjxYwDAiBEj4OfnJ3I66WEhU040bNgQoaGhAAA3NzfMmjULO3bswKRJk1CvXj2R01FxTJkyBUOHDsXdu3cVTjF06dIFZ86cETEZFVbu2DUAMDIygrGx8SdvJF2TJ09GhQoVkJCQoHClYd++fXHixAkRk0kTB/uWEwsXLpRforlgwQIMHjwY48aNQ82aNfH777+LnI6KIzQ0FOvXr8/TXrVqVTx58kSERFRUK1asgJ6eHgBg5cqV4oahEhMQEIC///5bPjlprpo1a+L+/fsipZIuFjLlhIuLi/xrMzMzVv0qREtLC69fv87THh0djcqVK4uQiIpqyJAh+X5NqiUtLS3fOZ+SkpKgpaUlQiJpYyFDJHGenp6YN28e9u7dC+DDWjwJCQn47rvv4OXlJXI6Ko6cnBzExMTg2bNnyMnJUdjGWbmlq02bNti2bRt+/PFHAB/eszk5OViyZAnXzioCXrWkwj616Fx+OGmadKWkpODrr79GWFgY3rx5A0tLSzx58gQtWrTA8ePHubq5RF28eBEDBgzA/fv38fGvaZlMhuzsbJGSUXHduHED7du3R6NGjXDq1Cl4enri5s2bSEpKwj///CNf2JcKhj0yKowTpZUPBgYGCAwMxLlz5xAZGYnU1FQ0atQIHTp0EDsaFcPYsWPh4uKCv/76i6uaq5h69eohOjoaq1evhp6eHlJTU9GrVy9MmDABFhYWYseTHPbIEKmQd+/eQUtLi3/0VEDFihURERGRZ4kCIlLEHplyJiwsDLdv3wYA1KlTB40bNxY5ERVXTk4OFixYgHXr1uHp06eIjo6GnZ0dZs6cCRsbG4wYMULsiFQEzZo1Q0xMDAsZFZWcnIzLly/nO/5p8ODBIqWSJhYy5cSDBw/Qv39//PPPPzA0NATw4Y3UsmVL7N69O89lgCQd8+fPx9atW7FkyRKMGjVK3l6vXj2sXLmShYyEREZGyr/29vaGn58fnjx5AicnpzyLgTo7O5d2PFKSo0ePYuDAgUhNTYW+vr5CD6pMJmMhU0g8tVROdOrUCcnJydi6dSscHBwAAFFRURg2bBj09fV5ObaE2dvbY/369Wjfvj309PQQEREBOzs73LlzBy1atMCrV6/EjkgFpKamBplMlmdwb67cbRzsK225M/ouXLgw38uwqXDYI1NOhISE4Pz58/IiBviwAN2vv/6KNm3aiJiMiuvhw4f5nn7IyclBZmamCImoqOLj48WOQKXg4cOH8PHxYRGjJCxkygkrK6t8/6hlZ2fD0tJShESkLHXq1MHZs2dhbW2t0L5//340bNhQpFRUFB8/h6SaPDw8EBYWBjs7O7GjqAQWMuXE0qVL4e3tjTVr1shn+Q0LC4Ovry9+/vlnkdNRccyaNQtDhgzBw4cPkZOTg4MHDyIqKgrbtm3DsWPHxI5HRACOHDki/7pr16749ttvcevWrXzHP3l6epZ2PEnjGBkVZmRkpDCILC0tDVlZWahQ4UP9mvt1xYoVkZSUJFZMUoKzZ89i3rx5iIiIkM8jM2vWLHTs2FHsaESED+OfCoLjnwqPPTIqjIvOqb6srCwsXLgQw4cPR2BgoNhxiOgTPr7EmpSHPTJEElepUiXcuHEDNjY2YkchogLYtm0b+vbtm2eByPfv32P37t28/LqQCtbXRURlVvv27RESEiJ2DCoBycnJ2LRpE6ZPny4//Xv16lU8fPhQ5GRUHMOGDUNKSkqe9jdv3mDYsGEiJJI2nloikrjOnTvj+++/x/Xr19G4ceM8i0Ry4KA0RUZGokOHDjAwMMC9e/cwatQoGBsb4+DBg0hISMC2bdvEjkhFlDsX0McePHgAAwMDERJJG08tEUnc5wYRcuCgdHXo0AGNGjXCkiVLFCY6PH/+PAYMGIB79+6JHZEKqWHDhpDJZIiIiEDdunXlF14AH6bCiI+PR6dOnbB3714RU0oPe2SIJI6DCFVTaGgo1q9fn6e9atWqePLkiQiJqLh69OgBAAgPD4eHhwcqVaok36apqQkbGxt4eXmJlE66WMgQEZVBWlpaeP36dZ726OhoVK5cWYREVFyzZ88GANjY2KBv377Q1tYWOZFq4KmlciQsLAx79+5FQkIC3r9/r7Dt4MGDIqWi4lq1alW+7TKZDNra2rC3t4erqyvU1dVLORkVx8iRI/Hy5Uvs3bsXxsbGiIyMhLq6Onr06AFXV1dOr0D0f1jIlBO5l/R5eHggICAAHTt2RHR0NJ4+fYqePXvC399f7IhURLa2tnj+/DnS09NhZGQEAHj16hV0dXVRqVIlPHv2DHZ2djh9+jSsrKxETksFlZKSgq+//hphYWF48+YNLC0t8eTJE7Ro0QLHjx/PM6ibqLxiIVNOODs7Y8yYMZgwYYJ84KCtrS3GjBkDCwsLzJ07V+yIVES7du3Chg0bsGnTJtSoUQMAEBMTgzFjxmD06NFo1aoV+vXrB3Nzc+zfv1/ktFRY586dQ2RkpHzG5g4dOogdiahMYSFTTlSsWBE3b96EjY0NTExMEBwcDCcnJ9y+fRvt2rXD48ePxY5IRVSjRg0cOHAADRo0UGi/du0avLy8EBcXh/Pnz8PLy4vPMxGpHA72LSeMjIzw5s0bAB+uerhx4wacnJyQnJyM9PR0kdNRcTx+/BhZWVl52rOysuRXt1haWsqff5KOoKAgBAUF4dmzZ3muTvv9999FSkVUtrCQKSdcXV0RGBgIJycn9O7dG76+vjh16hQCAwPRvn17seNRMbRt2xZjxozBpk2b0LBhQwAfemPGjRuHdu3aAQCuX78OW1tbMWNSIc2dOxfz5s2Di4sLLCws8p1AjaQpOzsbW7Zs+WSReurUKZGSSRNPLZUTSUlJePfuHSwtLZGTk4MlS5bg/PnzqFmzJmbMmCEfJErS8+TJEwwaNAhBQUHQ0NAA8KE3pn379ti+fTuqVKmC06dPIzMzk6thS4iFhQWWLFmCQYMGiR2FlGzixInYsmULunbtmm+RumLFCpGSSRMLGSIVERUVhaioKACAg4MDHBwcRE5ExWFiYoLLly/LB3CT6jA1NcW2bdvQpUsXsaOoBBYyRERl0HfffYdKlSph5syZYkchJbO0tERwcDBq1aoldhSVwEKGiKiMmDJlivzrnJwcbN26Fc7OznB2dpafNsy1fPny0o5HSrJs2TLExcVh9erVHPukBCxkiIjKiLZt2xZ439OnT5dgEipJPXv2xOnTp2FsbIy6devmKVI503rh8KolFRYZGYl69ep9dnVkIio7WJyUD4aGhujZs6fYMVQGe2RUmLq6Oh4/fgwzMzPY2dkhNDQUJiYmYsciogIYPnw4fvnlF+jp6Sm0p6Wlwdvbm/PIEP0fFjIqzMTEBMePH0ezZs2gpqaGp0+fctVcFZWcnIzLly/nOyfF4MGDRUpFxfHvDyL/9uLFC5ibm+c7CSJJy/PnzxWuNOTv56LhqSUV5uXlBTc3N/k8BS4uLp9cATkuLq6U05GyHD16FAMHDkRqair09fUVBg/KZDIWMhLz+vVrCIIAQRDw5s0baGtry7dlZ2fj+PHjeYobkpbcXrVt27bJP3ioq6tj8ODB+PXXX6GrqytyQmlhj4yKO3HiBGJiYuDj44N58+bl6abO5evrW8rJSFlq1aqFLl26YOHChfwFqALU1NQ+eyWLTCbD3Llz8cMPP5RiKlKmMWPG4OTJk1i9ejVatWoF4MPioD4+Pvjqq6+wdu1akRNKCwuZcmLYsGFYtWrVJwsZkq6KFSvi+vXrsLOzEzsKKUFISAgEQUC7du1w4MABGBsby7dpamrC2toalpaWIiak4jI1NcX+/fvh7u6u0H769Gn06dMHz58/FyeYRPHUUjnh7+8v//rBgwcAgGrVqokVh5TIw8MDYWFhLGRUhJubGwAgPj4e1atX5zwjKig9PR1VqlTJ025mZsZFfIuAPTLlRE5ODubPn49ly5YhNTUVAKCnpwc/Pz/88MMPvERbYo4cOSL/+vnz55g3bx6GDRsGJyenPHNSeHp6lnY8IvqM9u3bw8TEBNu2bZOPgXr79i2GDBmCpKQknDx5UuSE0sJCppyYPn06Nm/ejLlz5yqck50zZw5GjRqFBQsWiJyQCqOghadMJkN2dnYJpyGiwrhx4wY8PDyQkZGB+vXrAwAiIiKgra2Nv//+G3Xr1hU5obSwkCknLC0tsW7dujyfzg8fPozx48fj4cOHIiUjIip/0tPTsWPHDty5cwcA4OjoiIEDB0JHR0fkZNLDQqac0NbWRmRkZJ5FyqKiotCgQQO8fftWpGRUXNu2bUPfvn2hpaWl0P7+/Xvs3r2bl18TkUrjwIhyon79+li9enWe9tWrV8u7Nkmahg0bhpSUlDztb968wbBhw0RIRERUenjVUjmxZMkSdO3aFSdPnkSLFi0AABcuXEBiYiKOHz8ucjoqDkEQ8r2y5cGDBzAwMBAhESnD06dPMXXqVAQFBeHZs2f4uPOcY5+IPmAhU064ubkhOjoaa9askZ+T7dWrF8aPH885KSSqYcOGkMlkkMlkaN++PSpU+P9v5+zsbMTHx6NTp04iJqTiGDp0KBISEjBz5kz57NxElBfHyBBJ1Ny5c+X/+vn5oVKlSvJtmpqasLGxgZeXFzQ1NcWKSMWgp6eHs2fPokGDBmJHISrT2CNDJFGzZ88GANjY2KBv374Ka/KQ9FlZWeU5nUREebFHhoioDAoICMCyZcuwfv162NjYiB2HlMjIyCjfU4UymQza2tqwt7fH0KFDOVi/gFjIEEmQsbExoqOjYWpq+slfirmSkpJKMRkpi5GREdLT05GVlQVdXd08MzbzeZWuFStWYMGCBejcuTOaNm0KALh8+TJOnDiByZMnIz4+Htu3b8evv/6KUaNGiZy27OOpJSIJWrFihXwB0BUrVnAgqApauXKl2BGohJw7dw7z58/H2LFjFdrXr1+PgIAAHDhwAM7Ozli1ahULmQJgj0w58/z5c0RFRQEAHBwcULlyZZETERGVL5UqVUJ4eDjs7e0V2mNiYtCgQQOkpqYiNjYWzs7OSEtLEymldHBCvHIiLS0Nw4cPh6WlJVxdXeHq6gpLS0uMGDGCq61K3ODBg+Hv74/Y2Fixo5CSZWdn48CBA5g/fz7mz5+PQ4cOcf4YFWBsbIyjR4/maT969CiMjY0BfPidndvrSp/HU0vlxJQpUxASEoIjR44oLBrp4+MDPz8/rF27VuSEVFSamppYtGgRRowYgapVq8LNzQ3u7u5wc3NDzZo1xY5HRRQTE4MuXbrg4cOHcHBwAAAsWrQIVlZW+Ouvv1CjRg2RE1JRzZw5E+PGjcPp06flY2RCQ0Nx/PhxrFu3DgAQGBgINzc3MWNKBk8tlROmpqbYv38/3N3dFdpPnz6NPn364Pnz5+IEI6V5+PAhzpw5g5CQEISEhCA6OhoWFhZ48OCB2NGoCLp06QJBELBjxw75p/SXL1/im2++gZqaGv766y+RE1Jx/PPPP1i9erXCqX5vb2+0bNlS5GTSwx6ZciI9PR1VqlTJ025mZsZTSyrCyMgIJiYmMDIygqGhISpUqMAxUBIWEhKCixcvyosYADAxMcHixYvlvaokXa1ateLzqCQsZMqJFi1aYPbs2di2bZt84rS3b99i7ty58rWXSJr++9//Ijg4GNeuXYOjoyPc3Nzw/fffw9XVFUZGRmLHoyLS0tLCmzdv8rSnpqZytmYVkJOTg5iYGDx79gw5OTkK21xdXUVKJU08tVROXL9+HZ06dUJGRoZ8teuIiAhoa2vj77//Rt26dUVOSEWlpqaGypUrY/LkyejVqxdq1aoldiRSgsGDB+Pq1avYvHmzfBzFpUuXMGrUKDRu3BhbtmwRNyAV2cWLFzFgwADcv38/z+zNMpmMA7oLiYVMOZKeno4dO3bIF410dHTEwIEDoaOjI3IyKo6IiAiEhIQgODgYZ8+ehaampnzAr7u7OwsbiUpOTsaQIUNw9OhR+WR4WVlZ8PT0xJYtW7iyuYQ1aNAAtWrVwty5c/NdEJTPbeGwkCknzpw5g5YtWyqskAx8+MV4/vx5dmWqkIiICKxYsQI7duxATk4OP91JXExMDG7fvg3gw4ePj+ceIempWLEiIiIi+FwqCcfIlBNt27bF48ePYWZmptCekpKCtm3b8o+dhAmCgGvXriE4OBjBwcE4d+4cXr9+DWdnZ16+qQLs7e35B0/FNGvWDDExMXxelYSFTDkhCEK+09i/fPkSFStWFCERKYuxsTFSU1NRv359uLm5YdSoUWjTpg0MDQ3FjkZE+fD29oafnx+ePHkCJyenPOtoOTs7i5RMmnhqScX16tULAHD48GF06tQJWlpa8m3Z2dmIjIyEg4MDTpw4IVZEKqa//voLbdq0gb6+vthRiKgA1NTyTqovk8nkHzjZQ1447JFRcbmDxgRBgJ6ensLAXk1NTTRv3pyLkklc165dxY5ARIUQHx8vdgSVwh6ZcmLu3LmYOnUqTyMREZFKYSFTznD1ayJpiIyMzLddJpNBW1sb1atXVzhVTGXbkSNH0LlzZ2hoaODIkSOf3dfT07OUUqkGFjLlRHp6OiZOnIht27bJZ5FUV1fH4MGD8euvv0JXV1fkhET0b2pqavkO0M+loaGBvn37Yv369fLZuqnsUlNTw5MnT2BmZpbvGJlcHCNTeJ/+3ySVMnnyZISEhODo0aNITk5GcnIyDh8+jJCQEPj5+Ykdj4rhzJkzyMrKytOelZWFM2fOiJCIlOHQoUOoWbMmNmzYgPDwcISHh2PDhg1wcHDAzp07sXnzZpw6dQozZswQOyoVQE5Ojnz6i5ycnE/eWMQUgUDlgomJiXD69Ok87adOnRJMTU1LPxApjZqamvD06dM87S9evBDU1NRESETK0KRJE+HEiRN52k+cOCE0adJEEARBOHTokGBnZ1fa0aiYtm7dKrx79y5Pe0ZGhrB161YREkkbe2TKCa5+rboEzhGkkq5fvw5ra+s87dbW1rh+/TqAD1PdP378uLSjUTENGzYMKSkpedrfvHmDYcOGiZBI2nj5dTnB1a9VT+4cQTKZDEOHDs13jqCWLVuKFY+KqXbt2li8eDE2bNggX+06MzMTixcvRu3atQEADx8+zPcDCpVtn/rw8eDBA66zVAQsZMqJX375BR4eHqhWrVq+q1+T9HCOINW2Zs0aeHp6olq1avKZXq9fv47s7GwcO3YMABAXF4fx48eLGZMKoWHDhpDJZJDJZGjfvr3C2nfZ2dmIj49Hp06dREwoTbxqqRzh6teqiXMEqa43b95gx44diI6OBvBhyoQBAwZAT09P5GRUFHPnzpX/6+fnh0qVKsm3aWpqwsbGBl5eXvIeOCoYFjJERESlaOvWrejXrx/nAVISFjLlyN27d3H69Gk8e/ZMPpdMrlmzZomUipRh//792Lt3LxISEvD+/XuFbVevXhUpFRUX37OqKTExETKZDNWqVQMAXL58GTt37kSdOnUwevRokdNJDwuZcmLjxo0YN24cTE1NYW5urjDQTCaT8Y+dhK1atQo//PADhg4dig0bNmDYsGGIjY1FaGgoJkyYgAULFogdkYqA71nV1aZNG4wePRqDBg3CkydPUKtWLdSrVw93796Ft7c3i9RCYiFTTlhbW2P8+PH47rvvxI5CSla7dm3Mnj0b/fv3h56eHiIiImBnZ4dZs2YhKSkJq1evFjsiFQHfs6rLyMgIFy9ehIODA1atWoU9e/bgn3/+QUBAAMaOHYu4uDixI0oK55EpJ169eoXevXuLHYNKQEJCgvwyax0dHbx58wYAMGjQIOzatUvMaFQMfM+qrszMTPn4mJMnT8rXVqpduzbnBSoCFjLlRO/evREQECB2DCoB5ubmSEpKAgBUr14dFy9eBADEx8eDHa7Sxfes6qpbty7WrVuHs2fPIjAwUH7J9aNHj2BiYiJyOunhPDIqbNWqVfKv7e3tMXPmTFy8eBFOTk7Q0NBQ2NfHx6e045GStGvXDkeOHEHDhg0xbNgwTJ48Gfv370dYWJh80jySHr5nVddPP/2Enj17YunSpRgyZIh8bq8jR46gadOmIqeTHo6RUWG2trYF2k8mk/GcrITlLjaXO7nW7t27cf78edSsWRNjxozhnBQS9bn3L9+z0pednY3Xr1/DyMhI3nbv3j3o6urKF5ekgmEhQ0REVMqysrIQHByM2NhY+SSHjx49gr6+vsJEefRlLGTKodynPL+1Pkh6/P39UalSpTwDQ/ft24f09HQMGTJEpGRElJ/79++jU6dOSEhIQEZGBqKjo2FnZwdfX19kZGRg3bp1YkeUFI6RKUc2b96MFStW4O7duwCAmjVrYtKkSRg5cqTIyag4Fi1ahPXr1+dpNzMzw+jRo1nISMiUKVPw448/omLFipgyZcpn912+fHkppSJl8/X1hYuLCyIiIhQG9/bs2ZProxUBC5lyYtasWVi+fDm8vb3lq11fuHABkydPRkJCAubNmydyQiqqhISEfMdTWFtbIyEhQYREVFTXrl1DZmam/OtPYW+qtJ09exbnz5/PM37NxsYGDx8+FCmVdLGQKSfWrl2LjRs3on///vI2T09PODs7w9vbm4WMhJmZmSEyMhI2NjYK7R9/2qOy7/Tp0/l+TaolJycH2dnZedofPHjABUGLgPPIlBOZmZlwcXHJ0964cWNkZWWJkIiUpX///vDx8cHp06eRnZ2N7OxsnDp1Cr6+vujXr5/Y8aiI/vjjD6Snp4sdg0pAx44dsXLlSvl9mUyG1NRUzJ49G126dBEvmERxsG854e3tDQ0NjTzn1adOnYq3b99izZo1IiWj4nr//j0GDRqEffv2yS/BzsnJweDBg7Fu3Tpefi1RlStXxtu3b+Hp6YlvvvkGHh4eUFdXFzsWKUFiYiI6deoEQRBw9+5duLi44O7duzA1NcWZM2d4+XUhsZApJ7y9vbFt2zZYWVmhefPmAIBLly4hISEBgwcPVphsi4MIpenu3bsIDw+Hjo4OnJycYG1tLXYkKoasrCycOHECu3btwuHDh6Grq4vevXtj4MCB8iUpSLqysrKwZ88eREREIDU1FY0aNcLAgQOho6MjdjTJYSFTTrRt27ZA+8lkMpw6daqE0xBRYaSnp+PQoUPYuXMnTp48iWrVqiE2NlbsWFQEmZmZqF27No4dOwZHR0ex46gEDvYtJzhwkEi6dHV14eHhgVevXuH+/fu4ffu22JGoiDQ0NPDu3TuxY6gUDvYlIiqj0tPTsWPHDnTp0gVVq1bFypUr0bNnT9y8eVPsaFQMEyZMwE8//cQLLZSEp5aIiMqgfv364dixY9DV1UWfPn0wcOBA+RxQJG09e/ZEUFAQKlWqBCcnJ1SsWFFh+8GDB0VKJk08tUQkcZmZmXlWRs714sULmJqalnIiUgZ1dXXs3buXVyupIENDQ3h5eYkdQ2WwR4ZI4ry8vLB///48s70+ffoU7du3x40bN0RKRkRU8tgjQyRxCQkJGDlyJDZv3ixve/LkCdq2bYu6deuKmIwKa9WqVRg9ejS0tbWxatWqz+7r4+NTSqmIyjb2yJQzt27dQkJCAt6/f6/Q7unpKVIiKq7nz5/D1dUVnTt3xvLly/Ho0SO0bdsW9evXx+7du6GmxjH9UmFra4uwsDCYmJjAxsbmk2sqyWQyxMXFlXI6orKJhUw5ERcXh549e+L69euQyWTIfdpzf1Hmt+4HSUdiYiJat24NLy8vHDt2DI0aNcKOHTs4toKIVB4/qpUTvr6+sLW1xbNnz6Crq4ubN2/izJkzcHFxQXBwsNjxqJisrKwQGBiIHTt2oGnTpti1axeLGAnLzMxEjRo1OF8MUQFwjEw5ceHCBZw6dQqmpqZQU1ODmpoaWrdujUWLFsHHxwfXrl0TOyIVgpGRUb6nHdLT03H06FGFVa+TkpJKMxopASdNIyo4FjLlRHZ2tnx5eFNTUzx69AgODg6wtrZGVFSUyOmosP69ci6pptxJ0zZt2iRfDJSk60uDt/+NA7kLh2Nkyok2bdrAz88PPXr0wIABA/Dq1SvMmDEDGzZswJUrV3iJLlEZw0nTVIutrW2B9uNA7sJjmV9OzJgxA2lpaQCAefPm4T//+Q/atGkDExMT7NmzR+R0VBzHjx+Huro6PDw8FNoDAgKQnZ2Nzp07i5SMioOTpqmW+Ph4sSOoLPbIlGNJSUmfHGtB0uHs7IzFixejS5cuCu0nTpzAd999h4iICJGSERGVPBYyRBKno6OD27dvw8bGRqH93r17qFu3rrwnjojKjgcPHuDIkSP5zuu1fPlykVJJE08tEUmcgYEB4uLi8hQyMTExecZVUNnWsGHDAveQXr16tYTTUEkJCgqCp6cn7OzscOfOHdSrVw/37t2DIAho1KiR2PEkh/PIEElc9+7dMWnSJMTGxsrbYmJi4OfnxxmbJaZHjx7o3r07unfvDg8PD8TGxkJLSwvu7u5wd3eHtrY2YmNj84yHImmZPn06pk6diuvXr0NbWxsHDhxAYmIi3Nzc0Lt3b7HjSQ5PLRFJXEpKCjp16oSwsDBUq1YNwIdu6zZt2uDgwYMwNDQUNyAVyciRI2FhYYEff/xRoX327NlITEzE77//LlIyKi49PT2Eh4ejRo0aMDIywrlz51C3bl1ERESge/fuuHfvntgRJYWnlogkzsDAAOfPn0dgYCAiIiKgo6MDZ2dnuLq6ih2NimHfvn0ICwvL0/7NN9/AxcWFhYyEVaxYUT4uxsLCArGxsfIFXl+8eCFmNEliIUOkAmQyGTp27IiOHTuKHYWUREdHB//88w9q1qyp0P7PP/9AW1tbpFSkDM2bN8e5c+fg6OiILl26wM/PD9evX8fBgwfRvHlzseNJDgsZIhWQlpaGkJCQfK+A4Cyh0jRp0iSMGzcOV69eRdOmTQEAly5dwu+//46ZM2eKnI6KY/ny5UhNTQUAzJ07F6mpqdizZw9q1qzJK5aKgGNkiCTu2rVr6NKlC9LT05GWlgZjY2O8ePECurq6MDMz4yyhErZ371788ssv8sUjHR0d4evriz59+oicjKjsYCFDJHHu7u6oVasW1q1bBwMDA0REREBDQwPffPMNfH190atXL7EjEtEnpKamIicnR6FNX19fpDTSxEKGSOIMDQ1x6dIlODg4wNDQEBcuXICjoyMuXbqEIUOG4M6dO2JHJKJ/iY+Px8SJExEcHKywyrkgCJDJZMjOzhYxnfRwjAyRxGloaEBN7cOUUGZmZkhISICjoyMMDAyQmJgocjoi+tg333wDQRDw+++/o0qVKlwmpphYyBBJXMOGDREaGoqaNWvCzc0Ns2bNwosXL7B9+3bUq1dP7HhE9JGIiAhcuXIFDg4OYkdRCZzZl0jiFi5cCAsLCwDAggULYGRkhHHjxuH58+fYsGGDyOmI6GNNmjRhb6kScYwMEVEZk5mZidq1a+PYsWNwdHQUOw4pWWxsLMaOHYtvvvkG9erVg4aGhsJ2Z2dnkZJJE08tERGVMRoaGgqDQEm1PH/+HLGxsRg2bJi8TSaTcbBvEbFHhkiCuEqy6lu4cCGio6OxadMmVKjAz5yqpE6dOnB0dMS0adPyHexrbW0tUjJp4ruDSIJ69OghdgQqYaGhoQgKCkJAQACcnJxQsWJFhe0HDx4UKRkV1/3793HkyBHY29uLHUUlsJAhkqDZs2eLHYFKmKGhIby8vMSOQSWgXbt2iIiIYCGjJDy1RKQiwsLC5FPZ16lTB40bNxY5ERHlZ8OGDZg/fz6GDx8OJyenPIN9PT09RUomTSxkiCTuwYMH6N+/P/755x8YGhoCAJKTk9GyZUvs3r0b1apVEzcgESnIncAyPxzsW3gsZIgkrlOnTkhOTsbWrVvlE2xFRUVh2LBh0NfXx4kTJ0ROSEW1f/9+7N27N99VzTmIm+gDTohHJHEhISFYu3atwiyhDg4O+PXXX3HmzBkRk1FxrFq1CsOGDUOVKlVw7do1NG3aFCYmJoiLi0Pnzp3FjkdUZnCwL5HEWVlZITMzM097dnY2LC0tRUhEyvDbb79hw4YN6N+/P7Zs2YJp06bBzs4Os2bNQlJSktjxqJBWrVqF0aNHQ1tbG6tWrfrsvj4+PqWUSjXw1BKRxB0+fBgLFy7EmjVr4OLiAuDDwF9vb2989913vFRbonR1dXH79m1YW1vDzMwMgYGBqF+/Pu7evYvmzZvj5cuXYkekQrC1tUVYWBhMTExga2v7yf1kMhni4uJKMZn0sUeGSIKMjIwUJtFKS0tDs2bN5BOnZWVloUKFChg+fDgLGYkyNzdHUlISrK2tUb16dVy8eBH169dHfHw8+PlTeuLj4/P9moqPhQyRBK1cuVLsCFTC2rVrhyNHjqBhw4YYNmwYJk+ejP379yMsLAy9evUSOx5RmcFTS0REZVBOTg5ycnLkvWy7d+/G+fPnUbNmTYwZMwaampoiJ6Siys7OxpYtWxAUFIRnz54hJydHYfupU6dESiZNLGSIiIhK0cSJE7FlyxZ07doVFhYWedZaWrFihUjJpImFDBFRGeTv749KlSqhd+/eCu379u1Deno6hgwZIlIyKi5TU1Ns27YNXbp0ETuKSuA8MkREZdCiRYtgamqap93MzAwLFy4UIREpi6amJtdZUiIWMkREZVBCQkK+l+laW1sjISFBhESkLH5+fvjll1949ZmS8KolIqIyyMzMDJGRkbCxsVFoj4iIgImJiTihSCnOnTuH06dP43//+x/q1q2bZ9HIgwcPipRMmljIEKmAsLCwT67Jw1+K0tS/f3/4+PhAT08Prq6uAD4sR+Hr64t+/fqJnI6Kw9DQED179hQ7hsrgYF8iidu9ezcGDx4MDw8PBAQEoGPHjoiOjsbTp0/Rs2dP+Pv7ix2RiuD9+/cYNGgQ9u3bJ78EOycnB4MHD8a6det4+TXR/2EhQyRxzs7OGDNmDCZMmAA9PT1ERETA1tYWY8aMgYWFBebOnSt2RCqGu3fvIjw8HDo6OnBycoK1tbXYkYjKFBYyRBJXsWJF3Lx5EzY2NjAxMUFwcDCcnJxw+/ZttGvXDo8fPxY7IhF9ZP/+/Z88HXz16lWRUkkTr1oikjgjIyO8efMGAFC1alXcuHEDAJCcnIz09HQxoxFRPlatWoVhw4ahSpUquHbtGpo2bQoTExPExcWhc+fOYseTHBYyRBLn6uqKwMBAAEDv3r3h6+uLUaNGoX///mjfvr3I6YjoY7/99hs2bNiAX3/9FZqampg2bRoCAwPh4+ODlJQUseNJDk8tEUlcUlIS3r17B0tLS+Tk5GDJkiXyNXlmzJgBIyMjsSMS0b/o6uri9u3bsLa2hpmZGQIDA1G/fn3cvXsXzZs3x8uXL8WOKCm8/JpI4oyNjeVfq6mp4fvvvxcxDRF9ibm5OZKSkmBtbY3q1avj4sWLqF+/PuLj4zlJXhGwkCEiKsPS09PzHRDq7OwsUiIqrnbt2uHIkSNo2LAhhg0bhsmTJ2P//v0ICwtDr169xI4nOTy1RERUBj1//hzDhg3D//73v3y3Z2dnl3IiUpacnBzk5OTI5wfavXu3/HTwmDFjOEdQIbGQISIqgwYOHIj79+9j5cqVcHd3x6FDh/D06VPMnz8fy5YtQ9euXcWOSFQm8NQSkQRFRkaiXr16UFPjhYeq6tSpUzh8+DBcXFygpqYGa2trfPXVV9DX18eiRYtYyBD9H/4WJJKghg0b4sWLFwAAOzs7XuWggtLS0mBmZgbgw1xBz58/BwA4OTlxwjSif2EhQyRBhoaGiI+PBwDcu3cPOTk5IiciZXNwcEBUVBQAoH79+li/fj0ePnyIdevWwcLCQuR0RGUHTy0RSZCXlxfc3NxgYWEBmUwGFxcXqKur57tvXFxcKacjZfD19ZUvLzF79mx06tQJO3bsgKamJrZs2SJuOKIyhIN9iSTqxIkTiImJgY+PD+bNmwc9Pb189/P19S3lZFQS0tPTcefOHVSvXh2mpqZix6Fi2LVrF/r375/vtm+//RZLly4t5UTSxkKGSOKGDRuGVatWfbKQIaKyxdDQELt27cqzrtLkyZOxe/duLvRaSCxkiFTIgwcPAADVqlUTOQkRfcpff/2FgQMH4tixY2jdujUAwNvbGwcPHkRQUBBq164tckJp4WBfIonLycnBvHnzYGBgAGtra1hbW8PQ0BA//vgjBwETlUFdu3bFb7/9Bk9PT1y5cgXjx4/HwYMHcfr0aRYxRcDBvkQS98MPP2Dz5s1YvHgxWrVqBQA4d+4c5syZg3fv3mHBggUiJySijw0YMADJyclo1aoVKleujJCQENjb24sdS5J4aolI4iwtLbFu3Tp4enoqtB8+fBjjx4/Hw4cPRUpGhdWrVy9s2bIF+vr62LZtG/r27QstLS2xY5ESTJkyJd/2ffv2oVGjRqhRo4a8bfny5aUVSyWwkCGSOG1tbURGRqJWrVoK7VFRUWjQoAHevn0rUjIqLE1NTdy/fx8WFhZQV1fH48eP5ZPikbS1bdu2QPvJZDKcOnWqhNOoFp5aIpK4+vXrY/Xq1Vi1apVC++rVq1G/fn2RUlFR1K5dG9OnT0fbtm0hCAL27t0LfX39fPcdPHhwKaej4jh9+rTYEVQWe2SIJC4kJARdu3ZF9erV0aJFCwDAhQsXkJiYiOPHj6NNmzYiJ6SCOn/+PKZMmYLY2FgkJSVBT08PMpksz34ymQxJSUkiJCRlSElJQXZ2NoyNjRXak5KSUKFChU8Wr5Q/FjJEKuDRo0dYs2YN7ty5AwBwdHTE+PHjYWlpKXIyKio1NTU8efKEp5ZUUOfOndGtWzeMHz9eoX3dunU4cuQIjh8/LlIyaWIhQ0RUBt2/fx/Vq1fPt0eGpM3Y2Bj//PMPHB0dFdrv3LmDVq1acRHYQuIYGSKiMsja2hrJycnYvHkzbt++DQCoU6cORowYAQMDA5HTUXFkZGQgKysrT3tmZiYH5xcBJ8QjIiqDwsLCUKNGDaxYsQJJSUlISkrCihUrUKNGDVy9elXseFQMTZs2xYYNG/K0r1u3Do0bNxYhkbTx1BIRURnUpk0b2NvbY+PGjahQ4UPneVZWFkaOHIm4uDicOXNG5IRUVP/88w86dOiAJk2aoH379gCAoKAghIaGIiAggAP0C4mFDBFRGaSjo4Nr167lmbL+1q1bcHFxQXp6ukjJSBnCw8OxdOlShIeHQ0dHB87Ozpg+fTpq1qwpdjTJ4RgZIhXx/PlzREVFAQAcHBxQuXJlkRNRcejr6yMhISFPIZOYmMiVzlVAgwYNsGPHDrFjqASOkSGSuLS0NAwfPhyWlpZwdXWFq6srLC0tMWLECH5ql7C+fftixIgR2LNnDxITE5GYmIjdu3dj5MiR6N+/v9jxSEnevXuH169fK9yocNgjQyRxU6ZMQUhICI4cOaKwaKSPjw/8/Pywdu1akRNSUfz888+QyWQYPHiw/AoXDQ0NjBs3DosXLxY5HRVHeno6pk2bhr179+Z7qXV2drYIqaSLY2SIJM7U1BT79++Hu7u7Qvvp06fRp08fPH/+XJxgpBTp6emIjY0FANSoUQO6uroiJ6LimjBhAk6fPo0ff/wRgwYNwpo1a/Dw4UOsX78eixcvxsCBA8WOKCksZIgkTldXF1euXMkzudbNmzfRtGlTpKWliZSMiPJTvXp1bNu2De7u7tDX18fVq1dhb2+P7du3Y9euXZzZt5A4RoZI4lq0aIHZs2fj3bt38ra3b99i7ty58rWXiKjsSEpKgp2dHYAPg7pz181q3bo1L6svAo6RIZK4lStXolOnTqhWrZp8teuIiAhoa2vj77//FjkdEX3Mzs4O8fHxqF69OmrXro29e/eiadOmOHr0KAwNDcWOJzk8tUSkAtLT07Fjxw6FRSMHDhwIHR0dkZMR0cdWrFgBdXV1+Pj44OTJk+jWrRsEQUBmZiaWL18OX19fsSNKCgsZIok7c+YMWrZsKZ/9NVdWVhbOnz8PV1dXkZIRUUHcv38fV65cgb29PZydncWOIzksZIgkTl1dHY8fP4aZmZlC+8uXL2FmZsZLOSVs+/btWLduHeLj43HhwgVYW1tj5cqVsLW1Rffu3cWOR1QmcIwMkcQJggCZTJan/eXLl6hYsaIIiUgZ1q5di1mzZmHSpElYsGCBvCA1NDTEypUrWchIzKpVqwq8r4+PTwkmUT3skSGSqF69egEADh8+jE6dOkFLS0u+LTs7G5GRkXBwcMCJEyfEikjFUKdOHSxcuBA9evSAnp4eIiIiYGdnhxs3bsDd3R0vXrwQOyIVgq2tbYH2k8lkiIuLK+E0qoU9MkQSZWBgAOBDj4yenp7CwF5NTU00b94co0aNEiseFVN8fDwaNmyYp11LS4tzA0lQfHy82BFUFgsZIony9/cHANjY2GDq1Kk8jaRibG1tER4eDmtra4X2EydO5Jn8kKQr96RIfqeHqWA4IR6RxM2ePRsVK1bE8+fPce7cOZw7d47LEqiAKVOmYMKECdizZw8EQcDly5exYMECTJ8+HdOmTRM7HhXT5s2bUa9ePWhra0NbWxv16tXDpk2bxI4lSeyRIZK49PR0TJw4Edu2bUNOTg6AD1cyDR48GL/++ivX5pGokSNHQkdHBzNmzEB6ejoGDBgAS0tL/PLLL+jXr5/Y8agYZs2aheXLl8Pb21s++/aFCxcwefJkJCQkYN68eSInlBYO9iWSuDFjxuDkyZNYvXp1ntWvv/rqK65+LUFZWVnYuXMnPDw8UKVKFaSnpyM1NTXPJfYkTZUrV8aqVavQv39/hfZdu3bB29ubA7kLiYUMkcRx9WvVpKuri9u3b+cZI0PSZ2hoiNDQUNSsWVOhPTo6Gk2bNkVycrI4wSSKY2SIJC49PR1VqlTJ025mZob09HQREpEyNG3aFNeuXRM7BpWAQYMG5dtTumHDBgwcOFCERNLGMTJEEpe7+vW2bdugra0NgKtfq4Lx48fDz88PDx48QOPGjfNclcap7KVlypQp8q9lMhk2bdqEgIAANG/eHABw6dIlJCQkYPDgwWJFlCyeWiKSuBs3bsDDwwMZGRn5rn5dt25dkRNSUaip5e0wl8lk8pmcufSEtLRt27ZA+8lkMpw6daqE06gWFjJEKoCrX6ue+/fvf3Y7x84QfcBChoiIiCSLY2SIVMDdu3dx+vRpPHv2TD6XTK5Zs2aJlIqKY9u2bZ/dzrEURB+wR4ZI4jZu3Ihx48bB1NQU5ubmClOdy2QyXL16VcR0VFRGRkYK9zMzM5Geng5NTU3o6uoiKSlJpGREZQsLGSKJs7a2xvjx4/Hdd9+JHYVK2N27dzFu3Dh8++238PDwEDsOUZnAQoZI4vT19REeHg47Ozuxo1ApCAsLwzfffCMf2E1U3nGMDJHE9e7dGwEBARg7dqzYUagUVKhQAY8ePRI7BinBrVu3kJCQgPfv3yu0e3p6ipRImljIEEnQqlWr5F/b29tj5syZuHjxIpycnKChoaGwr4+PT2nHIyU4cuSIwn1BEPD48WOFNbVImuLi4tCzZ09cv35dPjcQAPn4Ns4RVDg8tUQkQba2tgXaTyaTIS4uroTTUEn4eEI8mUyGypUro127dli2bBksLCxESkbF1a1bN6irq2PTpk2wtbXF5cuX8fLlS/j5+eHnn39GmzZtxI4oKSxkiIiISpGpqSlOnToFZ2dnGBgY4PLly3BwcMCpU6fg5+fHNbYKiYtGEqkQQRDAzyaqYd68efku+vn27VvMmzdPhESkLNnZ2dDT0wPwoajJHfNkbW2NqKgoMaNJEgsZIhWwefNm1KtXD9ra2tDW1ka9evWwadMmsWNRMcydOxepqal52tPT0zF37lwREpGy1KtXDxEREQCAZs2aYcmSJfjnn38wb948Xn1YBBzsSyRxs2bNwvLly+Ht7S1f7frChQuYPHkyEhIS+OldonIXh/xYREQEjI2NRUhEyjJjxgykpaUB+NDz9p///Adt2rSBiYkJ9uzZI3I66eEYGSKJq1y5MlatWoX+/fsrtO/atQve3t548eKFSMmoKIyMjCCTyZCSkgJ9fX2FYiY7OxupqakYO3Ys1qxZI2JKUrakpCT5c0+Fwx4ZIonLzMyEi4tLnvbGjRsjKytLhERUHCtXroQgCBg+fDjmzp0LAwMD+TZNTU3Y2NjIe95IdbCXrejYI0Mkcd7e3tDQ0MDy5csV2qdOnYq3b9/yk7tEhYSEoFWrVqhQgZ83iT6HhQyRxHl7e2Pbtm3/r717D4qq/P8A/l6Ry8LCIpeEFBcMQY2BTEcBx0qt1MJrFydNxLxLiIZ2m1Ebnb4qKBrFWGRDmWWMQf6hjpisZmPGLSWKhQWBqKAcJZxABVk+3z/6eX6tIGFfbffQ+zWzM5znPGfPh7OjvOec53kWAQEBiIyMBADk5+ejrq4OsbGxVgvk3Rh2yH4dPnwYDg4Onb5TKTc3Fx0dHZgyZYqNKiOyLwwyRCo3fvz4HvXTaDQwGo13uBq6XcLDw7FlyxY89thjVu1HjhzBSy+9pMx6Ifq3Y5AhIrJDWq0WJpMJgYGBVu21tbW49957lVkvRP92XEeGiMgO6fX6Lr9eoqqqCm5ubjaoiMg+McgQEdmh6dOnY9WqVTh37pzSVlVVhaSkJH47MtGf8NESEZEdunTpEiZPnoyioiIMHDgQAPDTTz9h3LhxyMnJgaenp20LJLITDDJERHZKRPD555+jpKQEWq0W4eHheOCBB2xdFpFdYZAhIiIi1eJKS0S9RFlZGerq6tDW1mbVzvEU6pGWloYlS5bAxcUFaWlp3fZduXLlP1QVkX3jHRkilauursbMmTNRWloKjUaD6/+kr39ni8VisWV5dAuCgoJQVFQEb29vBAUF3bSfRqPpckYT0b8RgwyRyk2dOhUODg7YvXs3goKCUFBQgIsXLyIpKQnbtm3DuHHjbF0iEdEdwyBDpHI+Pj4wGo0IDw+HXq9HQUEBQkNDYTQakZSUhDNnzti6RPof3XiXjYj+H9eRIVI5i8UCd3d3AH+Emvr6egCAwWBARUWFLUuj/9F7772HsLAwuLi4wMXFBWFhYdi9e7etyyKyKxzsS6RyYWFhKCkpQVBQEMaMGYPk5GQ4OTkhIyMDgwcPtnV59DetX78eqampSEhIQFRUFADg9OnTWL16Nerq6rBx40YbV0hkH/hoiUjlcnNz0dLSglmzZqGqqgoxMTEwm83w9vZGVlYWJkyYYOsS6W/w9fVFWloannnmGav2ffv2ISEhARcuXLBRZUT2hUGGqBdqbGxEv379OKZCxTw9PVFYWIghQ4ZYtZvNZowePRpNTU22KYzIznCMDFEv5OXlxRCjcvPmzcOuXbs6tWdkZGDu3Lk2qIjIPvGODBGRHUpISMCePXsQEBCAyMhIAEB+fj7q6uoQGxsLR0dHpW9qaqqtyiSyOQYZIiI7NH78+B7102g0MBqNd7gaIvvFIENERESqxTEyREREpFoMMkRERKRaDDJERESkWgwyREREpFoMMkRERKRa/K4lIiI7VlZWhrq6OrS1tVm1T5s2zUYVEdkXBhkiIjtUXV2NmTNnorS0FBqNBtdXyri+YrPFYrFleUR2g4+WiIjsUGJiIoKCgnD+/Hm4urri+++/x8mTJzFq1CicOHHC1uUR2Q0uiEdEZId8fHxgNBoRHh4OvV6PgoIChIaGwmg0IikpCWfOnLF1iUR2gXdkiIjskMVigbu7O4A/Qk19fT0AwGAwoKKiwpalEdkVjpEhIrJDYWFhKCkpQVBQEMaMGYPk5GQ4OTkhIyMDgwcPtnV5RHaDj5aIiOxQbm4uWlpaMGvWLFRVVSEmJgZmsxne3t7IysrChAkTbF0ikV1gkCEiUonGxkb069dPmblERAwyREREpGIc7EtERESqxSBDREREqsUgQ0RERKrFIENEtywuLg4zZsxQth966CGsWrWq22MCAwOxc+fOO1qXvTpx4gQ0Gg2amppsXQpRr8MgQ6RSv/zyCxISEjB48GA4OzsjICAAU6dORV5e3j9eS05ODjZt2nRb3/P999+Hp6fnbXu/M2fO4KmnnkL//v3h4uKCIUOGYPHixTCbzbftHEDXoS46OhoNDQ3Q6/W39VxExCBDpEq1tbUYOXIkjEYjUlJSUFpaiiNHjmD8+PGIj4//x+vx8vJSVqG1RwcPHkRkZCRaW1vx0UcfwWQyYe/evdDr9Vi3bt0dP7+TkxP8/Pw4bZroThAiUp0pU6bIgAEDpLm5udO+3377Tfl5+/btEhYWJq6urjJw4EBZvny5/P7778r+zMxM0ev1cuTIERk6dKi4ubnJpEmTpL6+XunT3t4uq1evFr1eL15eXrJ27VqJjY2V6dOnK30efPBBSUxMVLZ//fVXiYmJERcXFwkMDJS9e/eKwWCQHTt29Ki248ePCwCr14YNG0RE5OrVq5KUlCR33323uLq6yujRo+X48eM3vVYtLS3i4+MjM2bM6HL/9evV3t4uzz33nAQGBoqLi4uEhITIzp07rfrOnz9fpk+fLq+99pr4+PiIu7u7LF26VFpbW5X9N9ZdU1Oj/D5//mw+/fRTGT58uDg5OYnBYJBt27ZZnctgMMjrr78uCxYsEJ1OJwEBAfLOO+8o+1tbWyU+Pl78/PzE2dlZBg0aJP/5z39ueh2IeisGGSKVuXjxomg0mh790dqxY4cYjUapqamRvLw8CQ0NleXLlyv7MzMzxdHRUR5++GEpLCyU4uJiGTZsmMyZM0fps3XrVunXr59kZ2dLWVmZLFy4UNzd3bsNMlOmTJGIiAg5ffq0FBUVSXR0tGi1Wqsg011tra2tsnPnTvHw8JCGhgZpaGhQQs6iRYskOjpaTp48KVVVVZKSkiLOzs5iNpu7vAY5OTkCQL766qtur1VbW5usX79eCgsLpbq6Wvbu3Suurq6SlZWl9Jk/f77odDqZPXu2fPfdd3Lw4EHx9fWVV199VUREmpqaJCoqShYvXqzU3d7e3inIFBUVSZ8+fWTjxo1SUVEhmZmZotVqJTMzUzmXwWAQLy8vSU9Pl8rKStm8ebP06dNHysvLRUQkJSVFAgIC5OTJk1JbWytffvmlfPzxx93+jkS9EYMMkcrk5+cLAMnJybnlY/fv3y/e3t7KdmZmpgCQqqoqpS09PV369++vbPv7+0tycrKyfe3aNRk4cOBNg0xFRYUAkIKCAmW/yWQSAFZBpie16fV6qz4//PCDODg4yM8//2zVPnHiRHnllVe6fN+tW7cKAGlsbLzpuW8mPj5ennjiCWV7/vz54uXlJS0tLUrbrl27RKfTicViEZHOoU5EOgWZOXPmyCOPPGLVZ+3atTJ8+HBl22AwyLPPPqtsd3R0yF133SW7du0SEZGEhASZMGGCdHR03PLvRdSbcIwMkcrILSzGfezYMUycOBEDBgyAu7s75s2bh4sXL+Ly5ctKH1dXV9xzzz3Ktr+/P86fPw8AuHTpEhoaGjBmzBhlf9++fTFq1KibntNkMqFv374YOXKk0jZ06NBOA3d7UtuNSktLYbFYEBISAp1Op7y++OILnDt3rstjbuV6paenY+TIkfD19YVOp0NGRgbq6uqs+kRERMDV1VXZjoqKQnNzM3788ccen8dkMmHs2LFWbWPHjkVlZSUsFovSFh4ervys0Wjg5+enfDZxcXE4e/YsQkNDsXLlShw9erTH5yfqTRhkiFRmyJAh0Gg0KC8v77ZfbW0tYmJiEB4ejuzsbBQXFyM9PR0A0NbWpvRzdHS0Ok6j0dzSH/+/o6e13ai5uRkODg4oLi7G2bNnlZfJZMIbb7zR5TEhISEA8JfX65NPPsGaNWuwcOFCHD16FGfPnsWCBQu6redO6+qz6ejoAADcf//9qKmpwaZNm3DlyhU8/fTTePLJJ21RJpFNMcgQqYyXlxcmTZqE9PR0tLS0dNp/fa2S4uJidHR0YPv27YiMjERISAjq6+tv6Vx6vR7+/v7Iz89X2trb21FcXHzTY4YOHdqpT0VFhdUaKj2pzcnJyeruBACMGDECFosF58+fR3BwsNXLz8+vy3oeffRR+Pj4IDk5ucv91+s6deoUoqOjsWLFCowYMQLBwcFd3uUpKSnBlStXlO2vv/4aOp0OAQEBN637RsOGDcOpU6es2k6dOoWQkBA4ODh0e+yfeXh4YPbs2Xj33XeRlZWF7OxsNDY29vh4ot6AQYZIhdLT02GxWDB69GhkZ2ejsrISJpMJaWlpiIqKAgAEBwfj2rVrePPNN1FdXY0PP/wQb7/99i2fKzExEVu2bMGBAwdQXl6OFStWdLuwW2hoKCZPnoylS5ciPz8fxcXFWLRoEbRardKnJ7UFBgaiubkZeXl5uHDhAi5fvoyQkBDMnTsXsbGxyMnJQU1NDQoKCrB582YcOnSoy3rc3Nywe/duHDp0CNOmTcOxY8dQW1uLoqIivPjii1i2bBmAP+50FRUVITc3F2azGevWrUNhYWGn92tra8PChQtRVlaGw4cPY8OGDXj++efRp08fpe78/HzU1tbiwoULyh2UP0tKSkJeXh42bdoEs9mMDz74AG+99RbWrFnzl5/Hdampqdi3bx/Ky8thNpuxf/9++Pn53da1d4hUwcZjdIjob6qvr5f4+HgxGAzi5OQkAwYMkGnTpllNRU5NTRV/f3/RarUyadIk2bNnj9Wg064G1H722Wfy5/8arl27JomJieLh4SGenp7ywgsv/OX064aGBnn88ceVacF79uzpNP36r2oTEVm2bJl4e3tbTb++PrsoMDBQHB0dxd/fX2bOnCnffvttt9ersLBQZs2aJb6+vuLs7CzBwcGyZMkSqaysFJE/pnXHxcWJXq8XT09PWb58ubz88ssSERGhvMf16dfr168Xb29v0el0snjxYrl69arSp6KiQiIjI0Wr1fZo+rWjo6MMGjRIUlJSrOq98XqJiERERCjXISMjQ+677z5xc3MTDw8PmThxonzzzTfdXgOi3kgjcocfhhMR9RJxcXFoamrCgQMHbF0KEf0fPloiIiIi1WKQISIiItXioyUiIiJSLd6RISIiItVikCEiIiLVYpAhIiIi1WKQISIiItVikCEiIiLVYpAhIiIi1WKQISIiItVikCEiIiLVYpAhIiIi1fov99Rkj+km97kAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"code","source":["# CLIP score 게산\n","import torch\n","import torch.nn.functional as F\n","\n","# 이미지 임베딩 생성\n","pixel_values = clip_processor(images=image, return_tensors=\"pt\").pixel_values\n","image_features = clip_model.get_image_features(pixel_values)\n","\n","# 캡션 임베딩 생성\n","print(generated_caption)\n","text_inputs = clip_processor(text=[generated_caption], return_tensors=\"pt\", padding=True)\n","text_features = clip_model.get_text_features(**text_inputs)\n","\n","# 임베딩 정규화 (벡터 길이를 1로 만듦)\n","image_features = F.normalize(image_features, p=2, dim=1)\n","text_features = F.normalize(text_features, p=2, dim=1)\n","\n","# 유사도 점수 계산 (코사인 유사도)\n","similarity_score = (image_features @ text_features.T).item()\n","print(f\"CLIP Similarity Score: {similarity_score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmoawUAlSJhb","executionInfo":{"status":"ok","timestamp":1733026304024,"user_tz":-540,"elapsed":1434,"user":{"displayName":"김유진","userId":"04225960051398709816"}},"outputId":"18ffa90d-6923-44e0-df38-14e7984ea587"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["a photo of a dog and cat laying on a blanket \n","CLIP Similarity Score: 0.25801822543144226\n"]}]}]}